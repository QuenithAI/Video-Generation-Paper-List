<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Video Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Video Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/Video-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/Video-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (Â∫îËææÂ≠¶ÊúØ)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of video generation.
>
> Your contributions are also vital‚Äîfeel free to [open an issue](https://github.com/QuenithAI/Video-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/Video-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> Êú¨‰ªìÂ∫ìÁî± **„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI)** ÁöÑ‰∏ÄÁ∫øÁßëÁ†îÂØºÂ∏àÂõ¢ÈòüÂÄæÂäõÊâìÈÄ†Âπ∂ÊåÅÁª≠Áª¥Êä§ÔºåÊó®Âú®‰∏∫ÊÇ®ÂëàÁé∞ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÊúÄÂÖ®Èù¢„ÄÅÊúÄÂâçÊ≤øÁöÑËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÁöÑËÆ∫Êñá„ÄÇ
>
> ÊÇ®ÁöÑË¥°ÁåÆÂØπÊàë‰ª¨ÂíåÁ§æÂå∫Êù•ËØ¥Ëá≥ÂÖ≥ÈáçË¶Å‚Äî‚ÄîÊàë‰ª¨ËØöÈÇÄÊúâÂøó‰πãÂ£´ÈÄöËøá [open an issue](https://github.com/QuenithAI/Video-Generation-Paper-List/issues) Êàñ [submit a pull request](https://github.com/QuenithAI/Video-Generation-Paper-List/pulls) Êù•Êàê‰∏∫Ëøô‰∏™È°πÁõÆÁöÑÂêà‰ΩúËÄÖ‰πã‰∏ÄÔºåÊúüÂæÖÊÇ®ÁöÑÂä†ÂÖ•ÔºÅ
> 
> Â¶ÇÊûúÊÇ®Âú®ÂÜ≤Âà∫ÁßëÁ†îÈ°∂‰ºöÁöÑÈÅìË∑Ø‰∏äÈúÄË¶Å‰∏ì‰∏öÁöÑ1V1ÊåáÂØºÔºåÊ¨¢Ëøé**ÈÄöËøá[ÂæÆ‰ø°](assets/wechat.jpg)Êàñ[ÈÇÆ‰ª∂](mailto:christzhaung@gmail.com)ËÅîÁ≥ªÊàë‰ª¨**„ÄÇ


<details>
<summary><strong>‚ö° Latest Updates</strong></summary>

- **(Aug 21th, 2025)**: Add a new direction: [üó£Ô∏è Audio-Driven Video Generation](#Ô∏è-audio-driven-video-generation).
- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

<!-- omit in toc -->
## <span id="contents">üìö Table of Contents</span>
- [üìú Papers \& Models](#-papers--models)
  - [‚úçÔ∏è Survey Papers](#Ô∏è-survey-papers)
  - [üé• Text-to-Video (T2V) Generation](#-text-to-video-t2v-generation)
  - [üñºÔ∏è Image-to-Video (I2V) Generation](#Ô∏è-image-to-video-i2v-generation)
  - [‚úÇÔ∏è Video-to-Video (V2V) Editing](#Ô∏è-video-to-video-v2v-editing)
  - [üïπÔ∏è Controllable Video Generation](#Ô∏è-controllable-video-generation)
  - [üó£Ô∏è Audio-Driven Video Generation](#Ô∏è-audio-driven-video-generation)
  - [üíÉ Human Image Animation](#-human-image-animation)
  - [‚ö° Fast Video Generation (Acceleration)](#-fast-video-generation-acceleration)
- [üóÇÔ∏è Datasets](#Ô∏è-datasets)
- [üéì About Us](#-about-us)
- [ü§ù Contributing](#-contributing)

---

## <span id="papers">üìú Papers & Models</span>

### <span id="survey">‚úçÔ∏è Survey Papers</span>
- [Controllable Video Generation: A Survey](http://arxiv.org/abs/2507.16869v1)
- [Diffusion Model-Based Video Editing: A Survey](http://arxiv.org/abs/2407.07111v1)
- [From Sora What We Can See: A Survey of Text-to-Video Generation](http://arxiv.org/abs/2405.10674v1)
- [A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights](https://arxiv.org/abs/2407.08428)
- [A Survey on Video Diffusion Models](https://arxiv.org/abs/2310.10647v1)
- [Video Diffusion Models: A Survey](https://arxiv.org/abs/2405.03150)
- [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
- [Video Diffusion Generation: Comprehensive Review and Open Problems](https://link.springer.com/article/10.1007/s10462-025-11331-6)

### <span id="t2v">üé• Text-to-Video (T2V) Generation</span>
<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** *AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2411.17221)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wangjiarui153/AIGV-Assessor)

*   **[CVPR 2025]** *Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2410.06241)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bujiazi/ByTheWay)

*   **[CVPR 2025]** *Retrieval-Augmented Prompt Optimization for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2405.15579)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://whynothaha.github.io/Prompt_optimizer/RAPO.html)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Vchitect/RAPO)

*   **[CVPR 2025]** *Identity-Preserving Text-to-Video Generation by Frequency Decomposition*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2411.17440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pku-yuangroup.github.io/ConsisID/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PKU-YuanGroup/ConsisID)

*   **[CVPR 2025]** *Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2504.06861)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://djagpal02.github.io/EIDT-V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/djagpal02/EIDT-V)

*   **[CVPR 2025]** ***TransPixeler:*** *Advancing Text-to-Video Generation with Transparency*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.03006)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wileewang.github.io/TransPixar/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wileewang/TransPixeler)

*   **[CVPR 2025]** *LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.00596)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pittisl/PhyT2V)

*   **[CVPR 2025]** *Improving Text-to-Video Generation via Instance-aware Structured Caption*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.09283)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NJU-PCALab/InstanceCap)

*   **[CVPR 2025]** *Compositional Text-to-Video Generation with Blob Video Representations*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.07647)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://blobgen-vid2.github.io/)

*   **[CVPR 2025]** *Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.09856)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lineargen.github.io/)

*   **[ICLR 2025]** ***OpenVid-1M:*** *A Large-Scale High-Quality Dataset for Text-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=j7kdXSrISM)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nju-pcalab.github.io/projects/openvid/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NJU-PCALab/OpenVid-1M)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/nkp37/OpenVid-1M)

*   **[ICLR 2025]** ***CogVideoX:*** *Text-to-Video Diffusion Models with An Expert Transformer*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LQzN6TRFg9)

*   **[ICLR 2025]** *Pyramidal Flow Matching for Efficient Video Generative Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=66NzcRQuOq)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pyramid-flow.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/Pyramid-Flow)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/pyramid-flow-miniflux)

</details>


<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228)
[![GitHub Stars](https://img.shields.io/github/stars/kr-panghu/LayerT2V?style=social)](https://github.com/kr-panghu/LayerT2V) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kr-panghu.github.io/LayerT2V/)
- [S¬≤Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation](https://arxiv.org/abs/2508.04016)
[![GitHub Stars](https://img.shields.io/github/stars/wlfeng0509/s2q-vdit?style=social)](https://github.com/wlfeng0509/s2q-vdit)  
- [LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation](https://arxiv.org/abs/2508.03694) [![GitHub Stars](https://img.shields.io/github/stars/vchitect/LongVie?style=social)](https://github.com/vchitect/LongVie)  
- [Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation](https://arxiv.org/abs/2508.03334) [![GitHub Stars](https://img.shields.io/github/stars/Tele-AI/MMPL?style=social)](https://github.com/Tele-AI/MMPL) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://nju-xunzhixiang.github.io/Anchor-Forcing-Page/)
- [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](https://arxiv.org/abs/2508.03254) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://jiiiisoo.github.io/VIP.github.io/)
- [QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots](http://arxiv.org/abs/2508.02512v1) [![GitHub Stars](https://img.shields.io/github/stars/losehu/QuaDreamer?style=social)](https://github.com/losehu/QuaDreamer)
- [PoseGuard: Pose-Guided Generation with Safety Guardrails](http://arxiv.org/abs/2508.02476v1)
- [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2508.00312v1) [![GitHub Stars](https://img.shields.io/github/stars/Sumutan/GV-VAD?style=social)](https://github.com/Sumutan/GV-VAD)
- [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](http://arxiv.org/abs/2507.22360v1)
- [Compositional Video Synthesis by Temporal Object-Centric Learning](http://arxiv.org/abs/2507.20855v1)
- [Enhancing Scene Transition Awareness in Video Generation via Post-Training](http://arxiv.org/abs/2507.18046v1)
- [Yume: An Interactive World Generation Model](http://arxiv.org/abs/2507.17744v1) [![GitHub Stars](https://img.shields.io/github/stars/stdstu12/YUME?style=social)](https://github.com/stdstu12/YUME) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://stdstu12.github.io/YUME-Project/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/stdstu123/Yume-I2V-540P)
- [EndoGen: Conditional Autoregressive Endoscopic Video Generation](http://arxiv.org/abs/2507.17388v1) [![GitHub Stars](https://img.shields.io/github/stars/CUHK-AIM-Group/EndoGen?style=social)](https://github.com/CUHK-AIM-Group/EndoGen)
- [MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation](http://arxiv.org/abs/2507.16310v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://motionshot.github.io)
- [PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation](http://arxiv.org/abs/2507.16116v1) [![GitHub Stars](https://img.shields.io/github/stars/Yaofang-Liu/Pusa-VidGen?style=social)](https://github.com/Yaofang-Liu/Pusa-VidGen) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://yaofang-liu.github.io/Pusa_Web/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/RaphaelLiu/Pusa-Wan2.2-V1)
- [TokensGen: Harnessing Condensed Tokens for Long Video Generation](http://arxiv.org/abs/2507.15728v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vicky0522.github.io/tokensgen-webpage/)
- [Conditional Video Generation for High-Efficiency Video Compression](http://arxiv.org/abs/2507.15269v1)
- [Taming Diffusion Transformer for Real-Time Mobile Video Generation](http://arxiv.org/abs/2507.13343v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/mobile_video_dit/)
- [LoViC: Efficient Long Video Generation with Context Compression](http://arxiv.org/abs/2507.12952v1)
- [World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving](http://arxiv.org/abs/2507.12762v1)
- [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](http://arxiv.org/abs/2507.11245v2)
- [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](http://arxiv.org/abs/2507.08801v1) [![GitHub Stars](https://img.shields.io/github/stars/alibaba-damo-academy/Lumos?style=social)](https://github.com/alibaba-damo-academy/Lumos)
- [Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](http://arxiv.org/abs/2507.08422v1)
- [Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling](http://arxiv.org/abs/2507.07982v1)
- [Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions](http://arxiv.org/abs/2507.07978v1)
- [Scaling RL to Long Videos](http://arxiv.org/abs/2507.07966v3)
- [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](http://arxiv.org/abs/2507.06739v1)
- [Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions](http://arxiv.org/abs/2507.06133v2)
- [Omni-Video: Democratizing Unified Video Understanding and Generation](http://arxiv.org/abs/2507.06119v2)
- [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](http://arxiv.org/abs/2507.05963v2)
- [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](http://arxiv.org/abs/2507.05675v1)
- [Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations](http://arxiv.org/abs/2507.04705v1) [![GitHub Stars](https://img.shields.io/github/stars/PKU-YuanGroup/ConsisID?style=social)](https://github.com/PKU-YuanGroup/ConsisID) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pku-yuangroup.github.io/ConsisID/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/BestWishYsh/ConsisID-preview-Data)
- [PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)
- [StreamDiT: Real-Time Streaming Text-to-Video Generation](http://arxiv.org/abs/2507.03745v2)
- [RefTok: Reference-Based Tokenization for Video Generation](http://arxiv.org/abs/2507.02862v1)
- [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](http://arxiv.org/abs/2507.02860v1)
- [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](http://arxiv.org/abs/2507.02608v1)
- [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](http://arxiv.org/abs/2507.01945v2) [![GitHub Stars](https://img.shields.io/github/stars/CN-makers/LongAnimation?style=social)](https://github.com/CN-makers/LongAnimation) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cn-makers.github.io/long_animation_web/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/CNcreator0331/LongAnimation)
- [LLM-based Realistic Safety-Critical Driving Video Generation](http://arxiv.org/abs/2507.01264v1)
- [Geometry-aware 4D Video Generation for Robot Manipulation](http://arxiv.org/abs/2507.01099v1)
- [Populate-A-Scene: Affordance-Aware Human Video Generation](http://arxiv.org/abs/2507.00334v1)
- [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](http://arxiv.org/abs/2507.00162v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://freelongvideo.github.io)
- [Epona: Autoregressive Diffusion World Model for Autonomous Driving](http://arxiv.org/abs/2506.24113v1) [![GitHub Stars](https://img.shields.io/github/stars/Kevin-thu/Epona?style=social)](https://github.com/Kevin-thu/Epona) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kevin-thu.github.io/Epona/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Kevin-thu/Epona)
- [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](http://arxiv.org/abs/2506.23858v1) [![GitHub Stars](https://img.shields.io/github/stars/KwaiVGI/VMoBA?style=social)](https://github.com/KwaiVGI/VMoBA)
- [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](http://arxiv.org/abs/2506.23690v1)
- [Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation](http://arxiv.org/abs/2506.19852v1)
- [GenHSI: Controllable Generation of Human-Scene Interaction Videos](http://arxiv.org/abs/2506.19840v1)
- [SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution](http://arxiv.org/abs/2506.19838v2)
- [Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation](http://arxiv.org/abs/2506.19348v1)
- [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](http://arxiv.org/abs/2506.18903v2) [![GitHub Stars](https://img.shields.io/github/stars/runjiali-rl/vmem?style=social)](https://github.com/runjiali-rl/vmem)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://v-mem.github.io/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/liguang0115/vmem)
- [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](http://arxiv.org/abs/2506.18899v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://filmaster-ai.github.io)
- [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](http://arxiv.org/abs/2506.18655v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://wwenxu.github.io/RDPO)
- [Emergent Temporal Correspondences from Video Diffusion Transformers](http://arxiv.org/abs/2506.17220v2) [![GitHub Stars](https://img.shields.io/github/stars/cvlab-kaist/DiffTrack?style=social)](https://github.com/cvlab-kaist/DiffTrack)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cvlab-kaist.github.io/DiffTrack/)
- [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](http://arxiv.org/abs/2506.17201v1) [![GitHub Stars](https://img.shields.io/github/stars/Tencent-Hunyuan/Hunyuan-GameCraft-1.0?style=social)](https://github.com/Tencent-Hunyuan/Hunyuan-GameCraft-1.0)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://hunyuan-gamecraft.github.io/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/tencent/Hunyuan-GameCraft-1.0)
- [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](http://arxiv.org/abs/2506.16119v1)
- [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](http://arxiv.org/abs/2506.16054v1)
- [Causally Steered Diffusion for Automated Video Counterfactual Generation](http://arxiv.org/abs/2506.14404v2) [![GitHub Stars](https://img.shields.io/github/stars/nysp78/counterfactual-video-generation?style=social)](https://github.com/nysp78/counterfactual-video-generation)
- [VideoMAR: Autoregressive Video Generation with Continuous Tokens](httpxiv.org/abs/2506.14168v2)
- [M4V: Multi-Modal Mamba for Text-to-Video Generation](http://arxiv.org/abs/2506.10915v1) [![GitHub Stars](https://img.shields.io/github/stars/huangjch526/M4V?style=social)](https://github.com/huangjch526/M4V)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://huangjch526.github.io/M4V_project/)
- [GigaVideo‚Äë1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning](http://arxiv.org/abs/2506.10639v1)
- [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](http://arxiv.org/abs/2506.10568v1)
- [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](http://arxiv.org/abs/2506.09350v1)
- [MagCache: Fast Video Generation with Magnitude-Aware Cache](http://arxiv.org/abs/2506.09045v1) [![GitHub Stars](https://img.shields.io/github/stars/Zehong-Ma/MagCache?style=social)](https://github.com/Zehong-Ma/MagCache)   [![Project Page](https://img.shields.io-badge/Project-Page-blue?logo=website)](https://zehong-ma.github.io/MagCache/)
- [Seedance¬†1.0: Exploring the Boundaries of Video Generation Models](http://arxiv.org/abs/2506.09113v2)
- [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](http://arxiv.org/abs/2506.08351v1)
- [Self¬†Forcing: Bridging the Train‚ÄëTest Gap in Autoregressive Video Diffusion](http://arxiv.org/abs/2506.08009v1) [![GitHub Stars](https://img.shields.io/github/stars/guandeh17/Self-Forcing?style=social)](https://github.com/guandeh17/Self-Forcing)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://self-forcing.github.io)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/gdhe17/Self-Forcing)
- [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](http://arxiv.org/abs/2506.07280v2) [![GitHub Stars](https://img.shields.io/github/stars/PabloAcuaaviva/Gen2Gen?style=social)](https://github.com/PabloAcuaaviva/Gen2Gen)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pabloacuaviva.github.io/Gen2Gen/)
- [Frame Guidance: Training‚ÄëFree Guidance for Frame‚ÄëLevel Control in Video Diffusion Models](http://arxiv.org/abs/2506.07177v1)
- [Hi‚ÄëVAE: Efficient Video Autoencoding with Global and Detailed Motion](http://arxiv.org/abs/2506.07136v1)
- [ContentV: Efficient Training of Video Generation Models with Limited Compute](http://arxiv.org/abs/2506.05343v2) [![GitHub Stars](https://img.shields.io/github/stars/bytedance/ContentV?style=social)](https://github.com/bytedance/ContentV)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://contentv.github.io)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ByteDance/ContentV-8B)
- [Astraea: A GPU‚ÄëOriented Token‚Äëwise Acceleration Framework for Video Diffusion Transformers](http://arxiv.org/abs/2506.05096v3)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://astraea-project.github.io/ASTRAEA/)
- [FPSAttention: Training-Aware FP8 and Sparsity Co‚ÄëDesign for Fast Video Diffusion](http://arxiv.org/abs/2506.04648v2)
- [LayerFlow: A Unified Model for Layer‚ÄëAware Video Generation](http://arxiv.org/abs/2506.04228v1) [![GitHub Stars](https://img.shields.io/github/stars/SihuiJi/LayerFlow?style=social)](https://github.com/SihuiJi/LayerFlow)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sihuiji.github.io/LayerFlow-Page/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/zjuJish/LayerFlow)
- [FullDiT2: Efficient In‚ÄëContext Conditioning for Video Diffusion Transformers](http://arxiv.org/abs/2506.04213v2)   [![Project Page](https://img.shields.io-badge/Project-Page-blue?logo=website)](https://fulldit2.github.io/)
- [DenseDPO: Fine‚ÄëGrained Temporal Preference Optimization for Video Diffusion Models](http://arxiv.org/abs/2506.03517v1)
- [Chipmunk: Training‚ÄëFree Acceleration of Diffusion Transformers with Dynamic Column‚ÄëSparse Deltas](http://arxiv.org/abs/2506.03275v1) [![GitHub Stars](https://img.shields.io/github/stars/sandyresearch/chipmunk?style=social)](https://github.com/sandyresearch/chipmunk)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sandyresearch.github.io/chipmunk-part-II/)
- [Context¬†as¬†Memory: Scene‚ÄëConsistent Interactive Long Video Generation with Memory Retrieval](http://arxiv.org/abs/2506.03141v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://context-as-memory.github.io/)
- [CamCloneMaster: Enabling Reference‚Äëbased Camera Control for Video Generation](http://arxiv.org/abs/2506.03140v1)
- [Dual‚ÄëExpert Consistency Model for Efficient and High‚ÄëQuality Video Generation](http://arxiv.org/abs/2506.03123v2) [![GitHub Stars](https://img.shields.io/github/stars/Vchitect/DCM?style=social)](https://github.com/Vchitect/DCM)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vchitect.github.io/DCM/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/cszy98/DCM)
- [Sparse‚ÄëvDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](http://arxiv.org/abs/2506.03065v1) [![GitHub Stars](https://img.shields.io/github/stars/Peyton-Chen/Sparse-vDiT?style=social)](https://github.com/Peyton-Chen/Sparse-vDiT)
- [LumosFlow: Motion‚ÄëGuided Long Video Generation](http://arxiv.org/abs/2506.02497v1)
- [Motion aware video generative model](http://arxiv.org/abs/2506.02244v1)
- [Many‚Äëfor‚ÄëMany: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks](http://arxiv.org/abs/2506.01758v2)
- [OpenS2V‚ÄëNexus: A Detailed Benchmark and Million‚ÄëScale Dataset for Subject‚Äëto‚ÄëVideo Generation](https://arxiv.org/abs/2505.20292)
- [Wan: Open and Advanced Large‚ÄëScale Video Generative Models](http://arxiv.org/abs/2503.20314v2)



</details>

</details>
<details>
<summary><h4>‚ú® 2024</h4></summary>


<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***Vlogger:*** *Make Your Dream A Vlog*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2401.09414.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Vchitect/Vlogger)

*   **[CVPR 2024]** ***Make Pixels Dance:*** *High-Dynamic Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.10982.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://makepixelsdance.github.io/)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://makepixelsdance.github.io/demo.html)

*   **[CVPR 2024]** ***VGen:*** *Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.04483)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://higen-t2v.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/VGen)

*   **[CVPR 2024]** ***GenTron:*** *Delving Deep into Diffusion Transformers for Image and Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.04557)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.shoufachen.com/gentron_website/)

*   **[CVPR 2024]** ***SimDA:*** *Simple Diffusion Adapter for Efficient Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2308.09710.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chenhsing.github.io/SimDA/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ChenHsing/SimDA)

*   **[CVPR 2024]** ***MicroCinema:*** *A Divide-and-Conquer Approach for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.18829)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wangyanhui666.github.io/MicroCinema.github.io/)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://youtube.com/shorts/H7O-Ku_lqPA)

*   **[CVPR 2024]** ***Generative Rendering:*** *Controllable 4D-Guided Video Generation with 2D Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.01409)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://primecai.github.io/generative_rendering/)

*   **[CVPR 2024]** ***PEEKABOO:*** *Interactive Video Generation via Masked-Diffusion*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.07509)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jinga-lala.github.io/projects/Peekaboo/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/microsoft/Peekaboo)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://huggingface.co/spaces/anshuln/peekaboo-demo)

*   **[CVPR 2024]** ***EvalCrafter:*** *Benchmarking and Evaluating Large Video Generation Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.11440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://evalcrafter.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/EvalCrafter/EvalCrafter)

*   **[CVPR 2024]** *A Recipe for Scaling up Text-to-Video Generation with Text-free Videos*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.15770)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tf-t2v.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/damo-vilab/i2vgen-xl)

*   **[CVPR 2024]** ***BIVDiff:*** *A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.02813)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://bivdiff.github.io/)

*   **[CVPR 2024]** ***Mind the Time:*** *Scaled Spatiotemporal Transformers for Text-to-Video Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2402.14797)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapvideo/video_ldm.html)

*   **[CVPR 2024]** ***Animate Anyone:*** *Consistent and Controllable Image-to-video Synthesis for Character Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.17117.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://humanaigc.github.io/animate-anyone/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/AnimateAnyone)

*   **[CVPR 2024]** ***MotionDirector:*** *Motion Customization of Text-to-Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.08465)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/showlab/MotionDirector)

*   **[CVPR 2024]** *Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Skorokhodov_Hierarchical_Patch_Diffusion_Models_for_High-Resolution_Video_Generation_CVPR_2024_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/hpdm/)

*   **[CVPR 2024]** ***DiffPerformer:*** *Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DiffPerformer_Iterative_Learning_of_Consistent_Latent_Guidance_for_Diffusion-based_Human_CVPR_2024_paper.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aipixel/)

*   **[CVPR 2024]** *Grid Diffusion Models for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2404.00234.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/taegyeong-lee/Grid-Diffusion-Models-for-Text-to-Video-Generation)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://taegyeong-lee.github.io/text2video)

*   **[ECCV 2024]** ***Emu Video:*** *Factorizing Text-to-Video Generation by Explicit Image Conditioning*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.10709.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/)

*   **[ECCV 2024]** ***W.A.L.T.:*** *Photorealistic Video Generation with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10270.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://walt-video-diffusion.github.io/)

*   **[ECCV 2024]** ***MoVideo:*** *Motion-Aware Video Generation with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06030.pdf)

*   **[ECCV 2024]** ***DrivingDiffusion:*** *Layout-Guided Multi-View Driving Scenarios Video Generation with Latent Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10097.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://drivingdiffusion.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/shalfun/DrivingDiffusion)

*   **[ECCV 2024]** ***MagDiff:*** *Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02738.pdf)

*   **[ECCV 2024]** ***HARIVO:*** *Harnessing Text-to-Image Models for Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06938.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kwonminki.github.io/HARIVO/)

*   **[ECCV 2024]** ***MEVG:*** *Multi-event Video Generation with Text-to-Video Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06012.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kuai-lab.github.io/eccv2024mevg/)

*   **[NeurIPS 2024]** ***DEMO:*** *Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/81f19c0e9f3e06c831630ab6662fd8ea-Paper-Conference.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PR-Ryan/DEMO)

*   **[ICML 2024]** ***Video-LaVIT:*** *Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S9lk6dk4LL)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-lavit.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/LaVIT)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/Video-LaVIT-v1)

*   **[ICLR 2024]** ***VDT:*** *General-purpose Video Diffusion Transformers via Mask Modeling*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2305.13311.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://vdt-2023.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RERV/VDT)

*   **[ICLR 2024]** ***VersVideo:*** *Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf?id=K9sVJ17zvB)

*   **[AAAI 2024]** ***Follow Your Pose:*** *Pose-Guided Text-to-Video Generation using Pose-Free Videos*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2304.01186)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://follow-your-pose.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mayuelala/FollowYourPose)

*   **[AAAI 2024]** ***E2HQV:*** *High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2401.08117)

*   **[AAAI 2024]** ***ConditionVideo:*** *Training-Free Condition-Guided Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.07697)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pengbo807.github.io/conditionvideo-website/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pengbo807/ConditionVideo)

*   **[AAAI 2024]** ***F3-Pruning:*** *A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text to-Video Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.03459)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Gender Bias in Text-to-Video Generation Models: A case study of Sora](http://arxiv.org/abs/2501.01987v2)
- [VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation](http://arxiv.org/abs/2412.21059v2)
- [Follow-Your-MultiPose: Tuning-Free Multi-Character Text-to-Video Generation via Pose Guidance](http://arxiv.org/abs/2412.16495v2)
- [CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training](http://arxiv.org/abs/2412.15646v2)
- [DirectorLLM for Human-Centric Video Generation](http://arxiv.org/abs/2412.14484v1)
- [Can Video Generation Replace Cinematographers? Research on the Cinematic Language of Generated Video](http://arxiv.org/abs/2412.12223v2)
- [LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity](http://arxiv.org/abs/2412.09856v2)
- [T-SVG: Text-Driven Stereoscopic Video Generation](http://arxiv.org/abs/2412.09323v2)
- [Mojito: Motion Trajectory and Intensity Control for Video Generation](http://arxiv.org/abs/2412.08948v2)
- [SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints](http://arxiv.org/abs/2412.07760v1)
- [Motion by Queries: Identity-Motion Trade-offs in Text-to-Video Generation](http://arxiv.org/abs/2412.07750v3)
- [STIV: Scalable Text and Image Conditioned Video Generation](http://arxiv.org/abs/2412.07730v1)
- [GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration](http://arxiv.org/abs/2412.04440v1)
- [Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback](http://arxiv.org/abs/2412.02617v1)
- [CPA: Camera-pose-awareness Diffusion Transformer for Video Generation](http://arxiv.org/abs/2412.01429v1)
- [MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation](http://arxiv.org/abs/2411.18281v2)
- [Scene Co-pilot: Procedural Text to Video Generation with Human in the Loop](http://arxiv.org/abs/2411.18644v1)
- [Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models](http://arxiv.org/abs/2411.17041v1)
- [DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation](http://arxiv.org/abs/2411.16657v3)
- [InTraGen: Trajectory-controlled Video Generation for Object Interactions](http://arxiv.org/abs/2411.16804v1)
- [Optical-Flow Guided Prompt Optimization for Coherent Video Generation](http://arxiv.org/abs/2411.15540v2)
- [VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement](http://arxiv.org/abs/2411.15115v2)
- [Motion Control for Enhanced Complex Action Video Generation](http://arxiv.org/abs/2411.08328v1)
- [GameGen-X: Interactive Open-world Game Video Generation](http://arxiv.org/abs/2411.00769v3)
- [Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning](http://arxiv.org/abs/2410.24219v1)
- [ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation](http://arxiv.org/abs/2410.20502v3)
- [Animating the Past: Reconstruct Trilobite via Video Generation](http://arxiv.org/abs/2410.14715v1)
- [ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way](http://arxiv.org/abs/2410.06241v3)
- [T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design](http://arxiv.org/abs/2410.05677v2)
- [The Dawn of Video Generation: Preliminary Explorations with SORA-like Models](http://arxiv.org/abs/2410.05227v2)
- [Compositional 3D-aware Video Generation with LLM Director](http://arxiv.org/abs/2409.00558v1)
- [Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation](http://arxiv.org/abs/2408.10453v2)
- [FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance](http://arxiv.org/abs/2408.08189v4)
- [Still-Moving: Customized Video Generation without Customized Video Data](http://arxiv.org/abs/2407.08674v1)
- [VEnhancer: Generative Space-Time Enhancement for Video Generation](http://arxiv.org/abs/2407.07667v1)
- [Mobius: A High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task](http://arxiv.org/abs/2407.06617v4)
- [VIMI: Grounding Video Generation through Multi-modal Instruction](http://arxiv.org/abs/2407.06304v1)
- [GVDIFF: Grounded Text-to-Video Generation with Diffusion Models](http://arxiv.org/abs/2407.01921v2)
- [Evaluation of Text-to-Video Generation Models: A Dynamics Perspective](http://arxiv.org/abs/2407.01094v1)
- [Text-Animator: Controllable Visual Text Video Generation](http://arxiv.org/abs/2406.17777v1)
- [MotionBooth: Motion-Aware Customized Text-to-Video Generation](http://arxiv.org/abs/2406.17758v3)
- [Hierarchical Patch Diffusion Models for High-Resolution Video Generation](http://arxiv.org/abs/2406.07792v1)
- [Compositional Video Generation as Flow Equalization](http://arxiv.org/abs/2407.06182v1)
- [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](http://arxiv.org/abs/2406.05338v6)
- [VideoTetris: Towards Compositional Text-to-Video Generation](http://arxiv.org/abs/2406.04277v2)
- [VideoPhy: Evaluating Physical Commonsense for Video Generation](http://arxiv.org/abs/2406.03520v2)
- [I4VGen: Image as Free Stepping Stone for Text-to-Video Generation](http://arxiv.org/abs/2406.02230v2)
- [DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control](http://arxiv.org/abs/2405.12796v1)
- [The Lost Melody: Empirical Observations on Text-to-Video Generation From A Storytelling Perspective](http://arxiv.org/abs/2405.08720v1)
- [TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation](http://arxiv.org/abs/2405.04682v4)
- [MotionMaster: Training-free Camera Motion Transfer For Video Generation](http://arxiv.org/abs/2404.15789v2)
- [ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion Model](http://arxiv.org/abs/2404.12903v1)
- [MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators](http://arxiv.org/abs/2404.05014v2)
- [CameraCtrl: Enabling Camera Control for Text-to-Video Generation](http://arxiv.org/abs/2404.02101v2)
- [Grid Diffusion Models for Text-to-Video Generation](http://arxiv.org/abs/2404.00234v2)
- [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](http://arxiv.org/abs/2403.14773v2)
- [S2DM: Sector-Shaped Diffusion Models for Video Generation](http://arxiv.org/abs/2403.13408v2)
- [Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](http://arxiv.org/abs/2403.13248v3)



</details>


</details>


<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2023]** ***Align your Latents:*** *High-resolution Video Synthesis with Latent Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2304.08818.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/srpkdyy/VideoLDM)

*   **[CVPR 2023]** ***Text2Video-Zero:*** *Text-to-image Diffusion Models are Zero-shot Video Generators*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://text2video-zero.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Text2Video-Zero)
    [![Demo](https://img.shields.io/badge/Hugging_Face-Demo-yellow?style=for-the-badge)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)

*   **[CVPR 2023]** *Video Probabilistic Diffusion Models in Projected Latent Space*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sihyun-yu/PVDM)

*   **[ICCV 2023]** ***PYOCO:*** *Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/dir/pyoco/)

*   **[ICCV 2023]** ***Gen-1:*** *Structure and Content-guided Video Synthesis with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.runwayml.com/gen1)

*   **[NeurIPS 2023]** *Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2204.03458.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-diffusion.github.io/)

*   **[NeurIPS 2023]** ***UniPi:*** *Learning Universal Policies via Text-Guided Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://papers.nips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://universal-policy.github.io/unipi/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/flow-diffusion/AVDC)

*   **[NeurIPS 2023]** ***VideoComposer:*** *Compositional Video Synthesis with Motion Controllability*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2306.02018.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://videocomposer.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/videocomposer)

*   **[ICLR 2023]** ***CogVideo:*** *Large-scale Pretraining for Text-to-video Generation via Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf?id=rB6TpjAuSRy)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/CogVideo)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://models.aminer.cn/cogvideo/)

*   **[ICLR 2023]** ***Make-A-Video:*** *Text-to-video Generation without Text-video Data*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2209.14792.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://makeavideo.studio/)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/lucidrains/make-a-video-pytorch)

*   **[ICLR 2023]** ***Phenaki:*** *Variable Length Video Generation From Open Domain Textual Description*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf/fe8e106a2746992c9c2e658bdc8cb9c89cc5a39a.pdf)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/lucidrains/phenaki-pytorch)



</details>




<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [FlashVideo: A Framework for Swift Inference in Text-to-Video Generation](http://arxiv.org/abs/2401.00869v1)
- [A Recipe for Scaling up Text-to-Video Generation with Text-free Videos](http://arxiv.org/abs/2312.15770v1)
- [Photorealistic Video Generation with Diffusion Models](http://arxiv.org/abs/2312.06662v1)
- [GenTron: Diffusion Transformers for Image and Video Generation](http://arxiv.org/abs/2312.04557v2)
- [Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation](http://arxiv.org/abs/2312.04483v1)
- [StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter](http://arxiv.org/abs/2312.00330v2)
- [ART$\boldsymbol{\cdot}$V: Auto-Regressive Text-to-Video Generation with Diffusion Models](http://arxiv.org/abs/2311.18834v1)
- [MotionZero: Exploiting Motion Priors for Zero-shot Text-to-Video Generation](http://arxiv.org/abs/2311.16635v1)
- [FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline](http://arxiv.org/abs/2311.13073v2)
- [GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning](http://arxiv.org/abs/2311.12631v3)
- [Make Pixels Dance: High-Dynamic Video Generation](http://arxiv.org/abs/2311.10982v1)
- [VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning on Language-Video Foundation Models](http://arxiv.org/abs/2311.00990v2)
- [POS: A Prompts Optimization Suite for Augmenting Text-to-Video Generation](http://arxiv.org/abs/2311.00949v3)
- [VideoCrafter1: Open Diffusion Models for High-Quality Video Generation](http://arxiv.org/abs/2310.19512v1)
- [LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation](http://arxiv.org/abs/2310.10769v1)
- [Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation](http://arxiv.org/abs/2309.16429v1)
- [Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation](http://arxiv.org/abs/2309.15818v3)
- [LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models](http://arxiv.org/abs/2309.15103v2)
- [VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning](http://arxiv.org/abs/2309.15091v2)
- [Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation](http://arxiv.org/abs/2309.03549v1)
- [VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation](http://arxiv.org/abs/2309.00398v2)
- [Dual-Stream Diffusion Net for Text-to-Video Generation](http://arxiv.org/abs/2308.08316v3)
- [Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation](http://arxiv.org/abs/2307.06940v1)
- [Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance](http://arxiv.org/abs/2306.00943v1)
- [DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation](http://arxiv.org/abs/2305.14330v3)
- [ControlVideo: Training-free Controllable Text-to-Video Generation](http://arxiv.org/abs/2305.13077v1)
- [Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](http://arxiv.org/abs/2305.10874v4)


</details>

</details>


[<small>‚áß Back to ToC</small>](#contents)

### <span id="i2v">üñºÔ∏è Image-to-Video (I2V) Generation</span>



<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***MotionStone:*** *Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.05848)

*   **[CVPR 2025]** ***MotionPro:*** *A Precise Motion Controller for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2503.00948)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/HiDream-ai/MotionPro)

*   **[CVPR 2025]** ***Through-The-Mask:*** *Mask-based Motion Trajectories for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.03059)

*   **[CVPR 2025]** *Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2503.00948)

*   **[CVPR 2025]** ***I2VGuard:*** *Safeguarding Images against Misuse in Diffusion-based Image-to-Video Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gui_I2VGuard_Safeguarding_Images_against_Misuse_in_Diffusion-based_Image-to-Video_Models_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***LeviTor:*** *3D Trajectory Oriented Image-to-Video Synthesis*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/ant-research/LeviTor)

*   **[ICCV 2025]** ***AnyI2V:*** *Animating Any Conditional Image with Motion Control*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2507.02857) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/FudanCVL/AnyI2V)

*   **[ICCV 2025]** *Versatile Transition Generation with Image-to-Video Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2508.01698)



*   **[ICLR 2025]** ***SG-I2V:*** *Self-Guided Trajectory Control in Image-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=uQjySppU9x)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kmcode1.github.io/Projects/SG-I2V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Kmcode1/SG-I2V)

*   **[ICLR 2025]** *Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ykD8a9gJvy)

*   **[ICLR 2025]** *Pyramidal Flow Matching for Efficient Video Generative Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=66NzcRQuOq)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pyramid-flow.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/Pyramid-Flow)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/pyramid-flow-miniflux)



   
</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

*   [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](http://arxiv.org/abs/2507.06830v1)
*   [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](http://arxiv.org/abs/2506.08456v1)
*   [Frame In-N-Out: Unbounded Controllable Image-to-Video Generation](http://arxiv.org/abs/2505.21491v1)
*   [Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM](http://arxiv.org/abs/2505.19901v3)
*   [Order Matters: On Parameter-Efficient Image-to-Video Probing for Recognizing Nearly Symmetric Actions](http://arxiv.org/abs/2503.24298v1)
*   [EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation](http://arxiv.org/abs/2503.18552v2)
*   [Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model](http://arxiv.org/abs/2503.11251v1)
*   [DreamInsert: Zero-Shot Image-to-Video Object Insertion from A Single Image](http://arxiv.org/abs/2503.10342v1)
*   [I2V3D: Controllable image-to-video generation with 3D guidance](http://arxiv.org/abs/2503.09733v1)
*   [Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think](http://arxiv.org/abs/2503.00948v1)
*   [Object-Centric Image to Video Generation with Language Guidance](http://arxiv.org/abs/2502.11655v1)
*   [VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation](http://arxiv.org/abs/2502.07531v3)
*   [MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation](http://arxiv.org/abs/2502.04299v1)
*   [Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation](http://arxiv.org/abs/2501.03059v1)

</details>

</details>


<details>
<summary><h4>‚ú® 2024</h4></summary>




<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** *Your Image Is My Video: Reshaping the Receptive Field via Image-to-Video Differentiable AutoAugmentation and Fusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00557)

*   **[CVPR 2024]** ***TRIP:*** *Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00828)

*   **[CVPR 2024]** *Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.01751)

*   **[CVPR 2024]** *Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00779) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/AnimateAnyone)

*   **[ECCV 2024]** ***MOFA-Video:*** *Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72655-2_7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MyNiuuu/MOFA-Video)

*   **[ECCV 2024]** *$\mathrm R2$-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72940-9_24) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yeliudev/R2-Tuning)

*   **[ECCV 2024]** ***PhysGen:*** *Rigid-Body Physics-Grounded Image-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73007-8_21) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/stevenlsw/physgen)

*   **[ECCV 2024]** *Rethinking Image-to-Video Adaptation: An Object-Centric Perspective*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72775-7_19)

*   **[NeurIPS 2024]** ***TPC:*** *Test-time Procrustes Calibration for Diffusion-based Human Image Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/d6fdc3002dda4c3cad3d595ac6fa5352-Abstract-Conference.html)

*   **[NeurIPS 2024]** *Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/35cb54b887e7aafe74829677cce6c5c6-Abstract-Conference.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/cond-image-leakage)

*   **[ICML 2024]** ***Video-LaVIT:*** *Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S9lk6dk4LL)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-lavit.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/LaVIT)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/Video-LaVIT-v1)

*   **[SIGGRAPH 2024]** ***I2V-Adapter:*** *A General Image-to-Video Adapter for Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1145/3641519.3657407) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/KwaiVGI/I2V-Adapter)


*   **[SIGGRAPH 2024]** ***Motion-I2V:*** *Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1145/3641519.3657497) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/G-U-N/Motion-I2V)

*   **[AAAI 2024]** *Continuous Piecewise-Affine Based Motion Model for Image Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v38i6.28351) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/whx-sjtu/AAAI2024-CPABMM)



</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



- [OmniDrag: Enabling Motion Control for Omnidirectional Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2412.09623v1) [![GitHub Stars](https://img.shields.io/github/stars/lwq20020127/OmniDrag?style=social)](https://github.com/lwq20020127/OmniDrag)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://lwq20020127.github.io/OmniDrag/)
- [CamI2V: Camera‚ÄëControlled Image‚Äëto‚ÄëVideo Diffusion Model](http://arxiv.org/abs/2410.15957v3) [![GitHub Stars](https://img.shields.io/github/stars/ZGCTroy/CamI2V?style=social)](https://github.com/ZGCTroy/CamI2V)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://zgctroy.github.io/CamI2V/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/MuteApo/CamI2V)
- [FrameBridge: Improving Image‚Äëto‚ÄëVideo Generation with Bridge Models](http://arxiv.org/abs/2410.15371v2) [![GitHub Stars](https://img.shields.io/github/stars/thu-ml/FrameBridge?style=social)](https://github.com/thu-ml/FrameBridge)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://framebridgei2v.github.io/)
- [Identifying and Solving Conditional Image Leakage in Image‚Äëto‚ÄëVideo Diffusion Model](http://arxiv.org/abs/2406.15735v3) [![GitHub Stars](https://img.shields.io/github/stars/thu-ml/cond-image-leakage?style=social)](https://github.com/thu-ml/cond-image-leakage)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cond-image-leak.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/Xiang-cd/DynamiCrafter-CIL)
- [CamCo: Camera‚ÄëControllable 3D‚ÄëConsistent Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2406.02509v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://ir1d.github.io/CamCo/)
- [CamViG: Camera Aware Image‚Äëto‚ÄëVideo Generation with Multimodal Transformers](http://arxiv.org/abs/2405.13195v1)
- [$R^2$‚ÄëTuning: Efficient Image‚Äëto‚ÄëVideo Transfer Learning for Video Temporal Grounding](http://arxiv.org/abs/2404.00801v2) [![GitHub Stars](https://img.shields.io/github/stars/yeliudev/R2-Tuning?style=social)](https://github.com/yeliudev/R2-Tuning)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/yeliudev/R2-Tuning)
- [TRIP: Temporal Residual Learning with Image Noise Prior for Image‚Äëto‚ÄëVideo Diffusion Models](http://arxiv.org/abs/2403.17005v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://trip-i2v.github.io/TRIP/)
- [Your Image is My Video: Reshaping the Receptive Field via Image‚ÄëTo‚ÄëVideo Differentiable AutoAugmentation and Fusion](http://arxiv.org/abs/2403.15194v1)
- [Tuning‚ÄëFree Noise Rectification for High Fidelity Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2403.02827v1) [![GitHub Stars](https://img.shields.io/github/stars/alimama-creative/Noise-Rectification?style=social)](https://github.com/alimama-creative/Noise-Rectification)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://noise-rectification.github.io/)
- [AtomoVideo: High Fidelity Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2403.01800v2) [![GitHub Stars](https://img.shields.io/github/stars/atomo-video/atomo-video.github.io?style=social)](https://github.com/atomo-video/atomo-video.github.io)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://atomo-video.github.io/)
- [ConsistI2V: Enhancing Visual Consistency for Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2402.04324v2) [![GitHub Stars](https://img.shields.io/github/stars/TIGER-AI-Lab/ConsistI2V?style=social)](https://github.com/TIGER-AI-Lab/ConsistI2V)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://tiger-ai-lab.github.io/ConsistI2V/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/TIGER-Lab/ConsistI2V)
- [AIGCBench: Comprehensive Evaluation of Image‚Äëto‚ÄëVideo Content Generated by AI](http://arxiv.org/abs/2401.01651v3) [![GitHub Stars](https://img.shields.io/github/stars/BenchCouncil/AIGCBench?style=social)](https://github.com/BenchCouncil/AIGCBench) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/stevenfan/AIGCBench_v1.0)


</details>




<summary><h4>‚ú® 2023</h4></summary>


<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[ICCV 2023]** ***DreamPose:*** *Fashion Image-to-Video Synthesis via Stable Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.02073) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/johannakarras/DreamPose)

*   **[ICCV 2023]** *Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.01281) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba-mmai-research/DiST)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance](http://arxiv.org/abs/2312.03018v4)
*   [Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning](http://arxiv.org/abs/2311.15769v1)



</details>

</details>


[<small>‚áß Back to ToC</small>](#contents)

### <span id="v2v">‚úÇÔ∏è Video-to-Video (V2V) Editing</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** ***VideoHandles:*** *Editing 3D Object Compositions in Videos Using Video Generative Priors*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Koo_VideoHandles_Editing_3D_Object_Compositions_in_Videos_Using_Video_Generative_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***VideoDirector:*** *Precise Video Editing via Text-to-Video Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoDirector_Precise_Video_Editing_via_Text-to-Video_Models_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Yukun66/Video_Director)

*   **[CVPR 2025]** ***VideoSPatS:*** *Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gonzalez_VideoSPatS_Video_SPatiotemporal_Splines_for_Disentangled_Occlusion_Appearance_and_Motion_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***Align-A-Video:*** *Deterministic Reward Tuning of Image Diffusion Models for Consistent Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Align-A-Video_Deterministic_Reward_Tuning_of_Image_Diffusion_Models_for_Consistent_CVPR_2025_paper.html)

*   **[CVPR 2025]** *Unity in Diversity: Video Editing via Gradient-Latent Purification*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unity_in_Diversity_Video_Editing_via_Gradient-Latent_Purification_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***VEU-Bench:*** *Towards Comprehensive Understanding of Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/littlespray/VE-Bench)

*   **[CVPR 2025]** ***SketchVideo:*** *Sketch-based Video Generation and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SketchVideo_Sketch-based_Video_Generation_and_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/IGLICT/SketchVideo)

*   **[CVPR 2025]** ***FATE:*** *Full-head Gaussian Avatar with Textural Editing from Monocular Video*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FATE_Full-head_Gaussian_Avatar_with_Textural_Editing_from_Monocular_Video_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/zjwfufu/FateAvatar)

*   **[CVPR 2025]** *Visual Prompting for One-shot Controllable Video Editing without Inversion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Visual_Prompting_for_One-shot_Controllable_Video_Editing_without_Inversion_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***FADE:*** *Frequency-Aware Diffusion Model Factorization for Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_FADE_Frequency-Aware_Diffusion_Model_Factorization_for_Video_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/EternalEvan/FADE)

*   **[ICCV 2025]** ***VACE:*** *All-in-One Video Creation and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.07598)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ali-vilab.github.io/VACE-Page/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/VACE)

*   **[ICCV 2025]** ***Reangle-A-Video:*** *4D Video Generation as Video-to-Video Translation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.09151)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://anony1anony2.github.io/)

*   **[ICCV 2025]** ***DIVE:*** *Taming DINO for Subject-Driven Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03347)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dino-video-editing.github.io/)

*   **[ICCV 2025]** **DynamicFace:** *High-Quality and Consistent Face Swapping for Image and Video using Composable 3D Facial Priors*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1368)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dynamic-face.github.io/)

*   **[ICCV 2025]** ***QK-Edit:*** *Revisiting Attention-based Injection in MM-DiT for Image and Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/215)

*   **[ICCV 2025]** ***Teleportraits:*** *Training-Free People Insertion into Any Scene*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/809)

*   **[ICLR 2025]** ***VideoGrain:*** *Modulating Space-Time Attention for Multi-Grained Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=SSslAtcPB6)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/knightyxp/VideoGrain)

*   **[AAAI 2025]** ***FreeMask:*** *Rethinking the Importance of Attention Masks for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i2.32185) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/LinglingCai0314/FreeMask)

*   **[AAAI 2025]** ***EditBoard:*** *Towards a Comprehensive Evaluation Benchmark for Text-Based Video Editing Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i15.33754) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/Samchen2003/EditBoard)


*   **[AAAI 2025]** ***VE-Bench:*** *Subjective-Aligned Benchmark Suite for Text-Driven Video Editing Quality Assessment*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i7.32763) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/littlespray/VE-Bench)

*   **[AAAI 2025]** *Re-Attentional Controllable Video Diffusion Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i8.32876) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/mdswyz/ReAtCo)

*   **[WACV 2025]** ***IP-FaceDiff:*** *Identity-Preserving Facial Video Editing with Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACVW65960.2025.00031)

*   **[WACV 2025]** ***SST-EM:*** *Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACVW65960.2025.00032) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git)

*   **[WACV 2025]** ***MagicStick:*** *Controllable Video Editing via Control Handle Transformations*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00909)

*   **[WACV 2025]** ***Ada-VE:*** *Training-Free Consistent Video Editing Using Adaptive Motion Prior*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00101)

*   **[WACV 2025]** ***FastVideoEdit:*** *Leveraging Consistency Models for Efficient Text-to-Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00360) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/youyuan-zhang/FastVideoEdit)




</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>


- [Consistent Video Editing as Flow-Driven Image-to-Video Generation](http://arxiv.org/abs/2506.07713v2)
- [UNIC: Unified In-Context Video Editing](http://arxiv.org/abs/2506.04216v1)
- [DreamVE: Unified Instruction-based Image and Video Editing](http://arxiv.org/abs/2508.06080v1)
- [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](http://arxiv.org/abs/2508.00299v1)
- [Low-Cost Test-Time Adaptation for Robust Video Editing](http://arxiv.org/abs/2507.21858v1)
- [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](http://arxiv.org/abs/2507.02790v1)
- [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](http://arxiv.org/abs/2506.22868v1)
- [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](http://arxiv.org/abs/2506.22432v1)
- [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](http://arxiv.org/abs/2506.20967v2)
- [Good Noise Makes Good Edits: A Training-Free Diffusion-Based Video Editing with Image and Text Prompts](http://arxiv.org/abs/2506.12520v1)
- [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](http://arxiv.org/abs/2506.10082v3)
- [TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation](http://arxiv.org/abs/2506.07205v1)
- [FADE: Frequency-Aware Diffusion Model Factorization for Video Editing](http://arxiv.org/abs/2506.05934v1)
- [FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing](http://arxiv.org/abs/2506.05046v1)
- [FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers](http://arxiv.org/abs/2506.04213v2)
- [OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation](http://arxiv.org/abs/2506.01801v1)
- [Motion-Aware Concept Alignment for Consistent Video Editing](http://arxiv.org/abs/2506.01004v1)
- [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](http://arxiv.org/abs/2505.23134v1)
- [REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing](http://arxiv.org/abs/2505.18880v1)
- [From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations](http://arxiv.org/abs/2505.12237v1)
- [DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models](http://arxiv.org/abs/2505.07057v1)
- [Photoshop Batch Rendering Using Actions for Stylistic Video Editing](http://arxiv.org/abs/2505.01001v1)
- [Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework](http://arxiv.org/abs/2504.16016v1)
- [Vidi: Large Multimodal Models for Video Understanding and Editing](http://arxiv.org/abs/2504.15681v3)
- [Visual Prompting for One-shot Controllable Video Editing without Inversion](http://arxiv.org/abs/2504.14335v1)
- [CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models](http://arxiv.org/abs/2504.09472v1)
- [VideoSPatS: Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing](http://arxiv.org/abs/2504.07146v1)
- [Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology-Inspired Computing Methods](http://arxiv.org/abs/2503.17975v2)
- [InstructVEdit: A Holistic Approach for Instructional Video Editing](http://arxiv.org/abs/2503.17641v1)
- [HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks](http://arxiv.org/abs/2503.17276v1)
- [VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation](http://arxiv.org/abs/2503.14350v2)
- [GIFT: Generated Indoor video frames for Texture-less point tracking](http://arxiv.org/abs/2503.12944v1)
- [RASA: Replace Anyone, Say Anything -- A Training-Free Framework for Audio-Driven and Universal Portrait Video Editing](http://arxiv.org/abs/2503.11571v1)
- [V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes](http://arxiv.org/abs/2503.10634v2)
- [Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space](http://arxiv.org/abs/2503.09419v1)
- [VACE: All-in-One Video Creation and Editing](http://arxiv.org/abs/2503.07598v2)
- [Get In Video: Add Anything You Want to the Video](http://arxiv.org/abs/2503.06268v1)
- [VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control](http://arxiv.org/abs/2503.05639v3)
- [VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing](http://arxiv.org/abs/2502.17258v1)
- [VideoDiff: Human-AI Video Co-Creation with Alternatives](http://arxiv.org/abs/2502.10190v1)
- [SportsBuddy: Designing and Evaluating an AI-Powered Sports Video Storytelling Tool Through Real-World Deployment](http://arxiv.org/abs/2502.08621v2)
- [AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection](http://arxiv.org/abs/2502.05433v1)
- [MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation](http://arxiv.org/abs/2502.04299v1)
- [SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing](http://arxiv.org/abs/2501.07554v1)
- [IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion](http://arxiv.org/abs/2501.07530v1)
- [Qffusion: Controllable Portrait Video Editing via Quadrant-Grid Attention Learning](http://arxiv.org/abs/2501.06438v3)
- [Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal LLMs](http://arxiv.org/abs/2501.05884v1)
- [Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion](http://arxiv.org/abs/2501.04606v4)
- [Edit as You See: Image-guided Video Editing via Masked Motion Modeling](http://arxiv.org/abs/2501.04325v1)


</details>
</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** *A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00719) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/STEM-Inv/stem-inv)

*   **[CVPR 2024]** ***VidToMe:*** *Video Token Merging for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00715) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/VISION-SJTU/VidToMe)

*   **[CVPR 2024]** ***Video-P2P:*** *Video Editing with Cross-Attention Control*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00821) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/dvlab-research/Video-P2P)

*   **[CVPR 2024]** ***CCEdit:*** *Creative and Controllable Video Editing via Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00641)

*   **[CVPR 2024]** ***RAVE:*** *Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00622) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/rehglab/RAVE)

*   **[CVPR 2024]** ***DynVideo-E:*** *Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00732) 

*   **[CVPR 2024]** ***MaskINT:*** *Video Editing via Interpolative Non-autoregressive Masked Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00707)

*   **[CVPR 2024]** ***MotionEditor:*** *Editing Video Motion via Content-Aware Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00753) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Francis-Rings/MotionEditor)

*   **[CVPR 2024]** ***CAMEL:*** *CAusal Motion Enhancement Tailored for Lifting Text-Driven Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00867) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/zhangguiwei610/CAMEL)

*   **[ICLR 2024]** ***Ground-A-Video:*** *Zero-shot Grounded Video Editing using Text-to-image Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=28L2FCtMWq) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Ground-A-Video/Ground-A-Video)

*   **[ICLR 2024]** *Video Decomposition Prior: Editing Videos Layer by Layer*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=nfMyERXNru)

*   **[ICLR 2024]** ***FLATTEN:*** *optical FLow-guided ATTENtion for consistent text-to-video editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=JgqftqZQZ7)

*   **[ICLR 2024]** ***TokenFlow:*** *Consistent Diffusion Features for Consistent Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=lKK50q2MtV) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/omerbt/TokenFlow)

*   **[ECCV 2024]** ***VIDEOSHOP:*** *Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73254-6_14) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sfanxiang/videoshop)

*   **[ECCV 2024]** ***DragVideo:*** *Interactive Drag-Style Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72992-8_11) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RickySkywalker/DragVideo-Official)

*   **[ECCV 2024]** ***WAVE:*** *Warping DDIM Inversion Features for Zero-Shot Text-to-Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73116-7_3)

*   **[ECCV 2024]** ***DreamMotion:*** *Space-Time Self-similar Score Distillation for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73404-5_21)

*   **[ECCV 2024]** *Object-Centric Diffusion for Efficient Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72998-0_6) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Qualcomm-AI-research/object-centric-diffusion)

*   **[ECCV 2024]** *Video Editing via Factorized Diffusion Distillation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73116-7_26)

*   **[ECCV 2024]** ***SAVE:*** *Protagonist Diversification with Structure Agnostic Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72989-8_3)

*   **[ECCV 2024]** ***DNI:*** *Dilutional Noise Initialization for Diffusion Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73195-2_11)

*   **[ECCV 2024]** ***MagDiff:*** *Multi-alignment Diffusion for High-Fidelity Video Generation and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72649-1_12)

*   **[ECCV 2024]** ***DeCo:*** *Decoupled Human-Centered Diffusion Video Editing with Motion Consistency*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72784-9_20)





</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>


- [MAKIMA: Tuning-free Multi-Attribute Open-domain Video Editing via Mask-Guided Attention Modulation](http://arxiv.org/abs/2412.19978v1)
- [DriveEditor: A Unified 3D Information-Guided Framework for Controllable Object Editing in Driving Scenes](http://arxiv.org/abs/2412.19458v2)
- [Re-Attentional Controllable Video Diffusion Editing](http://arxiv.org/abs/2412.11710v1)
- [MoViE: Mobile Diffusion for Video Editing](http://arxiv.org/abs/2412.06578v1)
- [DIVE: Taming DINO for Subject-Driven Video Editing](http://arxiv.org/abs/2412.03347v2)
- [Trajectory Attention for Fine-grained Video Motion Control](http://arxiv.org/abs/2411.19324v1)
- [VideoDirector: Precise Video Editing via Text-to-Video Models](http://arxiv.org/abs/2411.17592v3)
- [StableV2V: Stablizing Shape Consistency in Video-to-Video Editing](http://arxiv.org/abs/2411.11045v1)
- [OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models](http://arxiv.org/abs/2411.10501v1)
- [A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model](http://arxiv.org/abs/2411.04942v1)
- [Taming Rectified Flow for Inversion and Editing](http://arxiv.org/abs/2411.04746v3)
- [AutoVFX: Physically Realistic Video Editing from Natural Language Instructions](http://arxiv.org/abs/2411.02394v1)
- [Shaping a Stabilized Video by Mitigating Unintended Changes for Concept-Augmented Video Editing](http://arxiv.org/abs/2410.12526v2)
- [RNA: Video Editing with ROI-based Neural Atlas](http://arxiv.org/abs/2410.07600v1)
- [FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing](http://arxiv.org/abs/2409.20500v1)
- [DNI: Dilutional Noise Initialization for Diffusion Video Editing](http://arxiv.org/abs/2409.13037v1)
- [Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514v1)
- [DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion Consistency](http://arxiv.org/abs/2408.07481v1)
- [InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models](http://arxiv.org/abs/2407.10958v1)
- [MVOC: A Training-Free Multiple Video Object Composition Method with Diffusion Models](http://arxiv.org/abs/2406.15829v1)
- [VIA: Unified Spatiotemporal Video Adaptation Framework for Global and Local Video Editing](http://arxiv.org/abs/2406.12831v3)
- [COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing](http://arxiv.org/abs/2406.08850v2)
- [NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing](http://arxiv.org/abs/2406.06523v2)
- [FRAG: Frequency Adapting Group for Diffusion Video Editing](http://arxiv.org/abs/2406.06044v2)
- [Zero-Shot Video Editing through Adaptive Sliding Score Distillation](http://arxiv.org/abs/2406.04888v2)
- [Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion Prior](http://arxiv.org/abs/2406.04873v2)
- [Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting](http://arxiv.org/abs/2406.02541v4)
- [Temporally Consistent Object Editing in Videos using Extended Attention](http://arxiv.org/abs/2406.00272v1)
- [MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion](http://arxiv.org/abs/2405.20325v1)
- [Streaming Video Diffusion: Online Video Editing with Diffusion Models](http://arxiv.org/abs/2405.19726v1)
- [I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models](http://arxiv.org/abs/2405.16537v1)
- [ReVideo: Remake a Video with Motion and Content Control](http://arxiv.org/abs/2405.13865v1)
- [Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices](http://arxiv.org/abs/2405.12211v1)
- [GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models](http://arxiv.org/abs/2404.12541v1)
- [Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model](http://arxiv.org/abs/2404.09967v2)
- [S3Editor: A Sparse Semantic-Disentangled Self-Training Framework for Face Video Editing](http://arxiv.org/abs/2404.08111v1)
- [ExpressEdit: Video Editing with Natural Language and Sketching](http://arxiv.org/abs/2403.17693v1)
- [EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing](http://arxiv.org/abs/2403.16111v1)
- [Edit3K: Universal Representation Learning for Video Editing Components](http://arxiv.org/abs/2403.16048v2)
- [Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion](http://arxiv.org/abs/2403.14617v3)
- [AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks](http://arxiv.org/abs/2403.14468v4)
- [DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video Editing](http://arxiv.org/abs/2403.12002v2)
- [EffiVED: Efficient Video Editing via Text-instruction Diffusion Models](http://arxiv.org/abs/2403.11568v2)
- [AICL: Action In-Context Learning for Video Diffusion Model](http://arxiv.org/abs/2403.11535v2)
- [Video Editing via Factorized Diffusion Distillation](http://arxiv.org/abs/2403.09334v2)
- [VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis](http://arxiv.org/abs/2403.08764v1)
- [FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing](http://arxiv.org/abs/2403.06269v2)
- [Place Anything into Any Video](http://arxiv.org/abs/2402.14316v1)
- [UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing](http://arxiv.org/abs/2402.13185v4)
- [Anything in Any Scene: Photorealistic Video Object Insertion](http://arxiv.org/abs/2401.17509v1)
- [Object-Centric Diffusion for Efficient Video Editing](http://arxiv.org/abs/2401.05735v3)
- [VASE: Object-Centric Appearance and Shape Manipulation of Real Videos](http://arxiv.org/abs/2401.02473v1)
- [Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions](http://arxiv.org/abs/2401.01827v1)



</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="controllable">üïπÔ∏è Controllable Video Generation</span>
<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***IM-Zero:*** *Instance-level Motion Controllable Video Generation in a Zero-shot Manner*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IM-Zero_Instance-level_Motion_Controllable_Video_Generation_in_a_Zero-shot_Manner_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***AnimateAnything:*** *Consistent and Controllable Animation for Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Lei_AnimateAnything_Consistent_and_Controllable_Animation_for_Video_Generation_CVPR_2025_paper.html)

*   **[CVPR 2025]** *Customized Condition Controllable Generation for Video Soundtrack*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Customized_Condition_Controllable_Generation_for_Video_Soundtrack_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/FanQi-AI/CCCG-Video-Soundtrack) 

*   **[CVPR 2025]** ***StarGen:*** *A Spatiotemporal Autoregression Framework with Video Diffusion Model for Scalable and Controllable Scene Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_StarGen_A_Spatiotemporal_Autoregression_Framework_with_Video_Diffusion_Model_for_CVPR_2025_paper.html)

*   **[ICCV 2025]** ***Perception-as-Control:*** *Fine-grained Controllable Image Animation with 3D-aware Motion Representation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2501.05020)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chen-yingjie.github.io/projects/Perception-as-Control/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chen-yingjie/Perception-as-Control)

*   **[ICCV 2025]** ***MagicMirror:*** *ID-Preserved Video Generation in Video Diffusion Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2501.03931)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dvlab-research/MagicMirror)

*   **[ICCV 2025]** ***MagicDrive-V2:*** *High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1169)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gaoruiyuan.com/magicdrive-v2/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/flymin/MagicDrive-V2)

*   **[ICCV 2025]** ***InfiniCube:*** *Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2324)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/toronto-ai/infinicube/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nv-tlabs/InfiniCube)

*   **[ICCV 2025]** ***Free-Form Motion Control (SynFMC):*** *Controlling the 6D Poses of Camera and Objects in Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/414)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://henghuiding.com/SynFMC/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/FudanCVL/SynFMC)

*   **[ICCV 2025]** ***RealCam-I2V:*** *Real-World Image-to-Video Generation with Interactive Complex Camera Control*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zgctroy.github.io/RealCam-I2V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ZGCTroy/RealCam-I2V)

*   **[ICCV 2025]** ***MagicMotion:*** *Video Generation with a Smart Director*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2506)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://quanhaol.github.io/magicmotion-site/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/quanhaol/MagicMotion)

*   **[ICCV 2025]** ***UniMLVG:*** *Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.04842)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sensetime-fvg.github.io/UniMLVG/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SenseTime-FVG/UniMLVG)

*   **[ICLR 2025]** ***MotionClone:*** *Training-Free Motion Cloning for Controllable Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=aY3L65HgHJ) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/LPengYang/MotionClone) 

*   **[ICLR 2025]** ***Ctrl-U:*** *Robust Conditional Image Generation via Uncertainty-aware Reward Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=eC2ICbECNM) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/FanQi-AI/CCCG-Video-Soundtrack) 

*   **[AAAI 2025]** ***CAGE:*** *Unsupervised Visual Composition and Animation for Controllable Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i15.33775) [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?style=for-the-badge)](https://github.com/Araachie/cage)

*   **[AAAI 2025]** ***TrackGo:*** *A Flexible and Efficient Method for Controllable Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i10.33167)


*   **[WACV 2025]** *Fine-grained Controllable Video Generation via Object Appearance and Context*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00364)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](http://arxiv.org/abs/2506.03150v1)
*   [ATI: Any Trajectory Instruction for Controllable Video Generation](http://arxiv.org/abs/2505.22944v3)
*   [CamContextI2V: Context-aware Controllable Video Generation](http://arxiv.org/abs/2504.06022v1)
*   [Any2Caption: Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)
*   [MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance](http://arxiv.org/abs/2503.16421v2)
*   [MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent](http://arxiv.org/abs/2502.03207v1)
*   [Controllable Video Generation with Provable Disentanglement](http://arxiv.org/abs/2502.02690v2)




</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2024]** ***360DVD:*** *Controllable Panorama Video Generation with 360-Degree Video Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00660) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Akaneqwq/360DVD)

*   **[CVPR 2024]** ***Panacea:*** *Panoramic and Controllable Video Generation for Autonomous Driving*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00659) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wenyuqing/panacea)

*   **[AAAI 2024]** *Decouple Content and Motion for Conditional Image-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v38i5.28277)






</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving](http://arxiv.org/abs/2408.07605v1)
*   [InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions](http://arxiv.org/abs/2402.03040v1)



</details>

</details>

<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2023]** *Conditional Image-to-Video Generation with Latent Flow Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52729.2023.01769) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nihaomiao/CVPR23_LFDM)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [Controllable Video Generation by Learning the Underlying Dynamical System with Neural ODE](http://arxiv.org/abs/2303.05323v2)



</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)


### <span id="audio">üó£Ô∏è Audio-Driven Video Generation</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***KeyFace:*** *Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2503.01715)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://antonibigata.github.io/KeyFace/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/antonibigata/keyface_cvpr)

*   **[CVPR 2025]** ***AudCast:*** *Audio-Driven Human Video Generation by Cascaded Diffusion Transformers*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2503.19824)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://guanjz20.github.io/projects/AudCast/)

*   **[CVPR 2025]** ***MoEE:*** *Mixture of Emotion Experts for Audio-Driven Portrait Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2501.01808)

*   **[CVPR 2025]** ***Teller:*** *Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2503.18429)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://teller-avatar.github.io/)

*   **[CVPR 2025]** ***INFP:*** *Audio-Driven Interactive Head Generation in Dyadic Conversations*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2412.04037)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grisoon.github.io/INFP/)


*   **[ICCV 2025]** ***FLOAT:*** *Generative Motion Latent Flow Matching for Audio-driven Talking Portrait*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.01064)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://deepbrainai-research.github.io/float/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/deepbrainai-research/float)

*   **[ICCV 2025]** ***GaussianSpeech:*** *Audio-Driven Personalized 3D Gaussian Avatars*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.18675)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shivangi-aneja.github.io/projects/gaussianspeech/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/shivangi-aneja/GaussianSpeech)

*   **[ICCV 2025]** ***ACTalker:*** *Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2504.02542)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://harlanhong.github.io/publications/actalker/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/harlanhong/ACTalker)

*   **[ICLR 2025]** ***Hallo2:*** *Long-Duration and High-Resolution Audio-Driven Portrait Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2410.07718)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fudan-generative-vision.github.io/hallo2/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fudan-generative-vision/hallo2)

*   **[ICLR 2025]** ***Loopy:*** *Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2409.02634)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://loopyavataranony.github.io/)

*   **[ICLR 2025]** ***CyberHost:*** *A One-stage Diffusion Framework for Audio-driven Talking Body Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2409.01876)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cyberhost.github.io/)

*   **[AAAI 2025]** ***EchoMimic:*** *Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2409.13689)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://antgroup.github.io/ai/echomimic/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/antgroup/echomimic)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/AntGroup/EchoMimic)

*   **[AAAI 2025]** ***PointTalk:*** *Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2410.02700)


</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Scaling Up Audio-Synchronized Visual Animation: An Efficient Training Paradigm](http://arxiv.org/abs/2508.03955v1)
- [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](http://arxiv.org/abs/2508.00782v1)
- [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](http://arxiv.org/abs/2506.18866v1)
- [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](http://arxiv.org/abs/2506.09984v1)
- [AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation](http://arxiv.org/abs/2506.11144v1)
- [Audio-Sync Video Generation with Multi-Stream Temporal Control](http://arxiv.org/abs/2506.08003v1)
- [LLIA -- Enabling Low-Latency Interactive Avatars: Real-Time Audio-Driven Portrait Video Generation with Diffusion Models](http://arxiv.org/abs/2506.05806v1)
- [TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](http://arxiv.org/abs/2506.03099v1)

</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***FaceTalk:*** *Audio-Driven Motion Diffusion for Neural Parametric Head Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2312.08459)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shivangi-aneja.github.io/projects/facetalk/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/shivangi-aneja/FaceTalk)


*   **[ECCV 2024]** ***UniTalker:*** *Scaling up Audio-Driven 3D Facial Animation Through A Unified Model*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2408.00762)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://x-niper.github.io/unitalker/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/X-niper/UniTalker)

*   **[ECCV 2024]** *Audio-Driven Talking Face Generation with Stabilized Synchronization Loss*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2307.09368)

*   **[NeurIPS 2024]** ***VASA-1:*** *Lifelike Audio-Driven Talking Faces Generated in Real Time*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2404.10667)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.microsoft.com/en-us/research/project/vasa-1/)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal Audio-Video Generation](http://arxiv.org/abs/2412.15191v2)
- [SAVGBench: Benchmarking Spatially Aligned Audio-Video Generation](http://arxiv.org/abs/2412.13462v1)
- [SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale Spectral Diffusion Model](http://arxiv.org/abs/2412.03430v1)
- [SyncFlow: Toward Temporally Aligned Joint Audio-Video Generation from Text](http://arxiv.org/abs/2412.15220v1)
- [FLOAT: Generative Motion Latent Flow Matching for Audio-driven Talking Portrait](http://arxiv.org/abs/2412.01064v4)
- [Stereo-Talker: Audio-driven 3D Human Synthesis with Prior-Guided Mixture-of-Experts](http://arxiv.org/abs/2410.23836v1)
- [A Simple but Strong Baseline for Sounding Video Generation: Effective Adaptation of Audio and Video Diffusion Models for Joint Generation](http://arxiv.org/abs/2409.17550v3)
- [DiffTED: One-shot Audio-driven TED Talk Video Generation with Diffusion-based Co-speech Gestures](http://arxiv.org/abs/2409.07649v1)

</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="human">üíÉ Human Image Animation</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** *X-Dyna: Expressive Dynamic Human Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.10021)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bytedance/X-Dyna)

*   **[CVPR 2025]** *StableAnimator: High-Quality Identity-Preserving Human Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2411.17697)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://francis-rings.github.io/StableAnimator/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Francis-Rings/StableAnimator)

*   **[CVPR 2025]** ***Disco4D:*** *Disentangled 4D Human Generation and Animation from a Single Image*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Disco4D_Disentangled_4D_Human_Generation_and_Animation_from_a_Single_CVPR_2025_paper.html)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://disco-4d.github.io/)

*   **[ICCV 2025]** ***DreamActor-M1:*** *Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/288)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grisoon.github.io/DreamActor-M1/)

*   **[ICCV 2025]** ***Animate Anyone 2:*** *High-Fidelity Character Image Animation with Environment Affordance*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2502.06145)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://humanaigc.github.io/animate-anyone-2/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/animate-anyone-2)

*   **[ICCV 2025]** *Multi-identity Human Image Animation with Structural Video Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/638)

*   **[ICCV 2025]** ***OmniHuman-1:*** *Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2201)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omnihuman-lab.github.io/)

*   **[ICCV 2025]** ***AdaHuman:*** *Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2505.24877)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nvlabs.github.io/AdaHuman/)

*   **[ICCV 2025]** ***Ponimator:*** *Unfolding Interactive Pose for Versatile Human-human Interaction Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1453)


*   **[ICLR 2025]** ***Animate-X:*** *Universal Character Image Animation with Enhanced Motion Representation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2411.17697)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lucaria-academy.github.io/Animate-X/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/antgroup/animate-x)



</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](http://arxiv.org/abs/2507.15064v1)
- [HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions](http://arxiv.org/abs/2505.22977v1)
- [MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation](http://arxiv.org/abs/2505.10238v4)
- [TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection](http://arxiv.org/abs/2505.08437v1)
- [AnimateAnywhere: Rouse the Background in Human Image Animation](http://arxiv.org/abs/2504.19834v1)
- [UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer](http://arxiv.org/abs/2504.11289v1)
- [Taming Consistency Distillation for Accelerated Human Image Animation](http://arxiv.org/abs/2504.11143v1)
- [Multi-identity Human Image Animation with Structural Video Diffusion](http://arxiv.org/abs/2504.04126v1)
- [DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance](http://arxiv.org/abs/2504.01724v3)
- [DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation](http://arxiv.org/abs/2503.21246v2)
- [EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation](http://arxiv.org/abs/2503.18552v2)

</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>



*   **[CVPR 2024]** *MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2405.20325)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://francis-rings.github.io/MotionFollower/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Francis-Rings/MotionFollower)

*   **[CVPR 2024]** *MotionEditor: Editing Video Motion via Content-Aware Diffusion*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://openaccess.thecvf.com/content/CVPR2024/papers/Tu_MotionEditor_Editing_Video_Motion_via_Content-Aware_Diffusion_CVPR_2024_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://francis-rings.github.io/MotionEditor/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Francis-Rings/MotionEditor)


*   **[CVPR 2024]** ***MagicAnimate:*** *Temporally Consistent Human Image Animation using Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00147)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://showlab.github.io/magicanimate/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/magic-research/magic-animate)

*   **[ECCV 2024]** ***Champ:*** *Controllable and Consistent Human Image Animation with 3D Parametric Guidance*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73001-6_9)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fudan-generative-vision.github.io/champ/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fudan-generative-vision/champ)

*   **[NeurIPS 2024]** ***HumanVid:*** *Demystifying Training Data for Camera-controllable Human Image Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/23f3a0f82d79d985b6076bc84d14f66b-Abstract-Datasets_and_Benchmarks_Track.html)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://humanvid.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhenzhiwang/HumanVid)

*   **[NeurIPS 2024]** ***TPC:*** *Test-time Procrustes Calibration for Diffusion-based Human Image Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/d6fdc3002dda4c3cad3d595ac6fa5352-Abstract-Conference.html)

*   **[ICLR 2024]** *DisPose: Disentangling Pose Guidance for Controllable Human Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://openreview.net/forum?id=AumOa10MKG)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lihxxx.github.io/DisPose/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lihxxx/DisPose)


</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [DreamDance: Animating Human Images by Enriching 3D Geometry Cues from 2D Poses](http://arxiv.org/abs/2412.00397v1)
- [High Quality Human Image Animation using Regional Supervision and Motion Blur Condition](http://arxiv.org/abs/2409.19580v1)
- [Dormant: Defending against Pose-driven Human Image Animation](http://arxiv.org/abs/2409.14424v2)
- [TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models](http://arxiv.org/abs/2407.09012v1)
- [UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation](http://arxiv.org/abs/2406.01188v1)
- [VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation](http://arxiv.org/abs/2405.18156v1))


</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)



### <span id="acceleration">‚ö° Fast Video Generation (Acceleration)</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** *Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://cvpr.thecvf.com/virtual/2025/poster/34471)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://liewfeng.github.io/TeaCache/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali‚Äëvilab/TeaCache)

*   **[CVPR 2025]** *CausVid: From Slow Bidirectional to Fast Autoregressive VDMs*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Yin_From_Slow_Bidirectional_to_Fast_Autoregressive_Video_Diffusion_Models_CVPR_2025_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/tianweiy/CausVid)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tianweiy/CausVid)

*   **[CVPR 2025]** ***BlockDance:*** *Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=yJAk0n0NyU)

*   **[ICCV 2025]** ***AdaCache:*** *Adaptive Caching for Faster Video Generation with Diffusion Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.02397)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://adacache-dit.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AdaCache-DiT/AdaCache)

*   **[ICCV 2025]** ***TaylorSeer:*** *From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/448)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://taylorseer.github.io/TaylorSeer/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Shenyi-Z/TaylorSeer)

*   **[ICCV 2025]** *Accelerating Diffusion Transformer via Gradient-Optimized Cache*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1026)

*   **[ICCV 2025]** ***V.I.P.:*** *Iterative Online Preference Distillation for Efficient Video Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2508.03254)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jiiiisoo.github.io/VIP.github.io/)

*   **[ICCV 2025]** ***DMDX:*** *Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2528)

*   **[ICCV 2025]** ***OmniCache:*** *A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for DiT*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2508.16212)

*   **[ICLR 2025]** ***FasterCache:*** *Training-Free Video Diffusion Model Acceleration with High Quality*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=yJAk0n0NyU)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)]([[Ê≠§Â§ÑÊõøÊç¢‰∏∫GitHubÈìæÊé•]](https://github.com/Vchitect/FasterCache))

*   **[ICML 2025]** ***Sparse VideoGen:***  *Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=u8CA3qIS0V)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://svg-project.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/svg-project/Sparse-VideoGen)

*   **[ICML 2025]** *Fast Video Generation with Sliding Tile Attention*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=U74MOXPEJd)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hao-ai-lab.github.io/blogs/sta/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hao-ai-lab/FastVideo)

*   **[ICML 2025]** ***Ca2-VDM:*** *Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://icml.cc/virtual/2025/poster/45139)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Dawn‚ÄëLX/CausalCache‚ÄëVDM)

*   **[ICML 2025]** ***AsymRnR:*** *Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://icml.cc/virtual/2025/poster/46432)





</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](http://arxiv.org/abs/2507.02860v1)
- [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](http://arxiv.org/abs/2508.12969v1)
- [MagCache: Fast Video Generation with Magnitude-Aware Cache](http://arxiv.org/abs/2506.09045v1)
- [Seedance 1.0: Exploring the Boundaries of Video Generation Models](http://arxiv.org/abs/2506.09113v2)
- [SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling](http://arxiv.org/abs/2508.17756v1)
- [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](http://arxiv.org/abs/2508.12691v1)
- [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](http://arxiv.org/abs/2508.06082v1)
- [Taming Diffusion Transformer for Real-Time Mobile Video Generation](http://arxiv.org/abs/2507.13343v1)
- [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](http://arxiv.org/abs/2506.03065v1)
- [SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation](http://arxiv.org/abs/2505.19151v1)
- [Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation](http://arxiv.org/abs/2505.18875v1)
- [DVD-Quant: Data-free Video Diffusion Transformers Quantization](http://arxiv.org/abs/2505.18663v1)
- [AccVideo: Accelerating Video Diffusion Model with Synthetic Dataset](http://arxiv.org/abs/2503.19462v1)
- [Region Masking to Accelerate Video Processing on Neuromorphic Hardware](http://arxiv.org/abs/2503.16775v1)
- [DSV: Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training](http://arxiv.org/abs/2502.07590v3)



</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***Cache Me if You Can:*** *Accelerating Diffusion Models through Block Caching*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wimbauer_Cache_Me_if_You_Can_Accelerating_Diffusion_Models_through_Block_CVPR_2024_paper.pdf)

*   **[NeurIPS 2024]** *Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/c859b99b5d717c9035e79d43dfd69435-Abstract-Conference.html)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yhzhai.github.io/mcm/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yhZhai/mcm)

*   **[NeurIPS 2024]** *Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=cS63YtJ49A)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jiakangyuan.github.io/AdaptiveDiffusion/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UniModal4Reasoning/AdaptiveDiffusion)

*   **[NeurIPS 2024]** *Fast and Memory-Efficient Video Diffusion Using Streamlined Inference*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.01171)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wuyushuwys/FMEDiffusion)

*   **[IJCAI 2024]** ***FasterVD:*** *On Acceleration of Video Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ijcai.org/proceedings/2024/1044.pdf)

</details>





<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Accelerating Video Diffusion Models via Distribution Matching](http://arxiv.org/abs/2412.05899v1)
- [Adaptive Caching for Faster Video Generation with Diffusion Transformers](http://arxiv.org/abs/2411.02397v2)
- [OSV: One Step is Enough for High-Quality Image to Video Generation](http://arxiv.org/abs/2409.11367v2)
- [HAVANA: Hierarchical stochastic neighbor embedding for Accelerated Video ANnotAtions](http://arxiv.org/abs/2409.10641v1)
- [AnimateDiff-Lightning: Cross-Model Diffusion Distillation](http://arxiv.org/abs/2403.12706v1)


</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)



---

## <span id="datasets">üóÇÔ∏è Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **UCF101** | 2012 | Video | Unconditional Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/1212.0402.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.crcv.ucf.edu/data/UCF101.php) |
| **TaiChi-HD** | 2019 | Video | Unconditional Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://papers.nips.cc/paper_files/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/AliaksandrSiarohin/first-order-model) |
| **SkyTimeLapse** | 2020 | Video | Unconditional Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500290.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zhangzjn/DTVNet) |
| **WebVid-10M** | 2021 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2104.00650.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://maxbain.com/webvid-dataset/) |
| **HD-VG-130M** | 2023 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2305.10874.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/daooshee/HD-VG-130M) |
| **FETV** | 2023 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://papers.nips.cc/paper_files/paper/2023/file/c481049f7410f38e788f67c171c64ad5-Paper-Datasets_and_Benchmarks.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/llyx97/FETV) |
| **InternVid** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2307.06942) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) |
| **VidProM** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2403.06098.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/WangWenhao0716/VidProM) |
| **Panda-70M** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2402.19479) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://tsaishienchen.github.io/panda-70m/) |
| **SafeSora** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.14477v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/PKU-Alignment/safe-sora) |
| **ChronoMagic-Pro**| 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.18522v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/PKU-YuanGroup/ChronoMagic-Bench) |
| **T2V-CompBench** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2407.14505v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/KaiyueSun98/T2V-CompBench) |
| **VidGen-1M** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2408.02629v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/SAIS-FUXI/VidGen) |
| **PhyGenBench** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2410.05363v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/OpenGVLab/PhyGenBench) |
| **DH-FaceVid-1K** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2410.07151v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/DH-FaceVid-1K/DH-FaceVid-1K) |
| **StoryEval** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2412.16211v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/ypwang61/StoryEval) |
| **HOIGen-1M** | 2025 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2503.23715) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://huggingface.co/datasets/HOIGen/HOIGen-1M) |
| **OpenVid-1M** | 2025 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openreview.net/forum?id=j7kdXSrISM) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://nju-pcalab.github.io/projects/openvid/) |
| **HumanVid** | 2024 | Image, Video | Image-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/23f3a0f82d79d985b6076bc84d14f66b-Abstract-Datasets_and_Benchmarks_Track.html) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zhenzhiwang/HumanVid) |
| **TIP-I2V** | 2024 | Text, Image, Video | Image-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.04709v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/WangWenhao0716/TIP-I2V) |
| **TC-Bench** | 2024 | Text, Image, Video | Text-to-Video, Image-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.08656v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/weixi-feng/tc-bench) |
| **AnimeShooter** | 2025 | Text, Image, Video | Text-to-Video, Image-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2506.03126v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://qiulu66.github.io/animeshooter/) |
| **VE-Bench** | 2024 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2408.11481v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/littlespray/VE-Bench) |
| **DAVIS-Edit** | 2024 | Text, Video, Image | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.11045v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/AlonzoLeeeooo/StableV2V) |
| **DAVIS** | 2017 | Video, Image | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/1704.00675.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://davischallenge.org/) |
| **VIVID-10M** | 2024 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.15260v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://huggingface.co/datasets/KwaiVGI/VIVID-10M) |
| **Se√±orita-2M** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2502.06734v3) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zibojia/SENORITA) |
| **FiVE-Bench** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2503.13684v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/MinghanLi/FiVE-Bench) |
| **InsViE-1M** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2503.20287v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/langmanbusi/InsViE) |
| **VEU-Bench** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2504.17828) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/VEU-Benchmark/VEU-Benchmark) |
| **OpenS2V-5M** | 2025 | Text, Video, Audio | Text-to-Video, Image-to-Video, Subject-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2505.20292) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://pku-yuangroup.github.io/OpenS2V-Nexus/) |
| **SpeakerVid-5M** | 2025 | Text, Video, Audio | Audio-Driven Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2507.09862) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://dorniwang.github.io/SpeakerVid-5M/) |

[<small>‚áß Back to ToC</small>](#contents)

---

## <span id="about-us">üéì About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Video Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

‚û°Ô∏è **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI) ÊòØ‰∏ÄÂÆ∂Áî±È°∂Â∞ñÁ†îÁ©∂ËÄÖÁªÑÊàêÔºåËá¥Âäõ‰∫é‰∏∫ÂÖ®ÁêÉÈ´òÊ†°Â≠¶ÁîüÊèê‰æõÈ´òË¥®Èáè1V1ÁßëÁ†îËæÖÂØºÁöÑ‰∏ì‰∏öÊú∫ÊûÑ„ÄÇÊàë‰ª¨ÁöÑ‰ΩøÂëΩÊòØÂ∏ÆÂä©Â≠¶ÁîüÂüπÂÖªÂá∫Ëâ≤ÂçìË∂äÁöÑÁßëÁ†îÊäÄËÉΩÔºåÂú®È°∂Á∫ß‰ºöËÆÆÂíåÊúüÂàä‰∏äÂèëË°®Ëá™Â∑±ÁöÑÊàêÊûú„ÄÇ

Áª¥Êä§‰∏Ä‰∏™GitHubË∞ÉÁ†î‰ªìÂ∫ìÈúÄË¶ÅÂ∑®Â§ßÁöÑÁ≤æÂäõÔºåÊ≠£Â¶ÇÂÆåÊàê‰∏ÄÁØáÈ´òË¥®ÈáèÁöÑËÆ∫Êñá‰∏ÄÊ†∑ÔºåÁ¶ª‰∏çÂºÄ‰∏ìÊ≥®ÁöÑÊäïÂÖ•Âíå‰∏ì‰∏öÁöÑÊåáÂØº„ÄÇÂ¶ÇÊûúÊÇ®Â∏åÊúõÂú®Ëá™Â∑±ÁöÑÁ†îÁ©∂È°πÁõÆ‰∏≠ÔºåËé∑ÂæóÊù•Ëá™È°∂Â∞ñÂ≠¶ËÄÖÁöÑ‰∏ÄÂØπ‰∏ÄÊîØÊåÅÔºåÊàë‰ª¨ËØöÈÇÄÊÇ®‰∏éÊàë‰ª¨ÂèñÂæóËÅîÁ≥ª„ÄÇ

‚û°Ô∏è **Ê¨¢ËøéÈÄöËøá [ÂæÆ‰ø°](assets/wechat.jpg) Êàñ [ÈÇÆ‰ª∂](mailto:your.email@example.com) ËÅîÁ≥ªÊàë‰ª¨ÔºåÂºÄÂêØÊÇ®ÁöÑÁßëÁ†î‰πãÊóÖ„ÄÇ**


[<small>‚áß Back to ToC</small>](#contents)

---



## <span id="contributing">ü§ù Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.
