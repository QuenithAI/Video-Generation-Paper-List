<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Video Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Video Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/Video-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/Video-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (Â∫îËææÂ≠¶ÊúØ)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of video generation.
>
> Your contributions are also vital‚Äîfeel free to [open an issue](https://github.com/QuenithAI/Video-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/Video-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> Êú¨‰ªìÂ∫ìÁî± **„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI)** ÁöÑ‰∏ÄÁ∫øÁßëÁ†îÂØºÂ∏àÂõ¢ÈòüÂÄæÂäõÊâìÈÄ†Âπ∂ÊåÅÁª≠Áª¥Êä§ÔºåÊó®Âú®‰∏∫ÊÇ®ÂëàÁé∞ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÊúÄÂÖ®Èù¢„ÄÅÊúÄÂâçÊ≤øÁöÑËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÁöÑËÆ∫Êñá„ÄÇ
>
> ÊÇ®ÁöÑË¥°ÁåÆÂØπÊàë‰ª¨ÂíåÁ§æÂå∫Êù•ËØ¥Ëá≥ÂÖ≥ÈáçË¶Å‚Äî‚ÄîÊàë‰ª¨ËØöÈÇÄÊúâÂøó‰πãÂ£´ÈÄöËøá [open an issue](https://github.com/QuenithAI/Video-Generation-Paper-List/issues) Êàñ [submit a pull request](https://github.com/QuenithAI/Video-Generation-Paper-List/pulls) Êù•Êàê‰∏∫Ëøô‰∏™È°πÁõÆÁöÑÂêà‰ΩúËÄÖ‰πã‰∏ÄÔºåÊúüÂæÖÊÇ®ÁöÑÂä†ÂÖ•ÔºÅ
> 
> Â¶ÇÊûúÊÇ®Âú®ÂÜ≤Âà∫ÁßëÁ†îÈ°∂‰ºöÁöÑÈÅìË∑Ø‰∏äÈúÄË¶Å‰∏ì‰∏öÁöÑ1V1ÊåáÂØºÔºåÊ¨¢Ëøé**ÈÄöËøá[ÂæÆ‰ø°](assets/wechat.jpg)Êàñ[ÈÇÆ‰ª∂](mailto:christzhaung@gmail.com)ËÅîÁ≥ªÊàë‰ª¨**„ÄÇ


<details>
<summary><strong>‚ö° Latest Updates</strong></summary>

- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

<!-- omit in toc -->
## <span id="contents">üìö Table of Contents</span>
- [üìú Papers \& Models](#-papers--models)
  - [‚úçÔ∏è Survey Papers](#Ô∏è-survey-papers)
  - [üé• Text-to-Video (T2V) Generation](#-text-to-video-t2v-generation)
  - [üñºÔ∏è Image-to-Video (I2V) Generation](#Ô∏è-image-to-video-i2v-generation)
  - [‚úÇÔ∏è Video-to-Video (V2V) Editing](#Ô∏è-video-to-video-v2v-editing)
  - [üïπÔ∏è Controllable Video Generation](#Ô∏è-controllable-video-generation)
- [üóÇÔ∏è Datasets](#Ô∏è-datasets)
- [üéì About Us](#-about-us)
- [ü§ù Contributing](#-contributing)

---

## <span id="papers">üìú Papers & Models</span>

### <span id="survey">‚úçÔ∏è Survey Papers</span>
- [Controllable Video Generation: A Survey](http://arxiv.org/abs/2507.16869v1)
- [Diffusion Model-Based Video Editing: A Survey](http://arxiv.org/abs/2407.07111v1)
- [From Sora What We Can See: A Survey of Text-to-Video Generation](http://arxiv.org/abs/2405.10674v1)

### <span id="t2v">üé• Text-to-Video (T2V) Generation</span>
<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** *AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2411.17221)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wangjiarui153/AIGV-Assessor)

*   **[CVPR 2025]** *Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2410.06241)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bujiazi/ByTheWay)

*   **[CVPR 2025]** *Retrieval-Augmented Prompt Optimization for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2405.15579)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://whynothaha.github.io/Prompt_optimizer/RAPO.html)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Vchitect/RAPO)

*   **[CVPR 2025]** *Identity-Preserving Text-to-Video Generation by Frequency Decomposition*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2411.17440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pku-yuangroup.github.io/ConsisID/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PKU-YuanGroup/ConsisID)

*   **[CVPR 2025]** *Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2504.06861)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://djagpal02.github.io/EIDT-V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/djagpal02/EIDT-V)

*   **[CVPR 2025]** ***TransPixeler:*** *Advancing Text-to-Video Generation with Transparency*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.03006)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wileewang.github.io/TransPixar/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wileewang/TransPixeler)

*   **[CVPR 2025]** *LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.00596)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pittisl/PhyT2V)

*   **[CVPR 2025]** *Improving Text-to-Video Generation via Instance-aware Structured Caption*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.09283)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NJU-PCALab/InstanceCap)

*   **[CVPR 2025]** *Compositional Text-to-Video Generation with Blob Video Representations*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.07647)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://blobgen-vid2.github.io/)

*   **[CVPR 2025]** *Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.09856)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lineargen.github.io/)

*   **[ICLR 2025]** ***OpenVid-1M:*** *A Large-Scale High-Quality Dataset for Text-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=j7kdXSrISM)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nju-pcalab.github.io/projects/openvid/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NJU-PCALab/OpenVid-1M)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/nkp37/OpenVid-1M)

*   **[ICLR 2025]** ***CogVideoX:*** *Text-to-Video Diffusion Models with An Expert Transformer*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LQzN6TRFg9)

</details>


<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](http://arxiv.org/abs/2508.04228v1)
- [Motion is the Choreographer: Learning Latent Pose Dynamics for Seamless Sign Language Generation](http://arxiv.org/abs/2508.04049v1)
- [$\text{S}^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation](http://arxiv.org/abs/2508.04016v1)
- [LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation](http://arxiv.org/abs/2508.03694v1)
- [Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation](http://arxiv.org/abs/2508.03334v2)
- [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](http://arxiv.org/abs/2508.03254v1)
- [QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots](http://arxiv.org/abs/2508.02512v1)
- [PoseGuard: Pose-Guided Generation with Safety Guardrails](http://arxiv.org/abs/2508.02476v1)
- [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2508.00312v1)
- [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](http://arxiv.org/abs/2507.22360v1)
- [Compositional Video Synthesis by Temporal Object-Centric Learning](http://arxiv.org/abs/2507.20855v1)
- [Enhancing Scene Transition Awareness in Video Generation via Post-Training](http://arxiv.org/abs/2507.18046v1)
- [Yume: An Interactive World Generation Model](http://arxiv.org/abs/2507.17744v1)
- [EndoGen: Conditional Autoregressive Endoscopic Video Generation](http://arxiv.org/abs/2507.17388v1)
- [MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation](http://arxiv.org/abs/2507.16310v1)
- [PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation](http://arxiv.org/abs/2507.16116v1)
- [TokensGen: Harnessing Condensed Tokens for Long Video Generation](http://arxiv.org/abs/2507.15728v1)
- [Conditional Video Generation for High-Efficiency Video Compression](http://arxiv.org/abs/2507.15269v1)
- [Taming Diffusion Transformer for Real-Time Mobile Video Generation](http://arxiv.org/abs/2507.13343v1)
- [LoViC: Efficient Long Video Generation with Context Compression](http://arxiv.org/abs/2507.12952v1)
- [World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving](http://arxiv.org/abs/2507.12762v1)
- [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](http://arxiv.org/abs/2507.11245v2)
- [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](http://arxiv.org/abs/2507.08801v1)
- [Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](http://arxiv.org/abs/2507.08422v1)
- [Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling](http://arxiv.org/abs/2507.07982v1)
- [Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions](http://arxiv.org/abs/2507.07978v1)
- [Scaling RL to Long Videos](http://arxiv.org/abs/2507.07966v3)
- [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](http://arxiv.org/abs/2507.06739v1)
- [Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions](http://arxiv.org/abs/2507.06133v2)
- [Omni-Video: Democratizing Unified Video Understanding and Generation](http://arxiv.org/abs/2507.06119v2)
- [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](http://arxiv.org/abs/2507.05963v2)
- [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](http://arxiv.org/abs/2507.05675v1)
- [Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations](http://arxiv.org/abs/2507.04705v1)
- [PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)
- [StreamDiT: Real-Time Streaming Text-to-Video Generation](http://arxiv.org/abs/2507.03745v2)
- [RefTok: Reference-Based Tokenization for Video Generation](http://arxiv.org/abs/2507.02862v1)
- [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](http://arxiv.org/abs/2507.02860v1)
- [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](http://arxiv.org/abs/2507.02608v1)
- [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](http://arxiv.org/abs/2507.01945v2)
- [LLM-based Realistic Safety-Critical Driving Video Generation](http://arxiv.org/abs/2507.01264v1)
- [Geometry-aware 4D Video Generation for Robot Manipulation](http://arxiv.org/abs/2507.01099v1)
- [Populate-A-Scene: Affordance-Aware Human Video Generation](http://arxiv.org/abs/2507.00334v1)
- [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](http://arxiv.org/abs/2507.00162v1)
- [Epona: Autoregressive Diffusion World Model for Autonomous Driving](http://arxiv.org/abs/2506.24113v1)
- [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](http://arxiv.org/abs/2506.23858v1)
- [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](http://arxiv.org/abs/2506.23690v1)
- [Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation](http://arxiv.org/abs/2506.19852v1)
- [GenHSI: Controllable Generation of Human-Scene Interaction Videos](http://arxiv.org/abs/2506.19840v1)
- [SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution](http://arxiv.org/abs/2506.19838v2)
- [Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation](http://arxiv.org/abs/2506.19348v1)
- [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](http://arxiv.org/abs/2506.18903v2)
- [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](http://arxiv.org/abs/2506.18899v1)
- [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](http://arxiv.org/abs/2506.18655v1)
- [Emergent Temporal Correspondences from Video Diffusion Transformers](http://arxiv.org/abs/2506.17220v2)
- [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](http://arxiv.org/abs/2506.17201v1)
- [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](http://arxiv.org/abs/2506.16119v1)
- [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](http://arxiv.org/abs/2506.16054v1)
- [Causally Steered Diffusion for Automated Video Counterfactual Generation](http://arxiv.org/abs/2506.14404v2)
- [VideoMAR: Autoregressive Video Generation with Continuous Tokens](http://arxiv.org/abs/2506.14168v2)
- [M4V: Multi-Modal Mamba for Text-to-Video Generation](http://arxiv.org/abs/2506.10915v1)
- [GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning](http://arxiv.org/abs/2506.10639v1)
- [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](http://arxiv.org/abs/2506.10568v1)
- [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](http://arxiv.org/abs/2506.09350v1)
- [MagCache: Fast Video Generation with Magnitude-Aware Cache](http://arxiv.org/abs/2506.09045v1)
- [Seedance 1.0: Exploring the Boundaries of Video Generation Models](http://arxiv.org/abs/2506.09113v2)
- [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](http://arxiv.org/abs/2506.08351v1)
- [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](http://arxiv.org/abs/2506.08009v1)
- [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](http://arxiv.org/abs/2506.07280v2)
- [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](http://arxiv.org/abs/2506.07177v1)
- [Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion](http://arxiv.org/abs/2506.07136v1)
- [ContentV: Efficient Training of Video Generation Models with Limited Compute](http://arxiv.org/abs/2506.05343v2)
- [Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers](http://arxiv.org/abs/2506.05096v3)
- [FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion](http://arxiv.org/abs/2506.04648v2)
- [LayerFlow: A Unified Model for Layer-aware Video Generation](http://arxiv.org/abs/2506.04228v1)
- [FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers](http://arxiv.org/abs/2506.04213v2)
- [DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models](http://arxiv.org/abs/2506.03517v1)
- [Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](http://arxiv.org/abs/2506.03275v1)
- [Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval](http://arxiv.org/abs/2506.03141v1)
- [CamCloneMaster: Enabling Reference-based Camera Control for Video Generation](http://arxiv.org/abs/2506.03140v1)
- [Dual-Expert Consistency Model for Efficient and High-Quality Video Generation](http://arxiv.org/abs/2506.03123v2)
- [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](http://arxiv.org/abs/2506.03065v1)
- [LumosFlow: Motion-Guided Long Video Generation](http://arxiv.org/abs/2506.02497v1)
- [Motion aware video generative model](http://arxiv.org/abs/2506.02244v1)
- [Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks](http://arxiv.org/abs/2506.01758v2)
- [Wan: Open and Advanced Large-Scale Video Generative Models](http://arxiv.org/abs/2503.20314v2)


</details>

</details>
<details>
<summary><h4>‚ú® 2024</h4></summary>


<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***Vlogger:*** *Make Your Dream A Vlog*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2401.09414.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Vchitect/Vlogger)

*   **[CVPR 2024]** ***Make Pixels Dance:*** *High-Dynamic Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.10982.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://makepixelsdance.github.io/)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://makepixelsdance.github.io/demo.html)

*   **[CVPR 2024]** ***VGen:*** *Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.04483)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://higen-t2v.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/VGen)

*   **[CVPR 2024]** ***GenTron:*** *Delving Deep into Diffusion Transformers for Image and Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.04557)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.shoufachen.com/gentron_website/)

*   **[CVPR 2024]** ***SimDA:*** *Simple Diffusion Adapter for Efficient Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2308.09710.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chenhsing.github.io/SimDA/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ChenHsing/SimDA)

*   **[CVPR 2024]** ***MicroCinema:*** *A Divide-and-Conquer Approach for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.18829)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wangyanhui666.github.io/MicroCinema.github.io/)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://youtube.com/shorts/H7O-Ku_lqPA)

*   **[CVPR 2024]** ***Generative Rendering:*** *Controllable 4D-Guided Video Generation with 2D Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.01409)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://primecai.github.io/generative_rendering/)

*   **[CVPR 2024]** ***PEEKABOO:*** *Interactive Video Generation via Masked-Diffusion*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.07509)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jinga-lala.github.io/projects/Peekaboo/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/microsoft/Peekaboo)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://huggingface.co/spaces/anshuln/peekaboo-demo)

*   **[CVPR 2024]** ***EvalCrafter:*** *Benchmarking and Evaluating Large Video Generation Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.11440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://evalcrafter.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/EvalCrafter/EvalCrafter)

*   **[CVPR 2024]** *A Recipe for Scaling up Text-to-Video Generation with Text-free Videos*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.15770)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tf-t2v.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/damo-vilab/i2vgen-xl)

*   **[CVPR 2024]** ***BIVDiff:*** *A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.02813)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://bivdiff.github.io/)

*   **[CVPR 2024]** ***Mind the Time:*** *Scaled Spatiotemporal Transformers for Text-to-Video Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2402.14797)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapvideo/video_ldm.html)

*   **[CVPR 2024]** ***Animate Anyone:*** *Consistent and Controllable Image-to-video Synthesis for Character Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.17117.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://humanaigc.github.io/animate-anyone/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/AnimateAnyone)

*   **[CVPR 2024]** ***MotionDirector:*** *Motion Customization of Text-to-Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.08465)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/showlab/MotionDirector)

*   **[CVPR 2024]** *Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Skorokhodov_Hierarchical_Patch_Diffusion_Models_for_High-Resolution_Video_Generation_CVPR_2024_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/hpdm/)

*   **[CVPR 2024]** ***DiffPerformer:*** *Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DiffPerformer_Iterative_Learning_of_Consistent_Latent_Guidance_for_Diffusion-based_Human_CVPR_2024_paper.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aipixel/)

*   **[CVPR 2024]** *Grid Diffusion Models for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2404.00234.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/taegyeong-lee/Grid-Diffusion-Models-for-Text-to-Video-Generation)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://taegyeong-lee.github.io/text2video)

*   **[ECCV 2024]** ***Emu Video:*** *Factorizing Text-to-Video Generation by Explicit Image Conditioning*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.10709.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/)

*   **[ECCV 2024]** ***W.A.L.T.:*** *Photorealistic Video Generation with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10270.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://walt-video-diffusion.github.io/)

*   **[ECCV 2024]** ***MoVideo:*** *Motion-Aware Video Generation with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06030.pdf)

*   **[ECCV 2024]** ***DrivingDiffusion:*** *Layout-Guided Multi-View Driving Scenarios Video Generation with Latent Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10097.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://drivingdiffusion.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/shalfun/DrivingDiffusion)

*   **[ECCV 2024]** ***MagDiff:*** *Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02738.pdf)

*   **[ECCV 2024]** ***HARIVO:*** *Harnessing Text-to-Image Models for Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06938.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kwonminki.github.io/HARIVO/)

*   **[ECCV 2024]** ***MEVG:*** *Multi-event Video Generation with Text-to-Video Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06012.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kuai-lab.github.io/eccv2024mevg/)

*   **[NeurIPS 2024]** ***DEMO:*** *Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/81f19c0e9f3e06c831630ab6662fd8ea-Paper-Conference.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PR-Ryan/DEMO)

*   **[ICLR 2024]** ***VDT:*** *General-purpose Video Diffusion Transformers via Mask Modeling*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2305.13311.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://vdt-2023.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RERV/VDT)

*   **[ICLR 2024]** ***VersVideo:*** *Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf?id=K9sVJ17zvB)

*   **[AAAI 2024]** ***Follow Your Pose:*** *Pose-Guided Text-to-Video Generation using Pose-Free Videos*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2304.01186)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://follow-your-pose.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mayuelala/FollowYourPose)

*   **[AAAI 2024]** ***E2HQV:*** *High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2401.08117)

*   **[AAAI 2024]** ***ConditionVideo:*** *Training-Free Condition-Guided Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.07697)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pengbo807.github.io/conditionvideo-website/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pengbo807/ConditionVideo)

*   **[AAAI 2024]** ***F3-Pruning:*** *A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text to-Video Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.03459)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Gender Bias in Text-to-Video Generation Models: A case study of Sora](http://arxiv.org/abs/2501.01987v2)
- [VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation](http://arxiv.org/abs/2412.21059v2)
- [Follow-Your-MultiPose: Tuning-Free Multi-Character Text-to-Video Generation via Pose Guidance](http://arxiv.org/abs/2412.16495v2)
- [CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training](http://arxiv.org/abs/2412.15646v2)
- [DirectorLLM for Human-Centric Video Generation](http://arxiv.org/abs/2412.14484v1)
- [Can Video Generation Replace Cinematographers? Research on the Cinematic Language of Generated Video](http://arxiv.org/abs/2412.12223v2)
- [LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity](http://arxiv.org/abs/2412.09856v2)
- [T-SVG: Text-Driven Stereoscopic Video Generation](http://arxiv.org/abs/2412.09323v2)
- [Mojito: Motion Trajectory and Intensity Control for Video Generation](http://arxiv.org/abs/2412.08948v2)
- [SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints](http://arxiv.org/abs/2412.07760v1)
- [Motion by Queries: Identity-Motion Trade-offs in Text-to-Video Generation](http://arxiv.org/abs/2412.07750v3)
- [STIV: Scalable Text and Image Conditioned Video Generation](http://arxiv.org/abs/2412.07730v1)
- [GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration](http://arxiv.org/abs/2412.04440v1)
- [Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback](http://arxiv.org/abs/2412.02617v1)
- [CPA: Camera-pose-awareness Diffusion Transformer for Video Generation](http://arxiv.org/abs/2412.01429v1)
- [MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation](http://arxiv.org/abs/2411.18281v2)
- [Scene Co-pilot: Procedural Text to Video Generation with Human in the Loop](http://arxiv.org/abs/2411.18644v1)
- [Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models](http://arxiv.org/abs/2411.17041v1)
- [DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation](http://arxiv.org/abs/2411.16657v3)
- [InTraGen: Trajectory-controlled Video Generation for Object Interactions](http://arxiv.org/abs/2411.16804v1)
- [Optical-Flow Guided Prompt Optimization for Coherent Video Generation](http://arxiv.org/abs/2411.15540v2)
- [VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement](http://arxiv.org/abs/2411.15115v2)
- [Motion Control for Enhanced Complex Action Video Generation](http://arxiv.org/abs/2411.08328v1)
- [GameGen-X: Interactive Open-world Game Video Generation](http://arxiv.org/abs/2411.00769v3)
- [Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning](http://arxiv.org/abs/2410.24219v1)
- [ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation](http://arxiv.org/abs/2410.20502v3)
- [Animating the Past: Reconstruct Trilobite via Video Generation](http://arxiv.org/abs/2410.14715v1)
- [ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way](http://arxiv.org/abs/2410.06241v3)
- [T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design](http://arxiv.org/abs/2410.05677v2)
- [The Dawn of Video Generation: Preliminary Explorations with SORA-like Models](http://arxiv.org/abs/2410.05227v2)
- [Compositional 3D-aware Video Generation with LLM Director](http://arxiv.org/abs/2409.00558v1)
- [Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation](http://arxiv.org/abs/2408.10453v2)
- [FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance](http://arxiv.org/abs/2408.08189v4)
- [Still-Moving: Customized Video Generation without Customized Video Data](http://arxiv.org/abs/2407.08674v1)
- [VEnhancer: Generative Space-Time Enhancement for Video Generation](http://arxiv.org/abs/2407.07667v1)
- [Mobius: A High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task](http://arxiv.org/abs/2407.06617v4)
- [VIMI: Grounding Video Generation through Multi-modal Instruction](http://arxiv.org/abs/2407.06304v1)
- [GVDIFF: Grounded Text-to-Video Generation with Diffusion Models](http://arxiv.org/abs/2407.01921v2)
- [Evaluation of Text-to-Video Generation Models: A Dynamics Perspective](http://arxiv.org/abs/2407.01094v1)
- [Text-Animator: Controllable Visual Text Video Generation](http://arxiv.org/abs/2406.17777v1)
- [MotionBooth: Motion-Aware Customized Text-to-Video Generation](http://arxiv.org/abs/2406.17758v3)
- [Hierarchical Patch Diffusion Models for High-Resolution Video Generation](http://arxiv.org/abs/2406.07792v1)
- [Compositional Video Generation as Flow Equalization](http://arxiv.org/abs/2407.06182v1)
- [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](http://arxiv.org/abs/2406.05338v6)
- [VideoTetris: Towards Compositional Text-to-Video Generation](http://arxiv.org/abs/2406.04277v2)
- [VideoPhy: Evaluating Physical Commonsense for Video Generation](http://arxiv.org/abs/2406.03520v2)
- [I4VGen: Image as Free Stepping Stone for Text-to-Video Generation](http://arxiv.org/abs/2406.02230v2)
- [DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control](http://arxiv.org/abs/2405.12796v1)
- [The Lost Melody: Empirical Observations on Text-to-Video Generation From A Storytelling Perspective](http://arxiv.org/abs/2405.08720v1)
- [TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation](http://arxiv.org/abs/2405.04682v4)
- [MotionMaster: Training-free Camera Motion Transfer For Video Generation](http://arxiv.org/abs/2404.15789v2)
- [ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion Model](http://arxiv.org/abs/2404.12903v1)
- [MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators](http://arxiv.org/abs/2404.05014v2)
- [CameraCtrl: Enabling Camera Control for Text-to-Video Generation](http://arxiv.org/abs/2404.02101v2)
- [Grid Diffusion Models for Text-to-Video Generation](http://arxiv.org/abs/2404.00234v2)
- [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](http://arxiv.org/abs/2403.14773v2)
- [S2DM: Sector-Shaped Diffusion Models for Video Generation](http://arxiv.org/abs/2403.13408v2)
- [Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](http://arxiv.org/abs/2403.13248v3)



</details>


</details>


<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2023]** ***Align your Latents:*** *High-resolution Video Synthesis with Latent Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2304.08818.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/srpkdyy/VideoLDM)

*   **[CVPR 2023]** ***Text2Video-Zero:*** *Text-to-image Diffusion Models are Zero-shot Video Generators*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://text2video-zero.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Text2Video-Zero)
    [![Demo](https://img.shields.io/badge/Hugging_Face-Demo-yellow?style=for-the-badge)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)

*   **[CVPR 2023]** *Video Probabilistic Diffusion Models in Projected Latent Space*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sihyun-yu/PVDM)

*   **[ICCV 2023]** ***PYOCO:*** *Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/dir/pyoco/)

*   **[ICCV 2023]** ***Gen-1:*** *Structure and Content-guided Video Synthesis with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.runwayml.com/gen1)

*   **[NeurIPS 2023]** *Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2204.03458.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-diffusion.github.io/)

*   **[NeurIPS 2023]** ***UniPi:*** *Learning Universal Policies via Text-Guided Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://papers.nips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://universal-policy.github.io/unipi/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/flow-diffusion/AVDC)

*   **[NeurIPS 2023]** ***VideoComposer:*** *Compositional Video Synthesis with Motion Controllability*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2306.02018.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://videocomposer.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/videocomposer)

*   **[ICLR 2023]** ***CogVideo:*** *Large-scale Pretraining for Text-to-video Generation via Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf?id=rB6TpjAuSRy)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/CogVideo)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://models.aminer.cn/cogvideo/)

*   **[ICLR 2023]** ***Make-A-Video:*** *Text-to-video Generation without Text-video Data*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2209.14792.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://makeavideo.studio/)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/lucidrains/make-a-video-pytorch)

*   **[ICLR 2023]** ***Phenaki:*** *Variable Length Video Generation From Open Domain Textual Description*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf/fe8e106a2746992c9c2e658bdc8cb9c89cc5a39a.pdf)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/lucidrains/phenaki-pytorch)



</details>




<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [FlashVideo: A Framework for Swift Inference in Text-to-Video Generation](http://arxiv.org/abs/2401.00869v1)
- [A Recipe for Scaling up Text-to-Video Generation with Text-free Videos](http://arxiv.org/abs/2312.15770v1)
- [Photorealistic Video Generation with Diffusion Models](http://arxiv.org/abs/2312.06662v1)
- [GenTron: Diffusion Transformers for Image and Video Generation](http://arxiv.org/abs/2312.04557v2)
- [Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation](http://arxiv.org/abs/2312.04483v1)
- [StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter](http://arxiv.org/abs/2312.00330v2)
- [ART$\boldsymbol{\cdot}$V: Auto-Regressive Text-to-Video Generation with Diffusion Models](http://arxiv.org/abs/2311.18834v1)
- [MotionZero: Exploiting Motion Priors for Zero-shot Text-to-Video Generation](http://arxiv.org/abs/2311.16635v1)
- [FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline](http://arxiv.org/abs/2311.13073v2)
- [GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning](http://arxiv.org/abs/2311.12631v3)
- [Make Pixels Dance: High-Dynamic Video Generation](http://arxiv.org/abs/2311.10982v1)
- [VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning on Language-Video Foundation Models](http://arxiv.org/abs/2311.00990v2)
- [POS: A Prompts Optimization Suite for Augmenting Text-to-Video Generation](http://arxiv.org/abs/2311.00949v3)
- [VideoCrafter1: Open Diffusion Models for High-Quality Video Generation](http://arxiv.org/abs/2310.19512v1)
- [LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation](http://arxiv.org/abs/2310.10769v1)
- [Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation](http://arxiv.org/abs/2309.16429v1)
- [Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation](http://arxiv.org/abs/2309.15818v3)
- [LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models](http://arxiv.org/abs/2309.15103v2)
- [VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning](http://arxiv.org/abs/2309.15091v2)
- [Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation](http://arxiv.org/abs/2309.03549v1)
- [VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation](http://arxiv.org/abs/2309.00398v2)
- [Dual-Stream Diffusion Net for Text-to-Video Generation](http://arxiv.org/abs/2308.08316v3)
- [Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation](http://arxiv.org/abs/2307.06940v1)
- [Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance](http://arxiv.org/abs/2306.00943v1)
- [DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation](http://arxiv.org/abs/2305.14330v3)
- [ControlVideo: Training-free Controllable Text-to-Video Generation](http://arxiv.org/abs/2305.13077v1)
- [Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](http://arxiv.org/abs/2305.10874v4)


</details>

</details>


[<small>‚áß Back to ToC</small>](#contents)

### <span id="i2v">üñºÔ∏è Image-to-Video (I2V) Generation</span>



<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***MotionStone:*** *Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.05848)

*   **[CVPR 2025]** ***MotionPro:*** *A Precise Motion Controller for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2503.00948)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/HiDream-ai/MotionPro)

*   **[CVPR 2025]** ***Through-The-Mask:*** *Mask-based Motion Trajectories for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.03059)

*   **[CVPR 2025]** *Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2503.00948)

*   **[CVPR 2025]** ***I2VGuard:*** *Safeguarding Images against Misuse in Diffusion-based Image-to-Video Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gui_I2VGuard_Safeguarding_Images_against_Misuse_in_Diffusion-based_Image-to-Video_Models_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***LeviTor:*** *3D Trajectory Oriented Image-to-Video Synthesis*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/ant-research/LeviTor)

*   **[ICLR 2025]** ***SG-I2V:*** *Self-Guided Trajectory Control in Image-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=uQjySppU9x)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kmcode1.github.io/Projects/SG-I2V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Kmcode1/SG-I2V)

*   **[ICLR 2025]** *Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ykD8a9gJvy)



   
</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

*   [Versatile Transition Generation with Image-to-Video Diffusion](http://arxiv.org/abs/2508.01698v1)
*   [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](http://arxiv.org/abs/2507.06830v1)
*   [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](http://arxiv.org/abs/2506.08456v1)
*   [Frame In-N-Out: Unbounded Controllable Image-to-Video Generation](http://arxiv.org/abs/2505.21491v1)
*   [Dynamic-I2V: Exploring Image-to-Video Generation Models via Multimodal LLM](http://arxiv.org/abs/2505.19901v3)
*   [Order Matters: On Parameter-Efficient Image-to-Video Probing for Recognizing Nearly Symmetric Actions](http://arxiv.org/abs/2503.24298v1)
*   [EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation](http://arxiv.org/abs/2503.18552v2)
*   [Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model](http://arxiv.org/abs/2503.11251v1)
*   [DreamInsert: Zero-Shot Image-to-Video Object Insertion from A Single Image](http://arxiv.org/abs/2503.10342v1)
*   [I2V3D: Controllable image-to-video generation with 3D guidance](http://arxiv.org/abs/2503.09733v1)
*   [Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think](http://arxiv.org/abs/2503.00948v1)
*   [Object-Centric Image to Video Generation with Language Guidance](http://arxiv.org/abs/2502.11655v1)
*   [RealCam-I2V: Real-World Image-to-Video Generation with Interactive Complex Camera Control](http://arxiv.org/abs/2502.10059v2)
*   [VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation](http://arxiv.org/abs/2502.07531v3)
*   [MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation](http://arxiv.org/abs/2502.04299v1)
*   [Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation](http://arxiv.org/abs/2501.03059v1)

</details>

</details>


<details>
<summary><h4>‚ú® 2024</h4></summary>




<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** *Your Image Is My Video: Reshaping the Receptive Field via Image-to-Video Differentiable AutoAugmentation and Fusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00557)

*   **[CVPR 2024]** ***TRIP:*** *Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00828)

*   **[CVPR 2024]** *Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.01751)

*   **[CVPR 2024]** *Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00779) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/AnimateAnyone)

*   **[ECCV 2024]** ***MOFA-Video:*** *Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72655-2_7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MyNiuuu/MOFA-Video)

*   **[ECCV 2024]** *$\mathrm R2$-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72940-9_24) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yeliudev/R2-Tuning)

*   **[ECCV 2024]** ***PhysGen:*** *Rigid-Body Physics-Grounded Image-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73007-8_21) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/stevenlsw/physgen)

*   **[ECCV 2024]** *Rethinking Image-to-Video Adaptation: An Object-Centric Perspective*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72775-7_19)

*   **[NeurIPS 2024]** ***HumanVid:*** *Demystifying Training Data for Camera-controllable Human Image Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/23f3a0f82d79d985b6076bc84d14f66b-Abstract-Datasets_and_Benchmarks_Track.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhenzhiwang/HumanVid)

*   **[NeurIPS 2024]** ***TPC:*** *Test-time Procrustes Calibration for Diffusion-based Human Image Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/d6fdc3002dda4c3cad3d595ac6fa5352-Abstract-Conference.html)

*   **[NeurIPS 2024]** *Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/35cb54b887e7aafe74829677cce6c5c6-Abstract-Conference.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/cond-image-leakage)

*   **[SIGGRAPH 2024]** ***I2V-Adapter:*** *A General Image-to-Video Adapter for Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1145/3641519.3657407) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/KwaiVGI/I2V-Adapter)


*   **[SIGGRAPH 2024]** ***Motion-I2V:*** *Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1145/3641519.3657497) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/G-U-N/Motion-I2V)

*   **[AAAI 2024]** *Continuous Piecewise-Affine Based Motion Model for Image Animation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v38i6.28351) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/whx-sjtu/AAAI2024-CPABMM)



</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>


*   [LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis](http://arxiv.org/abs/2412.15214v2)
*   [OmniDrag: Enabling Motion Control for Omnidirectional Image-to-Video Generation](http://arxiv.org/abs/2412.09623v1)
*   [MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation](http://arxiv.org/abs/2412.05848v1)
*   [SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation](http://arxiv.org/abs/2411.04989v3)
*   [CamI2V: Camera-Controlled Image-to-Video Diffusion Model](http://arxiv.org/abs/2410.15957v3)
*   [FrameBridge: Improving Image-to-Video Generation with Bridge Models](http://arxiv.org/abs/2410.15371v2)
*   [Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model](http://arxiv.org/abs/2406.15735v3)
*   [CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation](http://arxiv.org/abs/2406.02509v1)
*   [MOFA-Video: Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model](http://arxiv.org/abs/2405.20222v3)
*   [CamViG: Camera Aware Image-to-Video Generation with Multimodal Transformers](http://arxiv.org/abs/2405.13195v1)
*   [$R^2$-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding](http://arxiv.org/abs/2404.00801v2)
*   [TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models](http://arxiv.org/abs/2403.17005v1)
*   [Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion](http://arxiv.org/abs/2403.15194v1)
*   [Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation](http://arxiv.org/abs/2403.02827v1)
*   [AtomoVideo: High Fidelity Image-to-Video Generation](http://arxiv.org/abs/2403.01800v2)
*   [ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation](http://arxiv.org/abs/2402.04324v2)
*   [AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI](http://arxiv.org/abs/2401.01651v3)


</details>




<summary><h4>‚ú® 2023</h4></summary>


<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[ICCV 2023]** ***DreamPose:*** *Fashion Image-to-Video Synthesis via Stable Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.02073) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/johannakarras/DreamPose)

*   **[ICCV 2023]** *Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.01281) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba-mmai-research/DiST)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance](http://arxiv.org/abs/2312.03018v4)
*   [Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation](http://arxiv.org/abs/2311.17117v3)
*   [Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning](http://arxiv.org/abs/2311.15769v1)
*   [I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models](http://arxiv.org/abs/2311.04145v1)
*   [Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning](http://arxiv.org/abs/2309.07911v1)
*   [DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion](http://arxiv.org/abs/2304.06025v4)



</details>

</details>


[<small>‚áß Back to ToC</small>](#contents)

### <span id="v2v">‚úÇÔ∏è Video-to-Video (V2V) Editing</span>

<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** ***VideoHandles:*** *Editing 3D Object Compositions in Videos Using Video Generative Priors*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Koo_VideoHandles_Editing_3D_Object_Compositions_in_Videos_Using_Video_Generative_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***VideoDirector:*** *Precise Video Editing via Text-to-Video Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoDirector_Precise_Video_Editing_via_Text-to-Video_Models_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Yukun66/Video_Director)

*   **[CVPR 2025]** ***VideoSPatS:*** *Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gonzalez_VideoSPatS_Video_SPatiotemporal_Splines_for_Disentangled_Occlusion_Appearance_and_Motion_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***Align-A-Video:*** *Deterministic Reward Tuning of Image Diffusion Models for Consistent Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Align-A-Video_Deterministic_Reward_Tuning_of_Image_Diffusion_Models_for_Consistent_CVPR_2025_paper.html)

*   **[CVPR 2025]** *Unity in Diversity: Video Editing via Gradient-Latent Purification*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unity_in_Diversity_Video_Editing_via_Gradient-Latent_Purification_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***VEU-Bench:*** *Towards Comprehensive Understanding of Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/littlespray/VE-Bench)

*   **[CVPR 2025]** ***SketchVideo:*** *Sketch-based Video Generation and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SketchVideo_Sketch-based_Video_Generation_and_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/IGLICT/SketchVideo)

*   **[CVPR 2025]** ***FATE:*** *Full-head Gaussian Avatar with Textural Editing from Monocular Video*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FATE_Full-head_Gaussian_Avatar_with_Textural_Editing_from_Monocular_Video_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/zjwfufu/FateAvatar)

*   **[CVPR 2025]** *Visual Prompting for One-shot Controllable Video Editing without Inversion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Visual_Prompting_for_One-shot_Controllable_Video_Editing_without_Inversion_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***FADE:*** *Frequency-Aware Diffusion Model Factorization for Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_FADE_Frequency-Aware_Diffusion_Model_Factorization_for_Video_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/EternalEvan/FADE)

*   **[ICLR 2025]** ***VideoGrain:*** *Modulating Space-Time Attention for Multi-Grained Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=SSslAtcPB6)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/knightyxp/VideoGrain)

*   **[AAAI 2025]** ***FreeMask:*** *Rethinking the Importance of Attention Masks for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i2.32185) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/LinglingCai0314/FreeMask)

*   **[AAAI 2025]** ***EditBoard:*** *Towards a Comprehensive Evaluation Benchmark for Text-Based Video Editing Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i15.33754) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/Samchen2003/EditBoard)


*   **[AAAI 2025]** ***VE-Bench:*** *Subjective-Aligned Benchmark Suite for Text-Driven Video Editing Quality Assessment*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i7.32763) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/littlespray/VE-Bench)

*   **[AAAI 2025]** *Re-Attentional Controllable Video Diffusion Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i8.32876) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/mdswyz/ReAtCo)

*   **[WACV 2025]** ***IP-FaceDiff:*** *Identity-Preserving Facial Video Editing with Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACVW65960.2025.00031)

*   **[WACV 2025]** ***SST-EM:*** *Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACVW65960.2025.00032) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git)

*   **[WACV 2025]** ***MagicStick:*** *Controllable Video Editing via Control Handle Transformations*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00909)

*   **[WACV 2025]** ***Ada-VE:*** *Training-Free Consistent Video Editing Using Adaptive Motion Prior*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00101)

*   **[WACV 2025]** ***FastVideoEdit:*** *Leveraging Consistency Models for Efficient Text-to-Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00360) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/youyuan-zhang/FastVideoEdit)




</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>


- [Consistent Video Editing as Flow-Driven Image-to-Video Generation](http://arxiv.org/abs/2506.07713v2)
- [UNIC: Unified In-Context Video Editing](http://arxiv.org/abs/2506.04216v1)
- [DreamVE: Unified Instruction-based Image and Video Editing](http://arxiv.org/abs/2508.06080v1)
- [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](http://arxiv.org/abs/2508.00299v1)
- [Low-Cost Test-Time Adaptation for Robust Video Editing](http://arxiv.org/abs/2507.21858v1)
- [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](http://arxiv.org/abs/2507.02790v1)
- [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](http://arxiv.org/abs/2506.22868v1)
- [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](http://arxiv.org/abs/2506.22432v1)
- [DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing](http://arxiv.org/abs/2506.20967v2)
- [Good Noise Makes Good Edits: A Training-Free Diffusion-Based Video Editing with Image and Text Prompts](http://arxiv.org/abs/2506.12520v1)
- [LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware LoRA Fine-Tuning](http://arxiv.org/abs/2506.10082v3)
- [TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation](http://arxiv.org/abs/2506.07205v1)
- [FADE: Frequency-Aware Diffusion Model Factorization for Video Editing](http://arxiv.org/abs/2506.05934v1)
- [FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing](http://arxiv.org/abs/2506.05046v1)
- [FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers](http://arxiv.org/abs/2506.04213v2)
- [OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation](http://arxiv.org/abs/2506.01801v1)
- [Motion-Aware Concept Alignment for Consistent Video Editing](http://arxiv.org/abs/2506.01004v1)
- [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](http://arxiv.org/abs/2505.23134v1)
- [REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing](http://arxiv.org/abs/2505.18880v1)
- [From Shots to Stories: LLM-Assisted Video Editing with Unified Language Representations](http://arxiv.org/abs/2505.12237v1)
- [DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models](http://arxiv.org/abs/2505.07057v1)
- [Photoshop Batch Rendering Using Actions for Stylistic Video Editing](http://arxiv.org/abs/2505.01001v1)
- [Efficient Temporal Consistency in Diffusion-Based Video Editing with Adaptor Modules: A Theoretical Framework](http://arxiv.org/abs/2504.16016v1)
- [Vidi: Large Multimodal Models for Video Understanding and Editing](http://arxiv.org/abs/2504.15681v3)
- [Visual Prompting for One-shot Controllable Video Editing without Inversion](http://arxiv.org/abs/2504.14335v1)
- [CamMimic: Zero-Shot Image To Camera Motion Personalized Video Generation Using Diffusion Models](http://arxiv.org/abs/2504.09472v1)
- [VideoSPatS: Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing](http://arxiv.org/abs/2504.07146v1)
- [Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology-Inspired Computing Methods](http://arxiv.org/abs/2503.17975v2)
- [InstructVEdit: A Holistic Approach for Instructional Video Editing](http://arxiv.org/abs/2503.17641v1)
- [HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks](http://arxiv.org/abs/2503.17276v1)
- [VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation](http://arxiv.org/abs/2503.14350v2)
- [GIFT: Generated Indoor video frames for Texture-less point tracking](http://arxiv.org/abs/2503.12944v1)
- [RASA: Replace Anyone, Say Anything -- A Training-Free Framework for Audio-Driven and Universal Portrait Video Editing](http://arxiv.org/abs/2503.11571v1)
- [V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes](http://arxiv.org/abs/2503.10634v2)
- [Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space](http://arxiv.org/abs/2503.09419v1)
- [VACE: All-in-One Video Creation and Editing](http://arxiv.org/abs/2503.07598v2)
- [Get In Video: Add Anything You Want to the Video](http://arxiv.org/abs/2503.06268v1)
- [VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control](http://arxiv.org/abs/2503.05639v3)
- [VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing](http://arxiv.org/abs/2502.17258v1)
- [VideoDiff: Human-AI Video Co-Creation with Alternatives](http://arxiv.org/abs/2502.10190v1)
- [SportsBuddy: Designing and Evaluating an AI-Powered Sports Video Storytelling Tool Through Real-World Deployment](http://arxiv.org/abs/2502.08621v2)
- [AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection](http://arxiv.org/abs/2502.05433v1)
- [MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation](http://arxiv.org/abs/2502.04299v1)
- [SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing](http://arxiv.org/abs/2501.07554v1)
- [IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion](http://arxiv.org/abs/2501.07530v1)
- [Qffusion: Controllable Portrait Video Editing via Quadrant-Grid Attention Learning](http://arxiv.org/abs/2501.06438v3)
- [Text-to-Edit: Controllable End-to-End Video Ad Creation via Multimodal LLMs](http://arxiv.org/abs/2501.05884v1)
- [Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion](http://arxiv.org/abs/2501.04606v4)
- [Edit as You See: Image-guided Video Editing via Masked Motion Modeling](http://arxiv.org/abs/2501.04325v1)


</details>
</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** *A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00719) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/STEM-Inv/stem-inv)

*   **[CVPR 2024]** ***VidToMe:*** *Video Token Merging for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00715) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/VISION-SJTU/VidToMe)

*   **[CVPR 2024]** ***Video-P2P:*** *Video Editing with Cross-Attention Control*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00821) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/dvlab-research/Video-P2P)

*   **[CVPR 2024]** ***CCEdit:*** *Creative and Controllable Video Editing via Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00641)

*   **[CVPR 2024]** ***RAVE:*** *Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00622) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/rehglab/RAVE)

*   **[CVPR 2024]** ***DynVideo-E:*** *Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00732) 

*   **[CVPR 2024]** ***MaskINT:*** *Video Editing via Interpolative Non-autoregressive Masked Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00707)

*   **[CVPR 2024]** ***MotionEditor:*** *Editing Video Motion via Content-Aware Diffusion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00753) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Francis-Rings/MotionEditor)

*   **[CVPR 2024]** ***CAMEL:*** *CAusal Motion Enhancement Tailored for Lifting Text-Driven Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00867) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/zhangguiwei610/CAMEL)

*   **[ICLR 2024]** ***Ground-A-Video:*** *Zero-shot Grounded Video Editing using Text-to-image Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=28L2FCtMWq) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Ground-A-Video/Ground-A-Video)

*   **[ICLR 2024]** *Video Decomposition Prior: Editing Videos Layer by Layer*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=nfMyERXNru)

*   **[ICLR 2024]** ***FLATTEN:*** *optical FLow-guided ATTENtion for consistent text-to-video editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=JgqftqZQZ7)

*   **[ICLR 2024]** ***TokenFlow:*** *Consistent Diffusion Features for Consistent Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=lKK50q2MtV) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/omerbt/TokenFlow)

*   **[ECCV 2024]** ***VIDEOSHOP:*** *Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73254-6_14) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sfanxiang/videoshop)

*   **[ECCV 2024]** ***DragVideo:*** *Interactive Drag-Style Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72992-8_11) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RickySkywalker/DragVideo-Official)

*   **[ECCV 2024]** ***WAVE:*** *Warping DDIM Inversion Features for Zero-Shot Text-to-Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73116-7_3)

*   **[ECCV 2024]** ***DreamMotion:*** *Space-Time Self-similar Score Distillation for Zero-Shot Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73404-5_21)

*   **[ECCV 2024]** *Object-Centric Diffusion for Efficient Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72998-0_6) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Qualcomm-AI-research/object-centric-diffusion)

*   **[ECCV 2024]** *Video Editing via Factorized Diffusion Distillation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73116-7_26)

*   **[ECCV 2024]** ***SAVE:*** *Protagonist Diversification with Structure Agnostic Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72989-8_3)

*   **[ECCV 2024]** ***DNI:*** *Dilutional Noise Initialization for Diffusion Video Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73195-2_11)

*   **[ECCV 2024]** ***MagDiff:*** *Multi-alignment Diffusion for High-Fidelity Video Generation and Editing*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72649-1_12)

*   **[ECCV 2024]** ***DeCo:*** *Decoupled Human-Centered Diffusion Video Editing with Motion Consistency*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72784-9_20)





</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>


- [MAKIMA: Tuning-free Multi-Attribute Open-domain Video Editing via Mask-Guided Attention Modulation](http://arxiv.org/abs/2412.19978v1)
- [DriveEditor: A Unified 3D Information-Guided Framework for Controllable Object Editing in Driving Scenes](http://arxiv.org/abs/2412.19458v2)
- [Re-Attentional Controllable Video Diffusion Editing](http://arxiv.org/abs/2412.11710v1)
- [MoViE: Mobile Diffusion for Video Editing](http://arxiv.org/abs/2412.06578v1)
- [DIVE: Taming DINO for Subject-Driven Video Editing](http://arxiv.org/abs/2412.03347v2)
- [Trajectory Attention for Fine-grained Video Motion Control](http://arxiv.org/abs/2411.19324v1)
- [VideoDirector: Precise Video Editing via Text-to-Video Models](http://arxiv.org/abs/2411.17592v3)
- [StableV2V: Stablizing Shape Consistency in Video-to-Video Editing](http://arxiv.org/abs/2411.11045v1)
- [OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models](http://arxiv.org/abs/2411.10501v1)
- [A Reinforcement Learning-Based Automatic Video Editing Method Using Pre-trained Vision-Language Model](http://arxiv.org/abs/2411.04942v1)
- [Taming Rectified Flow for Inversion and Editing](http://arxiv.org/abs/2411.04746v3)
- [AutoVFX: Physically Realistic Video Editing from Natural Language Instructions](http://arxiv.org/abs/2411.02394v1)
- [Shaping a Stabilized Video by Mitigating Unintended Changes for Concept-Augmented Video Editing](http://arxiv.org/abs/2410.12526v2)
- [RNA: Video Editing with ROI-based Neural Atlas](http://arxiv.org/abs/2410.07600v1)
- [FreeMask: Rethinking the Importance of Attention Masks for Zero-Shot Video Editing](http://arxiv.org/abs/2409.20500v1)
- [DNI: Dilutional Noise Initialization for Diffusion Video Editing](http://arxiv.org/abs/2409.13037v1)
- [Blended Latent Diffusion under Attention Control for Real-World Video Editing](http://arxiv.org/abs/2409.03514v1)
- [DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion Consistency](http://arxiv.org/abs/2408.07481v1)
- [InVi: Object Insertion In Videos Using Off-the-Shelf Diffusion Models](http://arxiv.org/abs/2407.10958v1)
- [MVOC: A Training-Free Multiple Video Object Composition Method with Diffusion Models](http://arxiv.org/abs/2406.15829v1)
- [VIA: Unified Spatiotemporal Video Adaptation Framework for Global and Local Video Editing](http://arxiv.org/abs/2406.12831v3)
- [COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing](http://arxiv.org/abs/2406.08850v2)
- [NaRCan: Natural Refined Canonical Image with Integration of Diffusion Prior for Video Editing](http://arxiv.org/abs/2406.06523v2)
- [FRAG: Frequency Adapting Group for Diffusion Video Editing](http://arxiv.org/abs/2406.06044v2)
- [Zero-Shot Video Editing through Adaptive Sliding Score Distillation](http://arxiv.org/abs/2406.04888v2)
- [Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion Prior](http://arxiv.org/abs/2406.04873v2)
- [Enhancing Temporal Consistency in Video Editing by Reconstructing Videos with 3D Gaussian Splatting](http://arxiv.org/abs/2406.02541v4)
- [Temporally Consistent Object Editing in Videos using Extended Attention](http://arxiv.org/abs/2406.00272v1)
- [MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion](http://arxiv.org/abs/2405.20325v1)
- [Streaming Video Diffusion: Online Video Editing with Diffusion Models](http://arxiv.org/abs/2405.19726v1)
- [I2VEdit: First-Frame-Guided Video Editing via Image-to-Video Diffusion Models](http://arxiv.org/abs/2405.16537v1)
- [ReVideo: Remake a Video with Motion and Content Control](http://arxiv.org/abs/2405.13865v1)
- [Slicedit: Zero-Shot Video Editing With Text-to-Image Diffusion Models Using Spatio-Temporal Slices](http://arxiv.org/abs/2405.12211v1)
- [GenVideo: One-shot Target-image and Shape Aware Video Editing using T2I Diffusion Models](http://arxiv.org/abs/2404.12541v1)
- [Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model](http://arxiv.org/abs/2404.09967v2)
- [S3Editor: A Sparse Semantic-Disentangled Self-Training Framework for Face Video Editing](http://arxiv.org/abs/2404.08111v1)
- [ExpressEdit: Video Editing with Natural Language and Sketching](http://arxiv.org/abs/2403.17693v1)
- [EVA: Zero-shot Accurate Attributes and Multi-Object Video Editing](http://arxiv.org/abs/2403.16111v1)
- [Edit3K: Universal Representation Learning for Video Editing Components](http://arxiv.org/abs/2403.16048v2)
- [Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion](http://arxiv.org/abs/2403.14617v3)
- [AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks](http://arxiv.org/abs/2403.14468v4)
- [DreamMotion: Space-Time Self-Similar Score Distillation for Zero-Shot Video Editing](http://arxiv.org/abs/2403.12002v2)
- [EffiVED: Efficient Video Editing via Text-instruction Diffusion Models](http://arxiv.org/abs/2403.11568v2)
- [AICL: Action In-Context Learning for Video Diffusion Model](http://arxiv.org/abs/2403.11535v2)
- [Video Editing via Factorized Diffusion Distillation](http://arxiv.org/abs/2403.09334v2)
- [VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis](http://arxiv.org/abs/2403.08764v1)
- [FastVideoEdit: Leveraging Consistency Models for Efficient Text-to-Video Editing](http://arxiv.org/abs/2403.06269v2)
- [Place Anything into Any Video](http://arxiv.org/abs/2402.14316v1)
- [UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing](http://arxiv.org/abs/2402.13185v4)
- [Anything in Any Scene: Photorealistic Video Object Insertion](http://arxiv.org/abs/2401.17509v1)
- [Object-Centric Diffusion for Efficient Video Editing](http://arxiv.org/abs/2401.05735v3)
- [VASE: Object-Centric Appearance and Shape Manipulation of Real Videos](http://arxiv.org/abs/2401.02473v1)
- [Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions](http://arxiv.org/abs/2401.01827v1)



</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="controllable">üïπÔ∏è Controllable Video Generation</span>
<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***IM-Zero:*** *Instance-level Motion Controllable Video Generation in a Zero-shot Manner*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IM-Zero_Instance-level_Motion_Controllable_Video_Generation_in_a_Zero-shot_Manner_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***AnimateAnything:*** *Consistent and Controllable Animation for Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Lei_AnimateAnything_Consistent_and_Controllable_Animation_for_Video_Generation_CVPR_2025_paper.html)

*   **[CVPR 2025]** *Customized Condition Controllable Generation for Video Soundtrack*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Customized_Condition_Controllable_Generation_for_Video_Soundtrack_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/FanQi-AI/CCCG-Video-Soundtrack) 

*   **[CVPR 2025]** ***StarGen:*** *A Spatiotemporal Autoregression Framework with Video Diffusion Model for Scalable and Controllable Scene Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_StarGen_A_Spatiotemporal_Autoregression_Framework_with_Video_Diffusion_Model_for_CVPR_2025_paper.html)

*   **[ICLR 2025]** ***MotionClone:*** *Training-Free Motion Cloning for Controllable Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=aY3L65HgHJ) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/LPengYang/MotionClone) 

*   **[ICLR 2025]** ***Ctrl-U:*** *Robust Conditional Image Generation via Uncertainty-aware Reward Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=eC2ICbECNM) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/FanQi-AI/CCCG-Video-Soundtrack) 

*   **[AAAI 2025]** ***CAGE:*** *Unsupervised Visual Composition and Animation for Controllable Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i15.33775) [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?style=for-the-badge)](https://github.com/Araachie/cage)

*   **[AAAI 2025]** ***TrackGo:*** *A Flexible and Efficient Method for Controllable Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i10.33167)


*   **[WACV 2025]** *Fine-grained Controllable Video Generation via Object Appearance and Context*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00364)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](http://arxiv.org/abs/2506.03150v1)
*   [ATI: Any Trajectory Instruction for Controllable Video Generation](http://arxiv.org/abs/2505.22944v3)
*   [CamContextI2V: Context-aware Controllable Video Generation](http://arxiv.org/abs/2504.06022v1)
*   [Any2Caption: Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)
*   [MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance](http://arxiv.org/abs/2503.16421v2)
*   [MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent](http://arxiv.org/abs/2502.03207v1)
*   [Controllable Video Generation with Provable Disentanglement](http://arxiv.org/abs/2502.02690v2)




</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2024]** ***360DVD:*** *Controllable Panorama Video Generation with 360-Degree Video Diffusion Model*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00660) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Akaneqwq/360DVD)

*   **[CVPR 2024]** ***Panacea:*** *Panoramic and Controllable Video Generation for Autonomous Driving*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00659) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wenyuqing/panacea)

*   **[AAAI 2024]** *Decouple Content and Motion for Conditional Image-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v38i5.28277)






</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving](http://arxiv.org/abs/2408.07605v1)
*   [InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions](http://arxiv.org/abs/2402.03040v1)



</details>

</details>

<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2023]** *Conditional Image-to-Video Generation with Latent Flow Diffusion Models*<br>
    [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52729.2023.01769) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nihaomiao/CVPR23_LFDM)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [Controllable Video Generation by Learning the Underlying Dynamical System with Neural ODE](http://arxiv.org/abs/2303.05323v2)



</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

---

## <span id="datasets">üóÇÔ∏è Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **WebVid-10M** | 2021 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2104.00650) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://m-bain.github.io/webvid-dataset/) |
| **FETV** | 2023 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2311.01813v3) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/llyx97/FETV) |
| **Panda-70M** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2402.19479) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://tsaishienchen.github.io/panda-70m/) |
| **SafeSora** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.14477v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/PKU-Alignment/safe-sora) |
| **ChronoMagic-Bench**| 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.18522v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/PKU-YuanGroup/ChronoMagic-Bench) |
| **T2V-CompBench** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2407.14505v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/KaiyueSun98/T2V-CompBench) |
| **VidGen-1M** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2408.02629v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/SAIS-FUXI/VidGen) |
| **PhyGenBench** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2410.05363v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/OpenGVLab/PhyGenBench) |
| **DH-FaceVid-1K** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2410.07151v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/DH-FaceVid-1K/DH-FaceVid-1K) |
| **StoryEval** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2412.16211v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/ypwang61/StoryEval) |
| **OpenVid-1M** | 2025 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=j7kdXSrISM) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://nju-pcalab.github.io/projects/openvid/) |
| **HumanVid** | 2024 | Image, Video | Image-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/23f3a0f82d79d985b6076bc84d14f66b-Abstract-Datasets_and_Benchmarks_Track.html) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zhenzhiwang/HumanVid) |
| **TIP-I2V** | 2024 | Text, Image, Video | Image-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.04709v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/WangWenhao0716/TIP-I2V) |
| **TC-Bench** | 2024 | Text, Image, Video | Text-to-Video, Image-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.08656v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/weixi-feng/tc-bench) |
| **AnimeShooter** | 2025 | Text, Image, Video | Text-to-Video, Image-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2506.03126v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://qiulu66.github.io/animeshooter/) |
| **VE-Bench** | 2024 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2408.11481v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/littlespray/VE-Bench) |
| **DAVIS-Edit** | 2024 | Text, Video, Image | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.11045v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/AlonzoLeeeooo/StableV2V) |
| **VIVID-10M** | 2024 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.15260v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://huggingface.co/datasets/KwaiVGI/VIVID-10M) |
| **Se√±orita-2M** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2502.06734v3) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zibojia/SENORITA) |
| **FiVE-Bench** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2503.13684v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/MinghanLi/FiVE-Bench) |
| **InsViE-1M** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2503.20287v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/langmanbusi/InsViE) |

[<small>‚áß Back to ToC</small>](#contents)

---

## <span id="about-us">üéì About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Video Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

‚û°Ô∏è **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI) ÊòØ‰∏ÄÂÆ∂Áî±È°∂Â∞ñÁ†îÁ©∂ËÄÖÁªÑÊàêÔºåËá¥Âäõ‰∫é‰∏∫ÂÖ®ÁêÉÈ´òÊ†°Â≠¶ÁîüÊèê‰æõÈ´òË¥®Èáè1V1ÁßëÁ†îËæÖÂØºÁöÑ‰∏ì‰∏öÊú∫ÊûÑ„ÄÇÊàë‰ª¨ÁöÑ‰ΩøÂëΩÊòØÂ∏ÆÂä©Â≠¶ÁîüÂüπÂÖªÂá∫Ëâ≤ÂçìË∂äÁöÑÁßëÁ†îÊäÄËÉΩÔºåÂú®È°∂Á∫ß‰ºöËÆÆÂíåÊúüÂàä‰∏äÂèëË°®Ëá™Â∑±ÁöÑÊàêÊûú„ÄÇ

Áª¥Êä§‰∏Ä‰∏™GitHubË∞ÉÁ†î‰ªìÂ∫ìÈúÄË¶ÅÂ∑®Â§ßÁöÑÁ≤æÂäõÔºåÊ≠£Â¶ÇÂÆåÊàê‰∏ÄÁØáÈ´òË¥®ÈáèÁöÑËÆ∫Êñá‰∏ÄÊ†∑ÔºåÁ¶ª‰∏çÂºÄ‰∏ìÊ≥®ÁöÑÊäïÂÖ•Âíå‰∏ì‰∏öÁöÑÊåáÂØº„ÄÇÂ¶ÇÊûúÊÇ®Â∏åÊúõÂú®Ëá™Â∑±ÁöÑÁ†îÁ©∂È°πÁõÆ‰∏≠ÔºåËé∑ÂæóÊù•Ëá™È°∂Â∞ñÂ≠¶ËÄÖÁöÑ‰∏ÄÂØπ‰∏ÄÊîØÊåÅÔºåÊàë‰ª¨ËØöÈÇÄÊÇ®‰∏éÊàë‰ª¨ÂèñÂæóËÅîÁ≥ª„ÄÇ

‚û°Ô∏è **Ê¨¢ËøéÈÄöËøá [ÂæÆ‰ø°](assets/wechat.jpg) Êàñ [ÈÇÆ‰ª∂](mailto:your.email@example.com) ËÅîÁ≥ªÊàë‰ª¨ÔºåÂºÄÂêØÊÇ®ÁöÑÁßëÁ†î‰πãÊóÖ„ÄÇ**


[<small>‚áß Back to ToC</small>](#contents)

---



## <span id="contributing">ü§ù Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.
