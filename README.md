<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/logo_run_cn.png" alt="QuenithAI Logo" width="200" height="200">
  </a>
</div>

<div align="center">
  <h1>Awesome Video Generation by QuenithAI</h1>
  <p>A curated collection of papers, models, and resources for the field of Video Generation.</p>
  <p>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg" alt="Awesome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/Video-Generation-Paper-List/pulls"><img src="https://img.shields.io/badge/PRs-Welcome-brightgreen.svg?style=flat-square" alt="PRs Welcome"></a>
    &nbsp;
    <a href="https://github.com/QuenithAI/Video-Generation-Paper-List/issues"><img src="https://img.shields.io/badge/Issues-Welcome-orange?style=flat-square" alt="Issues Welcome"></a>
  </p>
</div>

> [!NOTE]
> This repository is proudly maintained by the frontline research mentors at **QuenithAI (Â∫îËææÂ≠¶ÊúØ)**. It aims to provide the most comprehensive and cutting-edge map of papers and technologies in the field of video generation.
>
> Your contributions are also vital‚Äîfeel free to [open an issue](https://github.com/QuenithAI/Video-Generation-Paper-List/issues) or [submit a pull request](https://github.com/QuenithAI/Video-Generation-Paper-List/pulls) to become a collaborator of this repository. We expect your participation!
> 
>  If you require expert 1-on-1 guidance on your submissions to top-tier conferences and journals, we invite you to **contact us via [WeChat](assets/wechat.jpg) or [E-mail]((mailto:christzhaung@gmail.com))**.
>
>
> ---
>
> Êú¨‰ªìÂ∫ìÁî± **„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI)** ÁöÑ‰∏ÄÁ∫øÁßëÁ†îÂØºÂ∏àÂõ¢ÈòüÂÄæÂäõÊâìÈÄ†Âπ∂ÊåÅÁª≠Áª¥Êä§ÔºåÊó®Âú®‰∏∫ÊÇ®ÂëàÁé∞ËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÊúÄÂÖ®Èù¢„ÄÅÊúÄÂâçÊ≤øÁöÑËßÜÈ¢ëÁîüÊàêÈ¢ÜÂüüÁöÑËÆ∫Êñá„ÄÇ
>
> ÊÇ®ÁöÑË¥°ÁåÆÂØπÊàë‰ª¨ÂíåÁ§æÂå∫Êù•ËØ¥Ëá≥ÂÖ≥ÈáçË¶Å‚Äî‚ÄîÊàë‰ª¨ËØöÈÇÄÊúâÂøó‰πãÂ£´ÈÄöËøá [open an issue](https://github.com/QuenithAI/Video-Generation-Paper-List/issues) Êàñ [submit a pull request](https://github.com/QuenithAI/Video-Generation-Paper-List/pulls) Êù•Êàê‰∏∫Ëøô‰∏™È°πÁõÆÁöÑÂêà‰ΩúËÄÖ‰πã‰∏ÄÔºåÊúüÂæÖÊÇ®ÁöÑÂä†ÂÖ•ÔºÅ
> 
> Â¶ÇÊûúÊÇ®Âú®ÂÜ≤Âà∫ÁßëÁ†îÈ°∂‰ºöÁöÑÈÅìË∑Ø‰∏äÈúÄË¶Å‰∏ì‰∏öÁöÑ1V1ÊåáÂØºÔºåÊ¨¢Ëøé**ÈÄöËøá[ÂæÆ‰ø°](assets/wechat.jpg)Êàñ[ÈÇÆ‰ª∂](mailto:christzhaung@gmail.com)ËÅîÁ≥ªÊàë‰ª¨**„ÄÇ


<details>
<summary><strong>‚ö° Latest Updates</strong></summary>

- **(Nov 19th, 2025)**: We have updated all accepted papers in AAAI 2026.
- **(Sep 13th, 2025)**: Add a new direction: [üéØ Reinforcement Learning for Video Generation](#-reinforcement-learning-for-video-generation).
- **(Aug 21th, 2025)**: Add a new direction: [üó£Ô∏è Audio-Driven Video Generation](#Ô∏è-audio-driven-video-generation).
- **(Aug 20th, 2025)**: Initial commit and repository structure established.

</details>

---

<!-- omit in toc -->
## <span id="contents">üìö Table of Contents</span>
- [üìú Papers \& Models](#-papers--models)
  - [‚úçÔ∏è Survey Papers](#Ô∏è-survey-papers)
  - [üé• Text-to-Video (T2V) Generation](#-text-to-video-t2v-generation)
  - [üñºÔ∏è Image-to-Video (I2V) Generation](#Ô∏è-image-to-video-i2v-generation)
  - [‚úÇÔ∏è Video-to-Video (V2V) Editing](#Ô∏è-video-to-video-v2v-editing)
  - [üïπÔ∏è Controllable Video Generation](#Ô∏è-controllable-video-generation)
  - [üó£Ô∏è Audio-Driven Video Generation](#Ô∏è-audio-driven-video-generation)
  - [üíÉ Human Image Animation](#-human-image-animation)
  - [‚ö° Fast Video Generation (Acceleration)](#-fast-video-generation-acceleration)
  - [üéØ Reinforcement Learning for Video Generation](#-reinforcement-learning-for-video-generation)
- [üóÇÔ∏è Datasets](#Ô∏è-datasets)
- [üéì About Us](#-about-us)
- [ü§ù Contributing](#-contributing)
- [üí¨ Join the Community](#-join-the-community)

---

## <span id="papers">üìú Papers & Models</span>

### <span id="survey">‚úçÔ∏è Survey Papers</span>
- [Controllable Video Generation: A Survey](http://arxiv.org/abs/2507.16869v1)
- [Diffusion Model-Based Video Editing: A Survey](http://arxiv.org/abs/2407.07111v1)
- [From Sora What We Can See: A Survey of Text-to-Video Generation](http://arxiv.org/abs/2405.10674v1)
- [A Comprehensive Survey on Human Video Generation: Challenges, Methods, and Insights](https://arxiv.org/abs/2407.08428)
- [A Survey on Video Diffusion Models](https://arxiv.org/abs/2310.10647v1)
- [Video Diffusion Models: A Survey](https://arxiv.org/abs/2405.03150)
- [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
- [Video Diffusion Generation: Comprehensive Review and Open Problems](https://link.springer.com/article/10.1007/s10462-025-11331-6)

### <span id="t2v">üé• Text-to-Video (T2V) Generation</span>
<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[AAAI 2026]** ***FlashVideo:*** *Flowing Fidelity to Detail for Efficient High-Resolution Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2502.05179) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://foundationvision.github.io/flashvideo-page/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/FoundationVision/FlashVideo) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/FoundationVision/FlashVideo)

* **[AAAI 2026]** ***DreamRunner:*** *Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.16657) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zunwang1.github.io/DreamRunner) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wz0919/DreamRunner)

* **[AAAI 2026]** ***GenMAC:*** *Compositional Text-to-Video Generation with Multi-Agent Collaboration*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.04440) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://karine-h.github.io/GenMAC/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Karine-Huang/GenMAC)

* **[AAAI 2026]** ***EmoVid:*** *A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2511.11002) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zane-zyqiu.github.io/EmoVid)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>




<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** *AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2411.17221)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wangjiarui153/AIGV-Assessor)

*   **[CVPR 2025]** *Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2410.06241)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Bujiazi/ByTheWay)

*   **[CVPR 2025]** *Retrieval-Augmented Prompt Optimization for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2405.15579)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://whynothaha.github.io/Prompt_optimizer/RAPO.html)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Vchitect/RAPO)

*   **[CVPR 2025]** *Identity-Preserving Text-to-Video Generation by Frequency Decomposition*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2411.17440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pku-yuangroup.github.io/ConsisID/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PKU-YuanGroup/ConsisID)

*   **[CVPR 2025]** *Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2504.06861)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://djagpal02.github.io/EIDT-V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/djagpal02/EIDT-V)

*   **[CVPR 2025]** ***TransPixeler:*** *Advancing Text-to-Video Generation with Transparency*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.03006)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wileewang.github.io/TransPixar/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wileewang/TransPixeler)

*   **[CVPR 2025]** *LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.00596)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pittisl/PhyT2V)

*   **[CVPR 2025]** *Improving Text-to-Video Generation via Instance-aware Structured Caption*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.09283)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NJU-PCALab/InstanceCap)

*   **[CVPR 2025]** *Compositional Text-to-Video Generation with Blob Video Representations*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.07647)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://blobgen-vid2.github.io/)

*   **[CVPR 2025]** *Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.09856)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lineargen.github.io/)

* **[ICCV 2025]** ***T2Bs:***¬†*Text‚Äëto‚ÄëCharacter Blendshapes via Video¬†Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2509.10678) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/T2Bs/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/snap-research/T2Bs) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](#)

* **[ICCV 2025]** ***Animate¬†Your¬†Word:***¬†*Bringing¬†Text¬†to¬†Life via¬†Video¬†Diffusion¬†Prior*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2404.11614) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://animate-your-word.github.io/demo/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zliucz/animate-your-word) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](#)

* **[NeurIPS 2025]** ***Safe‚ÄëSora:***¬†*Safe¬†Text‚Äëto‚ÄëVideo Generation via¬†Graphical¬†Watermarking*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2505.12667) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pku-alignment.github.io/SafeSora/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PKU-Alignment/safe-sora) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/PKU-Alignment/SafeSora)

* **[ICCV 2025]** ***Prompt‚ÄëA‚ÄëVideo:***¬†*Prompt¬†Your¬†Video Diffusion¬†Model via¬†Preference‚ÄëAligned¬†LLM*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.15156) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](#) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jiyt17/Prompt-A-Video) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/jiyatai/Prompt-A-Video-SFT-data)

* **[ICCV 2025]** ***MotionShot:***¬†*Adaptive¬†Motion¬†Transfer across Arbitrary Objects for Text‚Äëto‚ÄëVideo¬†Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2507.16310) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://motionshot.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](#) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](#)

* **[ICCV 2025]** ***TITAN‚ÄëGuide:***¬†*Taming¬†Inference‚ÄëTime Alignment for¬†Guided¬†Text‚Äëto‚ÄëVideo Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2508.00289) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://titanguide.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](#) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](#)

* **[ICCV 2025]** ***Video‚ÄëT1:***¬†*Test‚ÄëTime¬†Scaling for¬†Video¬†Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.18942) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://liuff19.github.io/Video-T1/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/liuff19/Video-T1) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](#)

* **[ICCV 2025]** ***AnimateYourMesh:***¬†*Feed‚ÄëForward¬†4D¬†Foundation Model for¬†Text‚ÄëDriven¬†Mesh¬†Animation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2506.09982) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://animateanymesh.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tymochko/AnimateAnyMesh) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/JarrentWu/DyMesh_32f)


*   **[ICLR 2025]** ***OpenVid-1M:*** *A Large-Scale High-Quality Dataset for Text-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=j7kdXSrISM)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nju-pcalab.github.io/projects/openvid/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/NJU-PCALab/OpenVid-1M)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/nkp37/OpenVid-1M)

*   **[ICLR 2025]** ***CogVideoX:*** *Text-to-Video Diffusion Models with An Expert Transformer*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=LQzN6TRFg9)

*   **[ICLR 2025]** *Pyramidal Flow Matching for Efficient Video Generative Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=66NzcRQuOq)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pyramid-flow.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/Pyramid-Flow)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/pyramid-flow-miniflux)

* **[NeurIPS 2025]** *Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/116297) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://seaweed-apt.com/2)

* **[NeurIPS 2025]** ***ViewPoint:*** *Panoramic Video Generation with Pretrained Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/119145) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/ViewPoint)

* **[NeurIPS 2025]** ***PanoWan:*** *Lifting Diffusion Video Generation Models to 360¬∞ with Latitude/Longitude-Aware Mechanisms*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/119701) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://panowan.variantconst.com/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/VariantConst/PanoWan)


</details>


<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [LayerT2V: Interactive Multi-Object Trajectory Layering for Video Generation](https://arxiv.org/abs/2508.04228) [![GitHub Stars](https://img.shields.io/github/stars/kr-panghu/LayerT2V?style=social)](https://github.com/kr-panghu/LayerT2V) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kr-panghu.github.io/LayerT2V/)
- [S¬≤Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation](https://arxiv.org/abs/2508.04016) [![GitHub Stars](https://img.shields.io/github/stars/wlfeng0509/s2q-vdit?style=social)](https://github.com/wlfeng0509/s2q-vdit)  
- [LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation](https://arxiv.org/abs/2508.03694) [![GitHub Stars](https://img.shields.io/github/stars/vchitect/LongVie?style=social)](https://github.com/vchitect/LongVie)  
- [Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation](https://arxiv.org/abs/2508.03334) [![GitHub Stars](https://img.shields.io/github/stars/Tele-AI/MMPL?style=social)](https://github.com/Tele-AI/MMPL) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://nju-xunzhixiang.github.io/Anchor-Forcing-Page/)
- [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](https://arxiv.org/abs/2508.03254) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://jiiiisoo.github.io/VIP.github.io/)
- [QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots](http://arxiv.org/abs/2508.02512v1) [![GitHub Stars](https://img.shields.io/github/stars/losehu/QuaDreamer?style=social)](https://github.com/losehu/QuaDreamer)
- [PoseGuard: Pose-Guided Generation with Safety Guardrails](http://arxiv.org/abs/2508.02476v1)
- [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](http://arxiv.org/abs/2508.00312v1) [![GitHub Stars](https://img.shields.io/github/stars/Sumutan/GV-VAD?style=social)](https://github.com/Sumutan/GV-VAD)
- [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](http://arxiv.org/abs/2507.22360v1)
- [Compositional Video Synthesis by Temporal Object-Centric Learning](http://arxiv.org/abs/2507.20855v1)
- [Enhancing Scene Transition Awareness in Video Generation via Post-Training](http://arxiv.org/abs/2507.18046v1)
- [Yume: An Interactive World Generation Model](http://arxiv.org/abs/2507.17744v1) [![GitHub Stars](https://img.shields.io/github/stars/stdstu12/YUME?style=social)](https://github.com/stdstu12/YUME) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://stdstu12.github.io/YUME-Project/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/stdstu123/Yume-I2V-540P)
- [EndoGen: Conditional Autoregressive Endoscopic Video Generation](http://arxiv.org/abs/2507.17388v1) [![GitHub Stars](https://img.shields.io/github/stars/CUHK-AIM-Group/EndoGen?style=social)](https://github.com/CUHK-AIM-Group/EndoGen)
- [MotionShot: Adaptive Motion Transfer across Arbitrary Objects for Text-to-Video Generation](http://arxiv.org/abs/2507.16310v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://motionshot.github.io)
- [PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation](http://arxiv.org/abs/2507.16116v1) [![GitHub Stars](https://img.shields.io/github/stars/Yaofang-Liu/Pusa-VidGen?style=social)](https://github.com/Yaofang-Liu/Pusa-VidGen) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://yaofang-liu.github.io/Pusa_Web/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/RaphaelLiu/Pusa-Wan2.2-V1)
- [TokensGen: Harnessing Condensed Tokens for Long Video Generation](http://arxiv.org/abs/2507.15728v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vicky0522.github.io/tokensgen-webpage/)
- [Conditional Video Generation for High-Efficiency Video Compression](http://arxiv.org/abs/2507.15269v1)
- [Taming Diffusion Transformer for Real-Time Mobile Video Generation](http://arxiv.org/abs/2507.13343v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/mobile_video_dit/)
- [LoViC: Efficient Long Video Generation with Context Compression](http://arxiv.org/abs/2507.12952v1)
- [World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving](http://arxiv.org/abs/2507.12762v1)
- [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](http://arxiv.org/abs/2507.11245v2)
- [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](http://arxiv.org/abs/2507.08801v1) [![GitHub Stars](https://img.shields.io/github/stars/alibaba-damo-academy/Lumos?style=social)](https://github.com/alibaba-damo-academy/Lumos)
- [Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers](http://arxiv.org/abs/2507.08422v1)
- [Geometry Forcing: Marrying Video Diffusion and 3D Representation for Consistent World Modeling](http://arxiv.org/abs/2507.07982v1)
- [Martian World Models: Controllable Video Synthesis with Physically Accurate 3D Reconstructions](http://arxiv.org/abs/2507.07978v1)
- [Scaling RL to Long Videos](http://arxiv.org/abs/2507.07966v3)
- [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](http://arxiv.org/abs/2507.06739v1)
- [Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions](http://arxiv.org/abs/2507.06133v2)
- [Omni-Video: Democratizing Unified Video Understanding and Generation](http://arxiv.org/abs/2507.06119v2)
- [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](http://arxiv.org/abs/2507.05963v2)
- [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](http://arxiv.org/abs/2507.05675v1)
- [Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations](http://arxiv.org/abs/2507.04705v1) [![GitHub Stars](https://img.shields.io/github/stars/PKU-YuanGroup/ConsisID?style=social)](https://github.com/PKU-YuanGroup/ConsisID) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pku-yuangroup.github.io/ConsisID/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/BestWishYsh/ConsisID-preview-Data)
- [PresentAgent: Multimodal Agent for Presentation Video Generation](http://arxiv.org/abs/2507.04036v1)
- [RefTok: Reference-Based Tokenization for Video Generation](http://arxiv.org/abs/2507.02862v1)
- [Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching](http://arxiv.org/abs/2507.02860v1)
- [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](http://arxiv.org/abs/2507.02608v1)
- [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](http://arxiv.org/abs/2507.01945v2) [![GitHub Stars](https://img.shields.io/github/stars/CN-makers/LongAnimation?style=social)](https://github.com/CN-makers/LongAnimation) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cn-makers.github.io/long_animation_web/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/CNcreator0331/LongAnimation)
- [LLM-based Realistic Safety-Critical Driving Video Generation](http://arxiv.org/abs/2507.01264v1)
- [Geometry-aware 4D Video Generation for Robot Manipulation](http://arxiv.org/abs/2507.01099v1)
- [Populate-A-Scene: Affordance-Aware Human Video Generation](http://arxiv.org/abs/2507.00334v1)
- [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](http://arxiv.org/abs/2507.00162v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://freelongvideo.github.io)
- [Epona: Autoregressive Diffusion World Model for Autonomous Driving](http://arxiv.org/abs/2506.24113v1) [![GitHub Stars](https://img.shields.io/github/stars/Kevin-thu/Epona?style=social)](https://github.com/Kevin-thu/Epona) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kevin-thu.github.io/Epona/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Kevin-thu/Epona)
- [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](http://arxiv.org/abs/2506.23858v1) [![GitHub Stars](https://img.shields.io/github/stars/KwaiVGI/VMoBA?style=social)](https://github.com/KwaiVGI/VMoBA)
- [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](http://arxiv.org/abs/2506.23690v1)
- [Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation](http://arxiv.org/abs/2506.19852v1)
- [GenHSI: Controllable Generation of Human-Scene Interaction Videos](http://arxiv.org/abs/2506.19840v1)
- [SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution](http://arxiv.org/abs/2506.19838v2)
- [Training-Free Motion Customization for Distilled Video Generators with Adaptive Test-Time Distillation](http://arxiv.org/abs/2506.19348v1)
- [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](http://arxiv.org/abs/2506.18903v2) [![GitHub Stars](https://img.shields.io/github/stars/runjiali-rl/vmem?style=social)](https://github.com/runjiali-rl/vmem)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://v-mem.github.io/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/liguang0115/vmem)
- [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](http://arxiv.org/abs/2506.18899v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://filmaster-ai.github.io)
- [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](http://arxiv.org/abs/2506.18655v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://wwenxu.github.io/RDPO)
- [Emergent Temporal Correspondences from Video Diffusion Transformers](http://arxiv.org/abs/2506.17220v2) [![GitHub Stars](https://img.shields.io/github/stars/cvlab-kaist/DiffTrack?style=social)](https://github.com/cvlab-kaist/DiffTrack)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cvlab-kaist.github.io/DiffTrack/)
- [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](http://arxiv.org/abs/2506.17201v1) [![GitHub Stars](https://img.shields.io/github/stars/Tencent-Hunyuan/Hunyuan-GameCraft-1.0?style=social)](https://github.com/Tencent-Hunyuan/Hunyuan-GameCraft-1.0)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://hunyuan-gamecraft.github.io/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/tencent/Hunyuan-GameCraft-1.0)
- [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](http://arxiv.org/abs/2506.16119v1)
- [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](http://arxiv.org/abs/2506.16054v1)
- [Causally Steered Diffusion for Automated Video Counterfactual Generation](http://arxiv.org/abs/2506.14404v2) [![GitHub Stars](https://img.shields.io/github/stars/nysp78/counterfactual-video-generation?style=social)](https://github.com/nysp78/counterfactual-video-generation)
- [VideoMAR: Autoregressive Video Generation with Continuous Tokens](httpxiv.org/abs/2506.14168v2)
- [M4V: Multi-Modal Mamba for Text-to-Video Generation](http://arxiv.org/abs/2506.10915v1) [![GitHub Stars](https://img.shields.io/github/stars/huangjch526/M4V?style=social)](https://github.com/huangjch526/M4V)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://huangjch526.github.io/M4V_project/)
- [GigaVideo‚Äë1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning](http://arxiv.org/abs/2506.10639v1)
- [DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers](http://arxiv.org/abs/2506.10568v1)
- [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](http://arxiv.org/abs/2506.09350v1)
- [MagCache: Fast Video Generation with Magnitude-Aware Cache](http://arxiv.org/abs/2506.09045v1) [![GitHub Stars](https://img.shields.io/github/stars/Zehong-Ma/MagCache?style=social)](https://github.com/Zehong-Ma/MagCache)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://zehong-ma.github.io/MagCache/)
- [Seedance¬†1.0: Exploring the Boundaries of Video Generation Models](http://arxiv.org/abs/2506.09113v2)
- [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](http://arxiv.org/abs/2506.08351v1)
- [Self¬†Forcing: Bridging the Train‚ÄëTest Gap in Autoregressive Video Diffusion](http://arxiv.org/abs/2506.08009v1) [![GitHub Stars](https://img.shields.io/github/stars/guandeh17/Self-Forcing?style=social)](https://github.com/guandeh17/Self-Forcing)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://self-forcing.github.io)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/gdhe17/Self-Forcing)
- [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](http://arxiv.org/abs/2506.07280v2) [![GitHub Stars](https://img.shields.io/github/stars/PabloAcuaaviva/Gen2Gen?style=social)](https://github.com/PabloAcuaaviva/Gen2Gen)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pabloacuaviva.github.io/Gen2Gen/)
- [Frame Guidance: Training‚ÄëFree Guidance for Frame‚ÄëLevel Control in Video Diffusion Models](http://arxiv.org/abs/2506.07177v1)
- [Hi‚ÄëVAE: Efficient Video Autoencoding with Global and Detailed Motion](http://arxiv.org/abs/2506.07136v1)
- [ContentV: Efficient Training of Video Generation Models with Limited Compute](http://arxiv.org/abs/2506.05343v2) [![GitHub Stars](https://img.shields.io/github/stars/bytedance/ContentV?style=social)](https://github.com/bytedance/ContentV)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://contentv.github.io)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ByteDance/ContentV-8B)
- [Astraea: A GPU‚ÄëOriented Token‚Äëwise Acceleration Framework for Video Diffusion Transformers](http://arxiv.org/abs/2506.05096v3)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://astraea-project.github.io/ASTRAEA/)
- [FPSAttention: Training-Aware FP8 and Sparsity Co‚ÄëDesign for Fast Video Diffusion](http://arxiv.org/abs/2506.04648v2)
- [LayerFlow: A Unified Model for Layer‚ÄëAware Video Generation](http://arxiv.org/abs/2506.04228v1) [![GitHub Stars](https://img.shields.io/github/stars/SihuiJi/LayerFlow?style=social)](https://github.com/SihuiJi/LayerFlow)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sihuiji.github.io/LayerFlow-Page/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/zjuJish/LayerFlow)
- [FullDiT2: Efficient In‚ÄëContext Conditioning for Video Diffusion Transformers](http://arxiv.org/abs/2506.04213v2)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://fulldit2.github.io/)
- [DenseDPO: Fine‚ÄëGrained Temporal Preference Optimization for Video Diffusion Models](http://arxiv.org/abs/2506.03517v1)
- [Chipmunk: Training‚ÄëFree Acceleration of Diffusion Transformers with Dynamic Column‚ÄëSparse Deltas](http://arxiv.org/abs/2506.03275v1) [![GitHub Stars](https://img.shields.io/github/stars/sandyresearch/chipmunk?style=social)](https://github.com/sandyresearch/chipmunk)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sandyresearch.github.io/chipmunk-part-II/)
- [Context¬†as¬†Memory: Scene‚ÄëConsistent Interactive Long Video Generation with Memory Retrieval](http://arxiv.org/abs/2506.03141v1)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://context-as-memory.github.io/)
- [CamCloneMaster: Enabling Reference‚Äëbased Camera Control for Video Generation](http://arxiv.org/abs/2506.03140v1)
- [Dual‚ÄëExpert Consistency Model for Efficient and High‚ÄëQuality Video Generation](http://arxiv.org/abs/2506.03123v2) [![GitHub Stars](https://img.shields.io/github/stars/Vchitect/DCM?style=social)](https://github.com/Vchitect/DCM)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vchitect.github.io/DCM/)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/cszy98/DCM)
- [Sparse‚ÄëvDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](http://arxiv.org/abs/2506.03065v1) [![GitHub Stars](https://img.shields.io/github/stars/Peyton-Chen/Sparse-vDiT?style=social)](https://github.com/Peyton-Chen/Sparse-vDiT)
- [LumosFlow: Motion‚ÄëGuided Long Video Generation](http://arxiv.org/abs/2506.02497v1)
- [Motion aware video generative model](http://arxiv.org/abs/2506.02244v1)
- [Many‚Äëfor‚ÄëMany: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks](http://arxiv.org/abs/2506.01758v2)
- [OpenS2V‚ÄëNexus: A Detailed Benchmark and Million‚ÄëScale Dataset for Subject‚Äëto‚ÄëVideo Generation](https://arxiv.org/abs/2505.20292)
- [Wan: Open and Advanced Large‚ÄëScale Video Generative Models](http://arxiv.org/abs/2503.20314v2)



</details>

</details>
<details>
<summary><h4>‚ú® 2024</h4></summary>


<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***Vlogger:*** *Make Your Dream A Vlog*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2401.09414.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Vchitect/Vlogger)

*   **[CVPR 2024]** ***Make Pixels Dance:*** *High-Dynamic Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.10982.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://makepixelsdance.github.io/)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://makepixelsdance.github.io/demo.html)

*   **[CVPR 2024]** ***VGen:*** *Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.04483)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://higen-t2v.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/VGen)

*   **[CVPR 2024]** ***GenTron:*** *Delving Deep into Diffusion Transformers for Image and Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.04557)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.shoufachen.com/gentron_website/)

*   **[CVPR 2024]** ***SimDA:*** *Simple Diffusion Adapter for Efficient Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2308.09710.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chenhsing.github.io/SimDA/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ChenHsing/SimDA)

*   **[CVPR 2024]** ***MicroCinema:*** *A Divide-and-Conquer Approach for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.18829)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wangyanhui666.github.io/MicroCinema.github.io/)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://youtube.com/shorts/H7O-Ku_lqPA)

*   **[CVPR 2024]** ***Generative Rendering:*** *Controllable 4D-Guided Video Generation with 2D Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.01409)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://primecai.github.io/generative_rendering/)

*   **[CVPR 2024]** ***PEEKABOO:*** *Interactive Video Generation via Masked-Diffusion*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.07509)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jinga-lala.github.io/projects/Peekaboo/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/microsoft/Peekaboo)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://huggingface.co/spaces/anshuln/peekaboo-demo)

*   **[CVPR 2024]** ***EvalCrafter:*** *Benchmarking and Evaluating Large Video Generation Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.11440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://evalcrafter.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/EvalCrafter/EvalCrafter)

*   **[CVPR 2024]** *A Recipe for Scaling up Text-to-Video Generation with Text-free Videos*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.15770)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tf-t2v.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/damo-vilab/i2vgen-xl)

*   **[CVPR 2024]** ***BIVDiff:*** *A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.02813)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://bivdiff.github.io/)

*   **[CVPR 2024]** ***Mind the Time:*** *Scaled Spatiotemporal Transformers for Text-to-Video Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2402.14797)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/snapvideo/video_ldm.html)

*   **[CVPR 2024]** ***MotionDirector:*** *Motion Customization of Text-to-Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.08465)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/showlab/MotionDirector)

*   **[CVPR 2024]** *Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Skorokhodov_Hierarchical_Patch_Diffusion_Models_for_High-Resolution_Video_Generation_CVPR_2024_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://snap-research.github.io/hpdm/)

*   **[CVPR 2024]** ***DiffPerformer:*** *Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DiffPerformer_Iterative_Learning_of_Consistent_Latent_Guidance_for_Diffusion-based_Human_CVPR_2024_paper.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/aipixel/)

*   **[CVPR 2024]** *Grid Diffusion Models for Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2404.00234.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/taegyeong-lee/Grid-Diffusion-Models-for-Text-to-Video-Generation)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://taegyeong-lee.github.io/text2video)

*   **[ECCV 2024]** ***Emu Video:*** *Factorizing Text-to-Video Generation by Explicit Image Conditioning*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.10709.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ai.meta.com/blog/emu-text-to-video-generation-image-editing-research/)

*   **[ECCV 2024]** ***W.A.L.T.:*** *Photorealistic Video Generation with Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10270.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://walt-video-diffusion.github.io/)

*   **[ECCV 2024]** ***MoVideo:*** *Motion-Aware Video Generation with Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06030.pdf)

*   **[ECCV 2024]** ***DrivingDiffusion:*** *Layout-Guided Multi-View Driving Scenarios Video Generation with Latent Diffusion Model*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/10097.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://drivingdiffusion.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/shalfun/DrivingDiffusion)

*   **[ECCV 2024]** ***MagDiff:*** *Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02738.pdf)

*   **[ECCV 2024]** ***HARIVO:*** *Harnessing Text-to-Image Models for Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06938.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kwonminki.github.io/HARIVO/)

*   **[ECCV 2024]** ***MEVG:*** *Multi-event Video Generation with Text-to-Video Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06012.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kuai-lab.github.io/eccv2024mevg/)

*   **[NeurIPS 2024]** ***DEMO:*** *Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://proceedings.neurips.cc/paper_files/paper/2024/file/81f19c0e9f3e06c831630ab6662fd8ea-Paper-Conference.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/PR-Ryan/DEMO)

*   **[ICML 2024]** ***Video-LaVIT:*** *Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S9lk6dk4LL)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-lavit.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/LaVIT)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/Video-LaVIT-v1)

*   **[ICLR 2024]** ***VDT:*** *General-purpose Video Diffusion Transformers via Mask Modeling*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2305.13311.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://vdt-2023.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RERV/VDT)

*   **[ICLR 2024]** ***VersVideo:*** *Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf?id=K9sVJ17zvB)

*   **[AAAI 2024]** ***Follow Your Pose:*** *Pose-Guided Text-to-Video Generation using Pose-Free Videos*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2304.01186)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://follow-your-pose.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/mayuelala/FollowYourPose)

*   **[AAAI 2024]** ***E2HQV:*** *High-Quality Video Generation from Event Camera via Theory-Inspired Model-Aided Deep Learning*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2401.08117)

*   **[AAAI 2024]** ***ConditionVideo:*** *Training-Free Condition-Guided Text-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2310.07697)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pengbo807.github.io/conditionvideo-website/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/pengbo807/ConditionVideo)

*   **[AAAI 2024]** ***F3-Pruning:*** *A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text to-Video Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2312.03459)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Gender Bias in Text-to-Video Generation Models: A case study of Sora](http://arxiv.org/abs/2501.01987v2)
- [VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation](http://arxiv.org/abs/2412.21059v2) [![GitHub Stars](https://img.shields.io/github/stars/zai-org/VisionReward?style=social)](https://github.com/zai-org/VisionReward) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/zai-org/VisionReward-Video)
- [Follow-Your-MultiPose: Tuning-Free Multi-Character Text-to-Video Generation via Pose Guidance](http://arxiv.org/abs/2412.16495v2)
- [CustomTTT: Motion and Appearance Customized Video Generation via Test-Time Training](http://arxiv.org/abs/2412.15646v2) [![GitHub Stars](https://img.shields.io/github/stars/RongPiKing/CustomTTT?style=social)](https://github.com/RongPiKing/CustomTTT) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://customttt.github.io)
- [DirectorLLM for Human-Centric Video Generation](http://arxiv.org/abs/2412.14484v1)
- [Can Video Generation Replace Cinematographers? Research on the Cinematic Language of Generated Video](http://arxiv.org/abs/2412.12223v2)
- [LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity](http://arxiv.org/abs/2412.09856v2) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://lineargen.github.io)
- [T-SVG: Text-Driven Stereoscopic Video Generation](http://arxiv.org/abs/2412.09323v2)
- [Mojito: Motion Trajectory and Intensity Control for Video Generation](http://arxiv.org/abs/2412.08948v2)
- [SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints](http://arxiv.org/abs/2412.07760v1) [![GitHub Stars](https://img.shields.io/github/stars/KwaiVGI/SynCamMaster?style=social)](https://github.com/KwaiVGI/SynCamMaster) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/KwaiVGI/SynCamMaster-Wan2.1)
- [Motion by Queries: Identity-Motion Trade-offs in Text-to-Video Generation](http://arxiv.org/abs/2412.07750v3)
- [STIV: Scalable Text and Image Conditioned Video Generation](http://arxiv.org/abs/2412.07730v1)
- [GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration](http://arxiv.org/abs/2412.04440v1) [![GitHub Stars](https://img.shields.io/github/stars/Karine-Huang/GenMAC?style=social)](https://github.com/Karine-Huang/GenMAC) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://karine-h.github.io/GenMAC/)
- [CPA: Camera-pose-awareness Diffusion Transformer for Video Generation](http://arxiv.org/abs/2412.01429v1)
- [MotionCharacter: Identity-Preserving and Motion Controllable Human Video Generation](http://arxiv.org/abs/2411.18281v2)
- [Scene Co-pilot: Procedural Text to Video Generation with Human in the Loop](http://arxiv.org/abs/2411.18644v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://abolfazl-sh.github.io/Scene_co-pilot_site/)
- [Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models](http://arxiv.org/abs/2411.17041v1) [![GitHub Stars](https://img.shields.io/github/stars/kjm981995/free2guide?style=social)](https://github.com/kjm981995/free2guide) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://free2guide.github.io)
- [DreamRunner: Fine-Grained Compositional Story-to-Video Generation with Retrieval-Augmented Motion Adaptation](http://arxiv.org/abs/2411.16657v3)
- [InTraGen: Trajectory-controlled Video Generation for Object Interactions](http://arxiv.org/abs/2411.16804v1) [![GitHub Stars](https://img.shields.io/github/stars/insait-institute/InTraGen?style=social)](https://github.com/insait-institute/InTraGen)
- [Optical-Flow Guided Prompt Optimization for Coherent Video Generation](http://arxiv.org/abs/2411.15540v2) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://motionprompt.github.io)
- [VideoRepair: Improving Text-to-Video Generation via Misalignment Evaluation and Localized Refinement](http://arxiv.org/abs/2411.15115v2) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://video-repair.github.io)
- [Motion Control for Enhanced Complex Action Video Generation](http://arxiv.org/abs/2411.08328v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://mvideo-v1.github.io)
- [GameGen-X: Interactive Open-world Game Video Generation](http://arxiv.org/abs/2411.00769v3) [![GitHub Stars](https://img.shields.io/github/stars/GameGen-X/GameGen-X?style=social)](https://github.com/GameGen-X/GameGen-X) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://gamegen-x.github.io)
- [Enhancing Motion in Text-to-Video Generation with Decomposed Encoding and Conditioning](http://arxiv.org/abs/2410.24219v1) [![GitHub Stars](https://img.shields.io/github/stars/PR-Ryan/DEMO?style=social)](https://github.com/PR-Ryan/DEMO) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pr-ryan.github.io)
- [ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation](http://arxiv.org/abs/2410.20502v3)
- [Animating the Past: Reconstruct Trilobite via Video Generation](http://arxiv.org/abs/2410.14715v1)
- [ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way](http://arxiv.org/abs/2410.06241v3) [![GitHub Stars](https://img.shields.io/github/stars/Bujiazi/ByTheWay?style=social)](https://github.com/Bujiazi/ByTheWay)
- [T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through Data, Reward, and Conditional Guidance Design](http://arxiv.org/abs/2410.05677v2) [![GitHub Stars](https://img.shields.io/github/stars/Ji4chenLi/t2v-turbo?style=social)](https://github.com/Ji4chenLi/t2v-turbo) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://t2v-turbo-v2.github.io)
- [The Dawn of Video Generation: Preliminary Explorations with SORA-like Models](http://arxiv.org/abs/2410.05227v2)
- [Compositional 3D-aware Video Generation with LLM Director](http://arxiv.org/abs/2409.00558v1)
- [Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation](http://arxiv.org/abs/2408.10453v2)
- [FancyVideo: Towards Dynamic and Consistent Video Generation via Cross-frame Textual Guidance](http://arxiv.org/abs/2408.08189v4) [![GitHub Stars](https://img.shields.io/github/stars/360CVGroup/FancyVideo?style=social)](https://github.com/360CVGroup/FancyVideo) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://360cvgroup.github.io/FancyVideo/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/qihoo360/FancyVideo)
- [Still-Moving: Customized Video Generation without Customized Video Data](http://arxiv.org/abs/2407.08674v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://still-moving.github.io)
- [VEnhancer: Generative Space-Time Enhancement for Video Generation](http://arxiv.org/abs/2407.07667v1) [![GitHub Stars](https://img.shields.io/github/stars/Vchitect/VEnhancer?style=social)](https://github.com/Vchitect/VEnhancer) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vchitect.github.io/VEnhancer-project/)
- [Mobius: A High Efficient Spatial-Temporal Parallel Training Paradigm for Text-to-Video Generation Task](http://arxiv.org/abs/2407.06617v4)
- [VIMI: Grounding Video Generation through Multi-modal Instruction](http://arxiv.org/abs/2407.06304v1)
- [GVDIFF: Grounded Text-to-Video Generation with Diffusion Models](http://arxiv.org/abs/2407.01921v2)
- [Evaluation of Text-to-Video Generation Models: A Dynamics Perspective](http://arxiv.org/abs/2407.01094v1) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Video-Bench/Video-Bench)
- [Text-Animator: Controllable Visual Text Video Generation](http://arxiv.org/abs/2406.17777v1)
- [MotionBooth: Motion-Aware Customized Text-to-Video Generation](http://arxiv.org/abs/2406.17758v3) [![GitHub Stars](https://img.shields.io/github/stars/jianzongwu/MotionBooth?style=social)](https://github.com/jianzongwu/MotionBooth) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://jianzongwu.github.io/projects/motionbooth) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/jianzongwu/MotionBooth)
- [Hierarchical Patch Diffusion Models for High-Resolution Video Generation](http://arxiv.org/abs/2406.07792v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/hpdm/)
- [Compositional Video Generation as Flow Equalization](http://arxiv.org/abs/2407.06182v1)
- [MotionClone: Training-Free Motion Cloning for Controllable Video Generation](http://arxiv.org/abs/2406.05338v6) [![GitHub Stars](https://img.shields.io/github/stars/LPengYang/MotionClone?style=social)](https://github.com/LPengYang/MotionClone) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://bujiazi.github.io/motionclone.github.io)
- [VideoTetris: Towards Compositional Text-To-Video Generation](http://arxiv.org/abs/2406.04277v2) [![GitHub Stars](https://img.shields.io/github/stars/YangLing0818/VideoTetris?style=social)](https://github.com/YangLing0818/VideoTetris) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://videotetris.github.io)
- [VideoPhy: Evaluating Physical Commonsense for Video Generation](http://arxiv.org/abs/2406.03520v2)
- [I4VGen: Image as Free Stepping Stone for Text-to-Video Generation](http://arxiv.org/abs/2406.02230v2) [![GitHub Stars](https://img.shields.io/github/stars/xiefan-guo/i4vgen?style=social)](https://github.com/xiefan-guo/i4vgen) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://xiefan-guo.github.io/i4vgen)
- [DisenStudio: Customized Multi-subject Text-to-Video Generation with Disentangled Spatial Control](http://arxiv.org/abs/2405.12796v1)
- [The Lost Melody: Empirical Observations on Text-to-Video Generation From A Storytelling Perspective](http://arxiv.org/abs/2405.08720v1)
- [TALC: Time-Aligned Captions for Multi-Scene Text-to-Video Generation](http://arxiv.org/abs/2405.04682v4)
- [MotionMaster: Training-free Camera Motion Transfer For Video Generation](http://arxiv.org/abs/2404.15789v2)
- [ConCLVD: Controllable Chinese Landscape Video Generation via Diffusion Model](http://arxiv.org/abs/2404.12903v1)
- [MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators](http://arxiv.org/abs/2404.05014v2) [![GitHub Stars](https://img.shields.io/github/stars/PKU-YuanGroup/MagicTime?style=social)](https://github.com/PKU-YuanGroup/MagicTime) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pku-yuangroup.github.io/MagicTime/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/BestWishYsh/MagicTime)
- [CameraCtrl: Enabling Camera Control for Text-to-Video Generation](http://arxiv.org/abs/2404.02101v2)
- [Grid Diffusion Models for Text-to-Video Generation](http://arxiv.org/abs/2404.00234v2)
- [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](http://arxiv.org/abs/2403.14773v2)
- [S2DM: Sector-Shaped Diffusion Models for Video Generation](http://arxiv.org/abs/2403.13408v2)
- [Mora: Enabling Generalist Video Generation via A Multi-Agent Framework](http://arxiv.org/abs/2403.13248v3)



</details>


</details>


<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2023]** ***Align your Latents:*** *High-resolution Video Synthesis with Latent Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2304.08818.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/srpkdyy/VideoLDM)

*   **[CVPR 2023]** ***Text2Video-Zero:*** *Text-to-image Diffusion Models are Zero-shot Video Generators*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khachatryan_Text2Video-Zero_Text-to-Image_Diffusion_Models_are_Zero-Shot_Video_Generators_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://text2video-zero.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Picsart-AI-Research/Text2Video-Zero)
    [![Demo](https://img.shields.io/badge/Hugging_Face-Demo-yellow?style=for-the-badge)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)

*   **[CVPR 2023]** *Video Probabilistic Diffusion Models in Projected Latent Space*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2023/papers/Yu_Video_Probabilistic_Diffusion_Models_in_Projected_Latent_Space_CVPR_2023_paper.pdf)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sihyun-yu/PVDM)

*   **[ICCV 2023]** ***PYOCO:*** *Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/dir/pyoco/)

*   **[ICCV 2023]** ***Gen-1:*** *Structure and Content-guided Video Synthesis with Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.runwayml.com/gen1)

*   **[NeurIPS 2023]** *Video Diffusion Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2204.03458.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-diffusion.github.io/)

*   **[NeurIPS 2023]** ***UniPi:*** *Learning Universal Policies via Text-Guided Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://papers.nips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://universal-policy.github.io/unipi/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/flow-diffusion/AVDC)

*   **[NeurIPS 2023]** ***VideoComposer:*** *Compositional Video Synthesis with Motion Controllability*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2306.02018.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://videocomposer.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/videocomposer)

*   **[ICLR 2023]** ***CogVideo:*** *Large-scale Pretraining for Text-to-video Generation via Transformers*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf?id=rB6TpjAuSRy)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/THUDM/CogVideo)
    [![Demo](https://img.shields.io/badge/Video-Demo-orange?style=for-the-badge&logo=youtube)](https://models.aminer.cn/cogvideo/)

*   **[ICLR 2023]** ***Make-A-Video:*** *Text-to-video Generation without Text-video Data*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2209.14792.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://makeavideo.studio/)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/lucidrains/make-a-video-pytorch)

*   **[ICLR 2023]** ***Phenaki:*** *Variable Length Video Generation From Open Domain Textual Description*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/pdf/fe8e106a2746992c9c2e658bdc8cb9c89cc5a39a.pdf)
    [![GitHub](https://img.shields.io/badge/Reproduced-Code-green?style=for-the-badge&logo=github)](https://github.com/lucidrains/phenaki-pytorch)



</details>




<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [StreamDiT: Real-Time Streaming Text-to-Video Generation](https://arxiv.org/abs/2507.03745) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cumulo-autumn.github.io/StreamDiT/)
- [SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://arxiv.org/abs/2509.24695) [![GitHub Stars](https://img.shields.io/github/stars/NVlabs/Sana?style=social)](https://github.com/NVlabs/Sana) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://nvlabs.github.io/Sana/Video) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Efficient-Large-Model/SANA-Video_2B_480p)
- [FlashVideo: A Framework for Swift Inference in Text-to-Video Generation](http://arxiv.org/abs/2401.00869v1)
- [A Recipe for Scaling up Text-to-Video Generation with Text-free Videos](http://arxiv.org/abs/2312.15770v1)
- [Photorealistic Video Generation with Diffusion Models](http://arxiv.org/abs/2312.06662v1)
- [GenTron: Diffusion Transformers for Image and Video Generation](http://arxiv.org/abs/2312.04557v2)
- [Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation](http://arxiv.org/abs/2312.04483v1)
- [StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter](http://arxiv.org/abs/2312.00330v2)
- [ART$\boldsymbol{\cdot}$V: Auto-Regressive Text-to-Video Generation with Diffusion Models](http://arxiv.org/abs/2311.18834v1)
- [MotionZero: Exploiting Motion Priors for Zero-shot Text-to-Video Generation](http://arxiv.org/abs/2311.16635v1)
- [FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline](http://arxiv.org/abs/2311.13073v2)
- [GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning](http://arxiv.org/abs/2311.12631v3)
- [Make Pixels Dance: High-Dynamic Video Generation](http://arxiv.org/abs/2311.10982v1)
- [VideoDreamer: Customized Multi-Subject Text-to-Video Generation with Disen-Mix Finetuning on Language-Video Foundation Models](http://arxiv.org/abs/2311.00990v2)
- [POS: A Prompts Optimization Suite for Augmenting Text-to-Video Generation](http://arxiv.org/abs/2311.00949v3)
- [VideoCrafter1: Open Diffusion Models for High-Quality Video Generation](http://arxiv.org/abs/2310.19512v1)
- [LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation](http://arxiv.org/abs/2310.10769v1)
- [Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation](http://arxiv.org/abs/2309.16429v1)
- [Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation](http://arxiv.org/abs/2309.15818v3)
- [LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models](http://arxiv.org/abs/2309.15103v2)
- [VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning](http://arxiv.org/abs/2309.15091v2)
- [Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation](http://arxiv.org/abs/2309.03549v1)
- [VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation](http://arxiv.org/abs/2309.00398v2)
- [Dual-Stream Diffusion Net for Text-to-Video Generation](http://arxiv.org/abs/2308.08316v3)
- [Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation](http://arxiv.org/abs/2307.06940v1)
- [Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance](http://arxiv.org/abs/2306.00943v1)
- [DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation](http://arxiv.org/abs/2305.14330v3)
- [ControlVideo: Training-free Controllable Text-to-Video Generation](http://arxiv.org/abs/2305.13077v1)
- [Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation](http://arxiv.org/abs/2305.10874v4)


</details>

</details>


[<small>‚áß Back to ToC</small>](#contents)

### <span id="i2v">üñºÔ∏è Image-to-Video (I2V) Generation</span>

<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[AAAI 2026]** ***IPRO:*** *Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2510.14255.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ipro-alimama.github.io/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba/ROLL)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>


<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***MotionStone:*** *Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2412.05848)

*   **[CVPR 2025]** ***MotionPro:*** *A Precise Motion Controller for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2503.00948)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/HiDream-ai/MotionPro)

*   **[CVPR 2025]** ***Through-The-Mask:*** *Mask-based Motion Trajectories for Image-to-Video Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.03059)

*   **[CVPR 2025]** *Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2503.00948)

*   **[CVPR 2025]** ***I2VGuard:*** *Safeguarding Images against Misuse in Diffusion-based Image-to-Video Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gui_I2VGuard_Safeguarding_Images_against_Misuse_in_Diffusion-based_Image-to-Video_Models_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***LeviTor:*** *3D Trajectory Oriented Image-to-Video Synthesis*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/ant-research/LeviTor)

*   **[ICCV 2025]** ***AnyI2V:*** *Animating Any Conditional Image with Motion Control*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2507.02857) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/FudanCVL/AnyI2V)

*   **[ICCV 2025]** *Versatile Transition Generation with Image-to-Video Diffusion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2508.01698)

* **[ICCV 2025]** ***TIP‚ÄëI2V:*** *A Million‚ÄëScale Real¬†Text and¬†Image¬†Prompt¬†Dataset for¬†Image‚Äëto‚ÄëVideo¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2411.04709) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tip-i2v.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/WangWenhao0716/TIP-I2V) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/datasets/WenhaoWang/TIP-I2V)

* **[ICCV 2025]** *Unified¬†Video¬†Generation¬†via¬†Next‚ÄëSet¬†Prediction¬†in¬†Continuous¬†Domain*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/72)

* **[NeurIPS 2025]** ***GenRec:***¬†*Unifying¬†Video¬†Generation¬†and¬†Recognition¬†with¬†Diffusion¬†Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2408.15241) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wengzejia1/GenRec) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/cookieY/GenRec)

* **[ICCV 2025]** *Precise¬†Action‚Äëto‚ÄëVideo¬†Generation¬†Through¬†Visual¬†Action¬†Prompts*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2508.13104) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zju3dv.github.io/VAP)

* **[ICCV¬†2025]** ***STIV:***¬†*Scalable¬†Text¬†and¬†Image¬†Conditioned¬†Video¬†Generation*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.07730) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://machinelearning.apple.com/research/conditioned-video-generation) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/papers/STIV)

* **[ICLR 2025]** ***FrameBridge:***¬†*Improving¬†Image‚Äëto‚ÄëVideo¬†Generation¬†with¬†Bridge¬†Models*<br>
  [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.15371) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/FrameBridge) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://framebridge-icml.github.io)

*   **[ICLR 2025]** ***SG-I2V:*** *Self-Guided Trajectory Control in Image-to-Video Generation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=uQjySppU9x)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://kmcode1.github.io/Projects/SG-I2V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Kmcode1/SG-I2V)

*   **[ICLR 2025]** *Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=ykD8a9gJvy)

*   **[ICLR 2025]** *Pyramidal Flow Matching for Efficient Video Generative Modeling*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=66NzcRQuOq)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://pyramid-flow.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/Pyramid-Flow)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/pyramid-flow-miniflux)

* **[NeurIPS 2025]** ***MotionRAG:*** *Motion Retrieval-Augmented Image-to-Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/115107) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MCG-NJU/MotionRAG) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/MCG-NJU/MotionRAG)


   
</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Waver: Wave Your Way to Lifelike Video Generation](https://arxiv.org/pdf/2508.15761) [![GitHub Stars](https://img.shields.io/github/stars/FoundationVision/Waver?style=social)](https://github.com/FoundationVision/Waver)
- [FlexTraj: Image-to-Video Generation with Flexible Point Trajectory Control](https://arxiv.org/pdf/2510.08527) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://bestzzhang.github.io/FlexTraj/)
- [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/pdf/2510.08377) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://congwei1230.github.io/UniVideo/)
- [Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising](https://arxiv.org/pdf/2511.08633) [![GitHub Stars](https://img.shields.io/github/stars/time-to-move/TTM?style=social)](https://github.com/time-to-move/TTM) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://time-to-move.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/rmz92002/time-to-move)
- [Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation](https://arxiv.org/pdf/2511.14993) [![GitHub Stars](https://img.shields.io/github/stars/kandinskylab/kandinsky-5?style=social)](https://github.com/kandinskylab/kandinsky-5) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/kandinskylab/Kandinsky-5.0-I2V-Pro-sft-5s-Diffusers)
- [Physics‚ÄëGrounded Motion Forecasting via Equation Discovery for Trajectory‚ÄëGuided Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2507.06830v1)
- [Enhancing Motion Dynamics of Image‚Äëto‚ÄëVideo Models via Adaptive Low‚ÄëPass Guidance](http://arxiv.org/abs/2506.08456v1) [![GitHub Stars](https://img.shields.io/github/stars/choi403/ALG?style=social)](https://github.com/choi403/ALG)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://choi403.github.io/ALG/)
- [Frame In‚ÄëN‚ÄëOut: Unbounded Controllable Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2505.21491v1) [![GitHub Stars](https://img.shields.io/github/stars/UVA-Computer-Vision-Lab/FrameINO?style=social)](https://github.com/UVA-Computer-Vision-Lab/FrameINO)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://uva-computer-vision-lab.github.io/Frame-In-N-Out/)
- [Dynamic‚ÄëI2V: Exploring Image‚Äëto‚ÄëVideo Generation Models via Multimodal LLM](http://arxiv.org/abs/2505.19901v3)
- [Order Matters: On Parameter‚ÄëEfficient Image‚Äëto‚ÄëVideo Probing for Recognizing Nearly Symmetric Actions](http://arxiv.org/abs/2503.24298v1)
- [EvAnimate: Event‚ÄëConditioned Image‚Äëto‚ÄëVideo Generation for Human Animation](http://arxiv.org/abs/2503.18552v2) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://potentialming.github.io/projects/EvAnimate) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/potentialming/EvHumanMotion)
- [Step‚ÄëVideo‚ÄëTI2V Technical Report: A State‚Äëof‚Äëthe‚ÄëArt Text‚ÄëDriven Image‚Äëto‚ÄëVideo Generation Model](http://arxiv.org/abs/2503.11251v1) [![GitHub Stars](https://img.shields.io/github/stars/stepfun-ai/Step-Video-TI2V?style=social)](https://github.com/stepfun-ai/Step-Video-TI2V)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/stepfun-ai/stepvideo-ti2v)
- [DreamInsert: Zero‚ÄëShot Image‚Äëto‚ÄëVideo Object Insertion from A Single Image](http://arxiv.org/abs/2503.10342v1)
- [I2V3D: Controllable image‚Äëto‚Äëvideo generation with 3D guidance](http://arxiv.org/abs/2503.09733v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://bestzzhang.github.io/I2V3D/)
- [Extrapolating and Decoupling Image‚Äëto‚ÄëVideo Generation Models: Motion Modeling Is Easier Than You Think](http://arxiv.org/abs/2503.00948v1) [![GitHub Stars](https://img.shields.io/github/stars/Chuge0335/EDG?style=social)](https://github.com/Chuge0335/EDG) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Doubiiu/DynamiCrafter_512) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/GraceZhao/DynamiCrafter-CIL-512) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/GraceZhao/DynamiCrafter-Analytic-Init)
- [Object‚ÄëCentric Image‚Äëto‚ÄëVideo Generation with Language Guidance](http://arxiv.org/abs/2502.11655v1) [![GitHub Stars](https://img.shields.io/github/stars/angelvillar96/TextOCVP?style=social)](https://github.com/angelvillar96/TextOCVP)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://play-slot.github.io/TextOCVP)
- [VidCRAFT3: Camera, Object, and Lighting Control for Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2502.07531v3) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sixiaozheng.github.io/VidCRAFT3/)
- [MotionCanvas: Cinematic Shot Design with Controllable Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2502.04299v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://motion-canvas25.github.io/)
- [Through‚ÄëThe‚ÄëMask: Mask‚Äëbased Motion Trajectories for Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2501.03059v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://guyyariv.github.io/TTM/)

</details>

</details>


<details>
<summary><h4>‚ú® 2024</h4></summary>




<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***Animate Anyone:*** *Consistent and Controllable Image-to-video Synthesis for Character Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2311.17117.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://humanaigc.github.io/animate-anyone/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/AnimateAnyone)

*   **[CVPR 2024]** *Your Image Is My Video: Reshaping the Receptive Field via Image-to-Video Differentiable AutoAugmentation and Fusion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00557)

*   **[CVPR 2024]** ***TRIP:*** *Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00828)

*   **[CVPR 2024]** *Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.01751)

*   **[CVPR 2024]** *Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00779) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/AnimateAnyone)

*   **[ECCV 2024]** ***MOFA-Video:*** *Controllable Image Animation via Generative Motion Field Adaptions in Frozen Image-to-Video Diffusion Model*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72655-2_7) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/MyNiuuu/MOFA-Video)

*   **[ECCV 2024]** *$\mathrm R2$-Tuning: Efficient Image-to-Video Transfer Learning for Video Temporal Grounding*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72940-9_24) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yeliudev/R2-Tuning)

*   **[ECCV 2024]** ***PhysGen:*** *Rigid-Body Physics-Grounded Image-to-Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73007-8_21) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/stevenlsw/physgen)

*   **[ECCV 2024]** *Rethinking Image-to-Video Adaptation: An Object-Centric Perspective*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72775-7_19)

*   **[NeurIPS 2024]** ***TPC:*** *Test-time Procrustes Calibration for Diffusion-based Human Image Animation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/d6fdc3002dda4c3cad3d595ac6fa5352-Abstract-Conference.html)

*   **[NeurIPS 2024]** *Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/35cb54b887e7aafe74829677cce6c5c6-Abstract-Conference.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/thu-ml/cond-image-leakage)

*   **[ICML 2024]** ***Video-LaVIT:*** *Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization*<br>
    [![Paper](https://img.shields.io/badge/Paper-OpenReview-D15E5E?style=for-the-badge&logo=open-collective)](https://openreview.net/forum?id=S9lk6dk4LL)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://video-lavit.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/jy0205/LaVIT)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/rain1011/Video-LaVIT-v1)

*   **[SIGGRAPH 2024]** ***I2V-Adapter:*** *A General Image-to-Video Adapter for Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1145/3641519.3657407) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/KwaiVGI/I2V-Adapter)


*   **[SIGGRAPH 2024]** ***Motion-I2V:*** *Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1145/3641519.3657497) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/G-U-N/Motion-I2V)

*   **[AAAI 2024]** *Continuous Piecewise-Affine Based Motion Model for Image Animation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v38i6.28351) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/whx-sjtu/AAAI2024-CPABMM)



</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



- [OmniDrag: Enabling Motion Control for Omnidirectional Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2412.09623v1) [![GitHub Stars](https://img.shields.io/github/stars/lwq20020127/OmniDrag?style=social)](https://github.com/lwq20020127/OmniDrag)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://lwq20020127.github.io/OmniDrag/)
- [CamI2V: Camera‚ÄëControlled Image‚Äëto‚ÄëVideo Diffusion Model](http://arxiv.org/abs/2410.15957v3) [![GitHub Stars](https://img.shields.io/github/stars/ZGCTroy/CamI2V?style=social)](https://github.com/ZGCTroy/CamI2V)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://zgctroy.github.io/CamI2V/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/MuteApo/CamI2V)
- [Identifying and Solving Conditional Image Leakage in Image‚Äëto‚ÄëVideo Diffusion Model](http://arxiv.org/abs/2406.15735v3) [![GitHub Stars](https://img.shields.io/github/stars/thu-ml/cond-image-leakage?style=social)](https://github.com/thu-ml/cond-image-leakage)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cond-image-leak.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/Xiang-cd/DynamiCrafter-CIL)
- [CamCo: Camera‚ÄëControllable 3D‚ÄëConsistent Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2406.02509v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://ir1d.github.io/CamCo/)
- [CamViG: Camera Aware Image‚Äëto‚ÄëVideo Generation with Multimodal Transformers](http://arxiv.org/abs/2405.13195v1)
- [$R^2$‚ÄëTuning: Efficient Image‚Äëto‚ÄëVideo Transfer Learning for Video Temporal Grounding](http://arxiv.org/abs/2404.00801v2) [![GitHub Stars](https://img.shields.io/github/stars/yeliudev/R2-Tuning?style=social)](https://github.com/yeliudev/R2-Tuning)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/yeliudev/R2-Tuning)
- [TRIP: Temporal Residual Learning with Image Noise Prior for Image‚Äëto‚ÄëVideo Diffusion Models](http://arxiv.org/abs/2403.17005v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://trip-i2v.github.io/TRIP/)
- [Your Image is My Video: Reshaping the Receptive Field via Image‚ÄëTo‚ÄëVideo Differentiable AutoAugmentation and Fusion](http://arxiv.org/abs/2403.15194v1)
- [Tuning‚ÄëFree Noise Rectification for High Fidelity Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2403.02827v1) [![GitHub Stars](https://img.shields.io/github/stars/alimama-creative/Noise-Rectification?style=social)](https://github.com/alimama-creative/Noise-Rectification)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://noise-rectification.github.io/)
- [AtomoVideo: High Fidelity Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2403.01800v2) [![GitHub Stars](https://img.shields.io/github/stars/atomo-video/atomo-video.github.io?style=social)](https://github.com/atomo-video/atomo-video.github.io)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://atomo-video.github.io/)
- [ConsistI2V: Enhancing Visual Consistency for Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2402.04324v2) [![GitHub Stars](https://img.shields.io/github/stars/TIGER-AI-Lab/ConsistI2V?style=social)](https://github.com/TIGER-AI-Lab/ConsistI2V)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://tiger-ai-lab.github.io/ConsistI2V/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/TIGER-Lab/ConsistI2V)
- [AIGCBench: Comprehensive Evaluation of Image‚Äëto‚ÄëVideo Content Generated by AI](http://arxiv.org/abs/2401.01651v3) [![GitHub Stars](https://img.shields.io/github/stars/BenchCouncil/AIGCBench?style=social)](https://github.com/BenchCouncil/AIGCBench) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/stevenfan/AIGCBench_v1.0)


</details>




<summary><h4>‚ú® 2023</h4></summary>


<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[ICCV 2023]** ***DreamPose:*** *Fashion Image-to-Video Synthesis via Stable Diffusion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.02073) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/johannakarras/DreamPose)

*   **[ICCV 2023]** *Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/ICCV51070.2023.01281) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/alibaba-mmai-research/DiST)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance](http://arxiv.org/abs/2312.03018v4)
*   [Side4Video: Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning](http://arxiv.org/abs/2311.15769v1)



</details>

</details>


[<small>‚áß Back to ToC</small>](#contents)

### <span id="v2v">‚úÇÔ∏è Video-to-Video (V2V) Editing</span>

<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[AAAI 2026]** ***Vid-CamEdit:*** *Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2506.13697.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cvlab-kaist.github.io/Vid-CamEdit/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/cvlab-kaist/Vid-CamEdit)

* **[AAAI 2026]** ***FAME:*** *Fairness-aware Attention-modulated Video Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2510.22960.pdf)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>


<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** ***VideoHandles:*** *Editing 3D Object Compositions in Videos Using Video Generative Priors*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Koo_VideoHandles_Editing_3D_Object_Compositions_in_Videos_Using_Video_Generative_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***VideoDirector:*** *Precise Video Editing via Text-to-Video Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoDirector_Precise_Video_Editing_via_Text-to-Video_Models_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Yukun66/Video_Director)

*   **[CVPR 2025]** ***VideoSPatS:*** *Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gonzalez_VideoSPatS_Video_SPatiotemporal_Splines_for_Disentangled_Occlusion_Appearance_and_Motion_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***Align-A-Video:*** *Deterministic Reward Tuning of Image Diffusion Models for Consistent Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Align-A-Video_Deterministic_Reward_Tuning_of_Image_Diffusion_Models_for_Consistent_CVPR_2025_paper.html)

*   **[CVPR 2025]** *Unity in Diversity: Video Editing via Gradient-Latent Purification*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unity_in_Diversity_Video_Editing_via_Gradient-Latent_Purification_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***VEU-Bench:*** *Towards Comprehensive Understanding of Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/littlespray/VE-Bench)

*   **[CVPR 2025]** ***SketchVideo:*** *Sketch-based Video Generation and Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SketchVideo_Sketch-based_Video_Generation_and_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/IGLICT/SketchVideo)

*   **[CVPR 2025]** ***FATE:*** *Full-head Gaussian Avatar with Textural Editing from Monocular Video*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FATE_Full-head_Gaussian_Avatar_with_Textural_Editing_from_Monocular_Video_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/zjwfufu/FateAvatar)

*   **[CVPR 2025]** *Visual Prompting for One-shot Controllable Video Editing without Inversion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Visual_Prompting_for_One-shot_Controllable_Video_Editing_without_Inversion_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***FADE:*** *Frequency-Aware Diffusion Model Factorization for Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_FADE_Frequency-Aware_Diffusion_Model_Factorization_for_Video_Editing_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/EternalEvan/FADE)

*   **[ICCV 2025]** ***VACE:*** *All-in-One Video Creation and Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.07598)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://ali-vilab.github.io/VACE-Page/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali-vilab/VACE)

*   **[ICCV 2025]** ***Reangle-A-Video:*** *4D Video Generation as Video-to-Video Translation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2503.09151)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://anony1anony2.github.io/)

*   **[ICCV 2025]** ***DIVE:*** *Taming DINO for Subject-Driven Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.03347)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dino-video-editing.github.io/)

*   **[ICCV 2025]** **DynamicFace:** *High-Quality and Consistent Face Swapping for Image and Video using Composable 3D Facial Priors*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1368)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://dynamic-face.github.io/)

*   **[ICCV 2025]** ***QK-Edit:*** *Revisiting Attention-based Injection in MM-DiT for Image and Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/215)

*   **[ICCV 2025]** ***Teleportraits:*** *Training-Free People Insertion into Any Scene*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/809)

*   **[ICLR 2025]** ***VideoGrain:*** *Modulating Space-Time Attention for Multi-Grained Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=SSslAtcPB6)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/knightyxp/VideoGrain)

* **[NeurIPS 2025]** ***REGen:*** *Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/119175) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://wx83.github.io/REGen/)

*   **[AAAI 2025]** ***FreeMask:*** *Rethinking the Importance of Attention Masks for Zero-Shot Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i2.32185) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/LinglingCai0314/FreeMask)

*   **[AAAI 2025]** ***EditBoard:*** *Towards a Comprehensive Evaluation Benchmark for Text-Based Video Editing Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i15.33754) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/Samchen2003/EditBoard)


*   **[AAAI 2025]** ***VE-Bench:*** *Subjective-Aligned Benchmark Suite for Text-Driven Video Editing Quality Assessment*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i7.32763) [![GitHub](https://img.shields.io/badge/GitHub-repo-blue?style=for-the-badge)](https://github.com/littlespray/VE-Bench)

*   **[AAAI 2025]** *Re-Attentional Controllable Video Diffusion Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i8.32876) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/mdswyz/ReAtCo)

*   **[WACV 2025]** ***IP-FaceDiff:*** *Identity-Preserving Facial Video Editing with Diffusion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACVW65960.2025.00031)

*   **[WACV 2025]** ***SST-EM:*** *Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACVW65960.2025.00032) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git)

*   **[WACV 2025]** ***MagicStick:*** *Controllable Video Editing via Control Handle Transformations*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00909)

*   **[WACV 2025]** ***Ada-VE:*** *Training-Free Consistent Video Editing Using Adaptive Motion Prior*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00101)

*   **[WACV 2025]** ***FastVideoEdit:*** *Leveraging Consistency Models for Efficient Text-to-Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00360) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/youyuan-zhang/FastVideoEdit)




</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning](https://arxiv.org/pdf/2509.20360.pdf) [![GitHub Stars](https://img.shields.io/github/stars/adobe-research/EditVerse?style=social)](https://github.com/adobe-research/EditVerse) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/sooyek/EditVerseBench)
- [OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models](https://arxiv.org/pdf/2509.17627.pdf) [![GitHub Stars](https://img.shields.io/github/stars/Phantom-video/OmniInsert?style=social)](https://github.com/Phantom-video/OmniInsert) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://phantom-video.github.io/OmniInsert/)
- [UniVideo: Unified Understanding, Generation, and Editing for Videos](https://arxiv.org/pdf/2510.08377.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://congwei1230.github.io/UniVideo/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/KwaiVGI/UniVideo)
- [InstructX: Towards Unified Visual Editing with MLLM Guidance](https://arxiv.org/pdf/2510.08485.pdf) [![GitHub Stars](https://img.shields.io/github/stars/MC-E/InstructX?style=social)](https://github.com/MC-E/InstructX) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://mc-e.github.io/project/InstructX/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/datasets/Simons212/VIE-Bench)
- [Ditto: Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset](https://arxiv.org/pdf/2510.15742.pdf) [![GitHub Stars](https://img.shields.io/github/stars/EzioBy/Ditto?style=social)](https://github.com/EzioBy/Ditto) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/QingyanBai/Ditto_models)
- [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/pdf/2510.17803.pdf) [![GitHub Stars](https://img.shields.io/github/stars/zxYin/ConsistEdit_Code?style=social)](https://github.com/zxYin/ConsistEdit_Code) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://zxyin.github.io/ConsistEdit)
- [First Frame Is the Place to Go for Video Content Customization](https://arxiv.org/pdf/2511.15700.pdf) [![GitHub Stars](https://img.shields.io/github/stars/zli12321/FFGO-Video-Customization?style=social)](https://github.com/zli12321/FFGO-Video-Customization) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://firstframego.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Video-Customization/FFGO-Lora-Adapter)
- [Consistent Video Editing as Flow‚ÄëDriven Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2506.07713v2)
- [UNIC: Unified In‚ÄëContext Video Editing](http://arxiv.org/abs/2506.04216v1)
- [DreamVE: Unified Instruction‚Äëbased Image and Video Editing](http://arxiv.org/abs/2508.06080v1)
- [Controllable Pedestrian Video Editing for Multi‚ÄëView Driving Scenarios via Motion Sequence](http://arxiv.org/abs/2508.00299v1)
- [Low‚ÄëCost Test‚ÄëTime Adaptation for Robust Video Editing](http://arxiv.org/abs/2507.21858v1)
- [From Long Videos to Engaging Clips: A Human‚ÄëInspired Video Editing Framework with Multimodal Narrative Understanding](http://arxiv.org/abs/2507.02790v1)
- [STR‚ÄëMatch: Matching SpatioTemporal Relevance Score for Training‚ÄëFree Video Editing](http://arxiv.org/abs/2506.22868v1)
- [Shape‚Äëfor‚ÄëMotion: Precise and Consistent Video Editing with 3D Proxy](http://arxiv.org/abs/2506.22432v1)
- [DFVEdit: Conditional Delta Flow Vector for Zero‚Äëshot Video Editing](http://arxiv.org/abs/2506.20967v2)
- [Good Noise Makes Good Edits: A Training‚ÄëFree Diffusion‚ÄëBased Video Editing with Image and Text Prompts](http://arxiv.org/abs/2506.12520v1)
- [LoRA‚ÄëEdit: Controllable First‚ÄëFrame‚ÄëGuided Video Editing via Mask‚ÄëAware LoRA Fine‚ÄëTuning](http://arxiv.org/abs/2506.10082v3) [![GitHub Stars](https://img.shields.io/github/stars/cjeen/LoRAEdit?style=social)](https://github.com/cjeen/LoRAEdit)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cjeen.github.io/LoraEditPaper/)
- [TV‚ÄëLiVE: Training‚ÄëFree, Text‚ÄëGuided Video Editing via Layer Informed Vitality Exploitation](http://arxiv.org/abs/2506.07205v1)
- [FADE: Frequency‚ÄëAware Diffusion Model Factorization for Video Editing](http://arxiv.org/abs/2506.05934v1)
- [FlowDirector: Training‚ÄëFree Flow Steering for Precise Text‚Äëto‚ÄëVideo Editing](http://arxiv.org/abs/2506.05046v1) [![GitHub Stars](https://img.shields.io/github/stars/Westlake-AGI-Lab/FlowDirector?style=social)](https://github.com/Westlake-AGI-Lab/FlowDirector)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://flowdirector-edit.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/spaces/Westlake-AGI-Lab/FlowDirector)
- [FullDiT2: Efficient In‚ÄëContext Conditioning for Video Diffusion Transformers](http://arxiv.org/abs/2506.04213v2)
- [OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation](http://arxiv.org/abs/2506.01801v1)
- [Motion‚ÄëAware Concept Alignment for Consistent Video Editing](http://arxiv.org/abs/2506.01004v1)
- [Zero‚Äëto‚ÄëHero: Zero‚ÄëShot Initialization Empowering Reference‚ÄëBased Video Appearance Editing](http://arxiv.org/abs/2505.23134v1)
- [REGen: Multimodal Retrieval‚ÄëEmbedded Generation for Long‚Äëto‚ÄëShort Video Editing](http://arxiv.org/abs/2505.18880v1)
- [From Shots to Stories: LLM‚ÄëAssisted Video Editing with Unified Language Representations](http://arxiv.org/abs/2505.12237v1)
- [DAPE: Dual‚ÄëStage Parameter‚ÄëEfficient Fine‚ÄëTuning for Consistent Video Editing with Diffusion Models](http://arxiv.org/abs/2505.07057v1)
- [Photoshop Batch Rendering Using Actions for Stylistic Video Editing](http://arxiv.org/abs/2505.01001v1)
- [Efficient Temporal Consistency in Diffusion‚ÄëBased Video Editing with Adaptor Modules: A Theoretical Framework](http://arxiv.org/abs/2504.16016v1)
- [Vidi: Large Multimodal Models for Video Understanding and Editing](http://arxiv.org/abs/2504.15681v3)
- [Visual Prompting for One‚ÄëShot Controllable Video Editing without Inversion](http://arxiv.org/abs/2504.14335v1)
- [CamMimic: Zero‚ÄëShot Image To Camera Motion Personalized Video Generation Using Diffusion Models](http://arxiv.org/abs/2504.09472v1)
- [VideoSPatS: Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing](http://arxiv.org/abs/2504.07146v1)
- [Shot Sequence Ordering for Video Editing: Benchmarks, Metrics, and Cinematology‚ÄëInspired Computing Methods](http://arxiv.org/abs/2503.17975v2)
- [InstructVEdit: A Holistic Approach for Instructional Video Editing](http://arxiv.org/abs/2503.17641v1)
- [HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks](http://arxiv.org/abs/2503.17276v1)
- [VEGGIE: Instructional Editing and Reasoning of Video Concepts with Grounded Generation](http://arxiv.org/abs/2503.14350v2)
- [GIFT: Generated Indoor video frames for Texture‚Äëless point tracking](http://arxiv.org/abs/2503.12944v1)
- [RASA: Replace Anyone, Say Anything ‚Äî A Training‚ÄëFree Framework for Audio‚ÄëDriven and Universal Portrait Video Editing](http://arxiv.org/abs/2503.11571v1)
- [V2Edit: Versatile Video Diffusion Editor for Videos and 3D Scenes](http://arxiv.org/abs/2503.10634v2)
- [Alias‚ÄëFree Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space](http://arxiv.org/abs/2503.09419v1)
- [VACE: All‚Äëin‚ÄëOne Video Creation and Editing](http://arxiv.org/abs/2503.07598v2)
- [Get In Video: Add Anything You Want to the Video](http://arxiv.org/abs/2503.06268v1)
- [VideoPainter: Any‚Äëlength Video Inpainting and Editing with Plug‚Äëand‚ÄëPlay Context Control](http://arxiv.org/abs/2503.05639v3)
- [VideoGrain: Modulating Space‚ÄëTime Attention for Multi‚Äëgrained Video Editing](http://arxiv.org/abs/2502.17258v1)
- [VideoDiff: Human‚ÄëAI Video Co‚ÄëCreation with Alternatives](http://arxiv.org/abs/2502.10190v1)
- [SportsBuddy: Designing and Evaluating an AI‚ÄëPowered Sports Video Storytelling Tool Through Real‚ÄëWorld Deployment](http://arxiv.org/abs/2502.08621v2)
- [AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming and Keyframe Selection](http://arxiv.org/abs/2502.05433v1)
- [MotionCanvas: Cinematic Shot Design with Controllable Image‚Äëto‚ÄëVideo Generation](http://arxiv.org/abs/2502.04299v1)
- [SST‚ÄëEM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal Aspects in Video Editing](http://arxiv.org/abs/2501.07554v1)
- [IP‚ÄëFaceDiff: Identity‚ÄëPreserving Facial Video Editing with Diffusion](http://arxiv.org/abs/2501.07530v1)
- [Qffusion: Controllable Portrait Video Editing via Quadrant‚ÄëGrid Attention Learning](http://arxiv.org/abs/2501.06438v3)
- [Text‚Äëto‚ÄëEdit: Controllable End‚Äëto‚ÄëEnd Video Ad Creation via Multimodal LLMs](http://arxiv.org/abs/2501.05884v1)
- [Enhancing Low‚ÄëCost Video Editing with Lightweight Adaptors and Temporal‚ÄëAware Inversion](http://arxiv.org/abs/2501.04606v4)
- [Edit as You See: Image‚ÄëGuided Video Editing via Masked Motion Modeling](http://arxiv.org/abs/2501.04325v1)


</details>
</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** *A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00719) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/STEM-Inv/stem-inv)

*   **[CVPR 2024]** ***VidToMe:*** *Video Token Merging for Zero-Shot Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00715) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/VISION-SJTU/VidToMe)

*   **[CVPR 2024]** ***Video-P2P:*** *Video Editing with Cross-Attention Control*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00821) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/dvlab-research/Video-P2P)

*   **[CVPR 2024]** ***CCEdit:*** *Creative and Controllable Video Editing via Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00641)

*   **[CVPR 2024]** ***RAVE:*** *Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00622) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/rehglab/RAVE)

*   **[CVPR 2024]** ***DynVideo-E:*** *Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00732) 

*   **[CVPR 2024]** ***MaskINT:*** *Video Editing via Interpolative Non-autoregressive Masked Transformers*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00707)

*   **[CVPR 2024]** ***MotionEditor:*** *Editing Video Motion via Content-Aware Diffusion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00753) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Francis-Rings/MotionEditor)

*   **[CVPR 2024]** ***CAMEL:*** *CAusal Motion Enhancement Tailored for Lifting Text-Driven Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00867) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/zhangguiwei610/CAMEL)

*   **[ICLR 2024]** ***Ground-A-Video:*** *Zero-shot Grounded Video Editing using Text-to-image Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=28L2FCtMWq) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/Ground-A-Video/Ground-A-Video)

*   **[ICLR 2024]** *Video Decomposition Prior: Editing Videos Layer by Layer*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=nfMyERXNru)

*   **[ICLR 2024]** ***FLATTEN:*** *optical FLow-guided ATTENtion for consistent text-to-video editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=JgqftqZQZ7)

*   **[ICLR 2024]** ***TokenFlow:*** *Consistent Diffusion Features for Consistent Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=lKK50q2MtV) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/omerbt/TokenFlow)

*   **[ECCV 2024]** ***VIDEOSHOP:*** *Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73254-6_14) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/sfanxiang/videoshop)

*   **[ECCV 2024]** ***DragVideo:*** *Interactive Drag-Style Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72992-8_11) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/RickySkywalker/DragVideo-Official)

*   **[ECCV 2024]** ***WAVE:*** *Warping DDIM Inversion Features for Zero-Shot Text-to-Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73116-7_3)

*   **[ECCV 2024]** ***DreamMotion:*** *Space-Time Self-similar Score Distillation for Zero-Shot Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73404-5_21)

*   **[ECCV 2024]** *Object-Centric Diffusion for Efficient Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72998-0_6) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Qualcomm-AI-research/object-centric-diffusion)

*   **[ECCV 2024]** *Video Editing via Factorized Diffusion Distillation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73116-7_26)

*   **[ECCV 2024]** ***SAVE:*** *Protagonist Diversification with Structure Agnostic Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72989-8_3)

*   **[ECCV 2024]** ***DNI:*** *Dilutional Noise Initialization for Diffusion Video Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73195-2_11)

*   **[ECCV 2024]** ***MagDiff:*** *Multi-alignment Diffusion for High-Fidelity Video Generation and Editing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72649-1_12)

*   **[ECCV 2024]** ***DeCo:*** *Decoupled Human-Centered Diffusion Video Editing with Motion Consistency*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-72784-9_20)





</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>


- [MAKIMA:¬†Tuning‚Äëfree¬†Multi‚ÄëAttribute¬†Open‚Äëdomain¬†Video¬†Editing¬†via¬†Mask‚ÄëGuided¬†Attention¬†Modulation](http://arxiv.org/abs/2412.19978v1)
- [DriveEditor:¬†A¬†Unified¬†3D¬†Information‚ÄëGuided¬†Framework¬†for¬†Controllable¬†Object¬†Editing¬†in¬†Driving¬†Scenes](http://arxiv.org/abs/2412.19458v2) [![GitHub Stars](https://img.shields.io/github/stars/yvanliang/DriveEditor?style=social)](https://github.com/yvanliang/DriveEditor)
- [Re‚ÄëAttentional¬†Controllable¬†Video¬†Diffusion¬†Editing](http://arxiv.org/abs/2412.11710v1)
- [MoViE:¬†Mobile¬†Diffusion¬†for¬†Video¬†Editing](http://arxiv.org/abs/2412.06578v1)
- [DIVE:¬†Taming¬†DINO¬†for¬†Subject‚ÄëDriven¬†Video¬†Editing](http://arxiv.org/abs/2412.03347v2) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://dino-video-editing.github.io/)
- [Trajectory¬†Attention¬†for¬†Fine‚Äëgrained¬†Video¬†Motion¬†Control](http://arxiv.org/abs/2411.19324v1)
- [VideoDirector:¬†Precise¬†Video¬†Editing¬†via¬†Text‚Äëto‚ÄëVideo¬†Models](http://arxiv.org/abs/2411.17592v3)
- [StableV2V:¬†Stablizing¬†Shape¬†Consistency¬†in¬†Video‚Äëto‚ÄëVideo¬†Editing](http://arxiv.org/abs/2411.11045v1)
- [OnlyFlow:¬†Optical¬†Flow¬†based¬†Motion¬†Conditioning¬†for¬†Video¬†Diffusion¬†Models](http://arxiv.org/abs/2411.10501v1) [![GitHub Stars](https://img.shields.io/github/stars/obvious-research/OnlyFlow?style=social)](https://github.com/obvious-research/OnlyFlow) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://obvious-research.github.io/onlyflow/) [![Huggingface¬†Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/obvious-research/onlyflow)
- [A¬†Reinforcement¬†Learning‚ÄëBased¬†Automatic¬†Video¬†Editing¬†Method¬†Using¬†Pre‚Äëtrained¬†Vision‚ÄëLanguage¬†Model](http://arxiv.org/abs/2411.04942v1)
- [Taming¬†Rectified¬†Flow¬†for¬†Inversion¬†and¬†Editing](http://arxiv.org/abs/2411.04746v3)
- [AutoVFX:¬†Physically¬†Realistic¬†Video¬†Editing¬†from¬†Natural¬†Language¬†Instructions](http://arxiv.org/abs/2411.02394v1)
- [Shaping¬†a¬†Stabilized¬†Video¬†by¬†Mitigating¬†Unintended¬†Changes¬†for¬†Concept‚ÄëAugmented¬†Video¬†Editing](http://arxiv.org/abs/2410.12526v2)
- [RNA:¬†Video¬†Editing¬†with¬†ROI‚Äëbased¬†Neural¬†Atlas](http://arxiv.org/abs/2410.07600v1)
- [FreeMask:¬†Rethinking¬†the¬†Importance¬†of¬†Attention¬†Masks¬†for¬†Zero‚ÄëShot¬†Video¬†Editing](http://arxiv.org/abs/2409.20500v1)
- [DNI:¬†Dilutional¬†Noise¬†Initialization¬†for¬†Diffusion¬†Video¬†Editing](http://arxiv.org/abs/2409.13037v1)
- [Blended¬†Latent¬†Diffusion¬†under¬†Attention¬†Control¬†for¬†Real‚ÄëWorld¬†Video¬†Editing](http://arxiv.org/abs/2409.03514v1)
- [DeCo:¬†Decoupled¬†Human‚ÄëCentered¬†Diffusion¬†Video¬†Editing¬†with¬†Motion¬†Consistency](http://arxiv.org/abs/2408.07481v1)
- [InVi:¬†Object¬†Insertion¬†In¬†Videos¬†Using¬†Off‚Äëthe‚ÄëShelf¬†Diffusion¬†Models](http://arxiv.org/abs/2407.10958v1)
- [MVOC:¬†A¬†Training‚ÄëFree¬†Multiple¬†Video¬†Object¬†Composition¬†Method¬†with¬†Diffusion¬†Models](http://arxiv.org/abs/2406.15829v1)
- [VIA:¬†Unified¬†Spatiotemporal¬†Video¬†Adaptation¬†Framework¬†for¬†Global¬†and¬†Local¬†Video¬†Editing](http://arxiv.org/abs/2406.12831v3)
- [COVE:¬†Unleashing¬†the¬†Diffusion¬†Feature¬†Correspondence¬†for¬†Consistent¬†Video¬†Editing](http://arxiv.org/abs/2406.08850v2) [![GitHub Stars](https://img.shields.io/github/stars/wangjiangshan0725/COVE?style=social)](https://github.com/wangjiangshan0725/COVE) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://cove-video.github.io/)
- [NaRCan:¬†Natural¬†Refined¬†Canonical¬†Image¬†with¬†Integration¬†of¬†Diffusion¬†Prior¬†for¬†Video¬†Editing](http://arxiv.org/abs/2406.06523v2)
- [FRAG:¬†Frequency¬†Adapting¬†Group¬†for¬†Diffusion¬†Video¬†Editing](http://arxiv.org/abs/2406.06044v2) [![GitHub Stars](https://img.shields.io/github/stars/dbstjswo505/FRAG?style=social)](https://github.com/dbstjswo505/FRAG) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://dbstjswo505.github.io/FRAG-page/)
- [Zero‚ÄëShot¬†Video¬†Editing¬†through¬†Adaptive¬†Sliding¬†Score¬†Distillation](http://arxiv.org/abs/2406.04888v2)
- [Ada‚ÄëVE:¬†Training‚ÄëFree¬†Consistent¬†Video¬†Editing¬†Using¬†Adaptive¬†Motion¬†Prior](http://arxiv.org/abs/2406.04873v2)
- [Enhancing¬†Temporal¬†Consistency¬†in¬†Video¬†Editing¬†by¬†Reconstructing¬†Videos¬†with¬†3D¬†Gaussian¬†Splatting](http://arxiv.org/abs/2406.02541v4)
- [Temporally¬†Consistent¬†Object¬†Editing¬†in¬†Videos¬†using¬†Extended¬†Attention](http://arxiv.org/abs/2406.00272v1)
- [MotionFollower:¬†Editing¬†Video¬†Motion¬†via¬†Lightweight¬†Score‚ÄëGuided¬†Diffusion](http://arxiv.org/abs/2405.20325v1)
- [Streaming¬†Video¬†Diffusion:¬†Online¬†Video¬†Editing¬†with¬†Diffusion¬†Models](http://arxiv.org/abs/2405.19726v1)
- [I2VEdit:¬†First‚ÄëFrame‚ÄëGuided¬†Video¬†Editing¬†via¬†Image‚Äëto‚ÄëVideo¬†Diffusion¬†Models](http://arxiv.org/abs/2405.16537v1)
- [ReVideo:¬†Remake¬†a¬†Video¬†with¬†Motion¬†and¬†Content¬†Control](http://arxiv.org/abs/2405.13865v1)
- [Slicedit:¬†Zero‚ÄëShot¬†Video¬†Editing¬†With¬†Text‚Äëto‚ÄëImage¬†Diffusion¬†Models¬†Using¬†Spatio‚ÄëTemporal¬†Slices](http://arxiv.org/abs/2405.12211v1) [![GitHub Stars](https://img.shields.io/github/stars/fallenshock/Slicedit?style=social)](https://github.com/fallenshock/Slicedit) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://matankleiner.github.io/slicedit/)
- [GenVideo:¬†One‚Äëshot¬†Target‚Äëimage¬†and¬†Shape¬†Aware¬†Video¬†Editing¬†using¬†T2I¬†Diffusion¬†Models](http://arxiv.org/abs/2404.12541v1)
- [Ctrl‚ÄëAdapter:¬†An¬†Efficient¬†and¬†Versatile¬†Framework¬†for¬†Adapting¬†Diverse¬†Controls¬†to¬†Any¬†Diffusion¬†Model](http://arxiv.org/abs/2404.09967v2)
- [S3Editor:¬†A¬†Sparse¬†Semantic‚ÄëDisentangled¬†Self‚ÄëTraining¬†Framework¬†for¬†Face¬†Video¬†Editing](http://arxiv.org/abs/2404.08111v1)
- [ExpressEdit:¬†Video¬†Editing¬†with¬†Natural¬†Language¬†and¬†Sketching](http://arxiv.org/abs/2403.17693v1)
- [EVA:¬†Zero‚Äëshot¬†Accurate¬†Attributes¬†and¬†Multi‚ÄëObject¬†Video¬†Editing](http://arxiv.org/abs/2403.16111v1)
- [Edit3K:¬†Universal¬†Representation¬†Learning¬†for¬†Video¬†Editing¬†Components](http://arxiv.org/abs/2403.16048v2)
- [Videoshop:¬†Localized¬†Semantic¬†Video¬†Editing¬†with¬†Noise‚ÄëExtrapolated¬†Diffusion¬†Inversion](http://arxiv.org/abs/2403.14617v3)
- [AnyV2V:¬†A¬†Tuning‚ÄëFree¬†Framework¬†For¬†Any¬†Video‚Äëto‚ÄëVideo¬†Editing¬†Tasks](http://arxiv.org/abs/2403.14468v4)
- [DreamMotion:¬†Space‚ÄëTime¬†Self‚ÄëSimilar¬†Score¬†Distillation¬†for¬†Zero‚ÄëShot¬†Video¬†Editing](http://arxiv.org/abs/2403.12002v2)
- [EffiVED:¬†Efficient¬†Video¬†Editing¬†via¬†Text‚Äëinstruction¬†Diffusion¬†Models](http://arxiv.org/abs/2403.11568v2)
- [AICL:¬†Action¬†In‚ÄëContext¬†Learning¬†for¬†Video¬†Diffusion¬†Model](http://arxiv.org/abs/2403.11535v2)
- [Video¬†Editing¬†via¬†Factorized¬†Diffusion¬†Distillation](http://arxiv.org/abs/2403.09334v2)
- [VLOGGER:¬†Multimodal¬†Diffusion¬†for¬†Embodied¬†Avatar¬†Synthesis](http://arxiv.org/abs/2403.08764v1)
- [FastVideoEdit:¬†Leveraging¬†Consistency¬†Models¬†for¬†Efficient¬†Text‚Äëto‚ÄëVideo¬†Editing](http://arxiv.org/abs/2403.06269v2)
- [Place¬†Anything¬†into¬†Any¬†Video](http://arxiv.org/abs/2402.14316v1)
- [UniEdit:¬†A¬†Unified¬†Tuning‚ÄëFree¬†Framework¬†for¬†Video¬†Motion¬†and¬†Appearance¬†Editing](http://arxiv.org/abs/2402.13185v4)
- [Anything¬†in¬†Any¬†Scene:¬†Photorealistic¬†Video¬†Object¬†Insertion](http://arxiv.org/abs/2401.17509v1)
- [Object‚ÄëCentric¬†Diffusion¬†for¬†Efficient¬†Video¬†Editing](http://arxiv.org/abs/2401.05735v3)
- [VASE:¬†Object‚ÄëCentric¬†Appearance¬†and¬†Shape¬†Manipulation¬†of¬†Real¬†Videos](http://arxiv.org/abs/2401.02473v1)
- [Moonshot:¬†Towards¬†Controllable¬†Video¬†Generation¬†and¬†Editing¬†with¬†Multimodal¬†Conditions](http://arxiv.org/abs/2401.01827v1)



</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="controllable">üïπÔ∏è Controllable Video Generation</span>


<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[AAAI 2026]** ***OmniVDiff:*** *Omni Controllable Video Diffusion for Generation and Understanding*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2504.10825) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tele-ai.github.io/OmniVDiff/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Tele-AI/OmniVDiff)

* **[AAAI 2026]** ***MotionFlow:*** *Attention-Driven Motion Transfer in Video Diffusion Models*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.05275) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://motionflow-diffusion.github.io/)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>


<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***IM-Zero:*** *Instance-level Motion Controllable Video Generation in a Zero-shot Manner*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IM-Zero_Instance-level_Motion_Controllable_Video_Generation_in_a_Zero-shot_Manner_CVPR_2025_paper.html)

*   **[CVPR 2025]** ***AnimateAnything:*** *Consistent and Controllable Animation for Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Lei_AnimateAnything_Consistent_and_Controllable_Animation_for_Video_Generation_CVPR_2025_paper.html)

*   **[CVPR 2025]** *Customized Condition Controllable Generation for Video Soundtrack*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Customized_Condition_Controllable_Generation_for_Video_Soundtrack_CVPR_2025_paper.html) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/FanQi-AI/CCCG-Video-Soundtrack) 

*   **[CVPR 2025]** ***StarGen:*** *A Spatiotemporal Autoregression Framework with Video Diffusion Model for Scalable and Controllable Scene Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_StarGen_A_Spatiotemporal_Autoregression_Framework_with_Video_Diffusion_Model_for_CVPR_2025_paper.html)

*   **[ICCV 2025]** ***Perception-as-Control:*** *Fine-grained Controllable Image Animation with 3D-aware Motion Representation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2501.05020)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://chen-yingjie.github.io/projects/Perception-as-Control/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/chen-yingjie/Perception-as-Control)

*   **[ICCV 2025]** ***MagicMirror:*** *ID-Preserved Video Generation in Video Diffusion Transformers*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2501.03931)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/dvlab-research/MagicMirror)

*   **[ICCV 2025]** ***MagicDrive-V2:*** *High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1169)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://gaoruiyuan.com/magicdrive-v2/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/flymin/MagicDrive-V2)

*   **[ICCV 2025]** ***InfiniCube:*** *Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2324)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://research.nvidia.com/labs/toronto-ai/infinicube/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nv-tlabs/InfiniCube)

*   **[ICCV 2025]** ***Free-Form Motion Control (SynFMC):*** *Controlling the 6D Poses of Camera and Objects in Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/414)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://henghuiding.com/SynFMC/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/FudanCVL/SynFMC)

*   **[ICCV 2025]** ***RealCam-I2V:*** *Real-World Image-to-Video Generation with Interactive Complex Camera Control*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1440)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zgctroy.github.io/RealCam-I2V/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ZGCTroy/RealCam-I2V)

*   **[ICCV 2025]** ***MagicMotion:*** *Video Generation with a Smart Director*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2506)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://quanhaol.github.io/magicmotion-site/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/quanhaol/MagicMotion)

*   **[ICCV 2025]** ***UniMLVG:*** *Unified Framework for Multi-view Long Video Generation with Comprehensive Control Capabilities for Autonomous Driving*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.04842)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://sensetime-fvg.github.io/UniMLVG/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/SenseTime-FVG/UniMLVG)

*   **[ICLR 2025]** ***MotionClone:*** *Training-Free Motion Cloning for Controllable Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=aY3L65HgHJ) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge)](https://github.com/LPengYang/MotionClone) 

*   **[AAAI 2025]** ***CAGE:*** *Unsupervised Visual Composition and Animation for Controllable Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i15.33775) [![GitHub](https://img.shields.io/badge/GitHub-Repository-blue?style=for-the-badge)](https://github.com/Araachie/cage)

*   **[AAAI 2025]** ***TrackGo:*** *A Flexible and Efficient Method for Controllable Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i10.33167)


*   **[WACV 2025]** *Fine-grained Controllable Video Generation via Object Appearance and Context*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/WACV61041.2025.00364)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



- [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](http://arxiv.org/abs/2506.03150v1)¬†[![GitHub Stars](https://img.shields.io/github/stars/yuanze-lin/IllumiCraft?style=social)](https://github.com/yuanze-lin/IllumiCraft)
- [ATI: Any Trajectory Instruction for Controllable Video Generation](http://arxiv.org/abs/2505.22944v3)¬†[![GitHub Stars](https://img.shields.io/github/stars/bytedance/ATI?style=social)](https://github.com/bytedance/ATI) ¬†[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://anytraj.github.io/) ¬†[![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/bytedance-research/ATI)
- [CamContextI2V: Context‚Äëaware Controllable Video Generation](http://arxiv.org/abs/2504.06022v1)¬†[![GitHub Stars](https://img.shields.io/github/stars/LDenninger/CamContextI2V?style=social)](https://github.com/LDenninger/CamContextI2V)
- [Any2Caption: Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)¬†[![GitHub Stars](https://img.shields.io/github/stars/ChocoWu/Any2Caption?style=social)](https://github.com/ChocoWu/Any2Caption)
- [MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance](http://arxiv.org/abs/2503.16421v2)¬†[![GitHub Stars](https://img.shields.io/github/stars/quanhaol/MagicMotion?style=social)](https://github.com/quanhaol/MagicMotion) ¬†[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://quanhaol.github.io/magicmotion-site/) ¬†[![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/quanhaol/MagicMotion)
- [MotionAgent: Fine-grained Controllable Video Generation via Motion Field Agent](http://arxiv.org/abs/2502.03207v1)
- [Controllable Video Generation with Provable Disentanglement](http://arxiv.org/abs/2502.02690v2)



</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2024]** ***360DVD:*** *Controllable Panorama Video Generation with 360-Degree Video Diffusion Model*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00660) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Akaneqwq/360DVD)

*   **[CVPR 2024]** ***Panacea:*** *Panoramic and Controllable Video Generation for Autonomous Driving*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00659) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wenyuqing/panacea)

*   **[AAAI 2024]** *Decouple Content and Motion for Conditional Image-to-Video Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v38i5.28277)






</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving](http://arxiv.org/abs/2408.07605v1)
*   [InteractiveVideo: User-Centric Controllable Video Generation with Synergistic Multimodal Instructions](http://arxiv.org/abs/2402.03040v1)



</details>

</details>

<details>
<summary><h4>‚ú® 2023</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2023]** *Conditional Image-to-Video Generation with Latent Flow Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52729.2023.01769) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/nihaomiao/CVPR23_LFDM)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>



*   [Controllable Video Generation by Learning the Underlying Dynamical System with Neural ODE](http://arxiv.org/abs/2303.05323v2)



</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)


### <span id="audio">üó£Ô∏è Audio-Driven Video Generation</span>

<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[AAAI 2026]** ***EchoMimicV3:*** *EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2507.03905) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://antgroup.github.io/ai/echomimic_v3/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/antgroup/echomimic_v3) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BadToBest/EchoMimicV3)

* **[AAAI 2026]** ***FantasyTalking2:*** *FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2508.11255) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fantasy-amap.github.io/fantasy-talking2/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Fantasy-AMAP/fantasy-talking2) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/papers/2508.11255)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>


<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


*   **[CVPR 2025]** ***KeyFace:*** *Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2503.01715)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://antonibigata.github.io/KeyFace/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/antonibigata/keyface_cvpr)

*   **[CVPR 2025]** ***AudCast:*** *Audio-Driven Human Video Generation by Cascaded Diffusion Transformers*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2503.19824)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://guanjz20.github.io/projects/AudCast/)

*   **[CVPR 2025]** ***MoEE:*** *Mixture of Emotion Experts for Audio-Driven Portrait Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2501.01808)

*   **[CVPR 2025]** ***Teller:*** *Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2503.18429)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://teller-avatar.github.io/)

*   **[CVPR 2025]** ***INFP:*** *Audio-Driven Interactive Head Generation in Dyadic Conversations*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2412.04037)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grisoon.github.io/INFP/)


*   **[ICCV 2025]** ***FLOAT:*** *Generative Motion Latent Flow Matching for Audio-driven Talking Portrait*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.01064)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://deepbrainai-research.github.io/float/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/deepbrainai-research/float)

*   **[ICCV 2025]** ***GaussianSpeech:*** *Audio-Driven Personalized 3D Gaussian Avatars*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.18675)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shivangi-aneja.github.io/projects/gaussianspeech/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/shivangi-aneja/GaussianSpeech)

*   **[ICCV 2025]** ***ACTalker:*** *Audio-visual Controlled Video Diffusion with Masked Selective State Spaces Modeling for Natural Talking Head Generation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2504.02542)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://harlanhong.github.io/publications/actalker/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/harlanhong/ACTalker)

*   **[ICLR 2025]** ***Hallo2:*** *Long-Duration and High-Resolution Audio-Driven Portrait Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2410.07718)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fudan-generative-vision.github.io/hallo2/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fudan-generative-vision/hallo2)

*   **[ICLR 2025]** ***Loopy:*** *Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2409.02634)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://loopyavataranony.github.io/)

*   **[ICLR 2025]** ***CyberHost:*** *A One-stage Diffusion Framework for Audio-driven Talking Body Generation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2409.01876)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cyberhost.github.io/)

* **[NeurIPS 2025]** ***MTV:*** *Audio-Sync Video Generation with Multi-Stream Temporal Control*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/120270) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hjzheng.net/projects/MTV/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/suimuc/MTV_Framework) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/BAAI/MTVCraft)

*   **[AAAI 2025]** ***EchoMimic:*** *Lifelike Audio-Driven Portrait Animations through Editable Landmark Conditions*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2409.13689)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://antgroup.github.io/ai/echomimic/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/antgroup/echomimic)
    [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/spaces/AntGroup/EchoMimic)

*   **[AAAI 2025]** ***PointTalk:*** *Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2410.02700)


</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Scaling Up Audio‚ÄëSynchronized Visual Animation: An Efficient Training Paradigm](http://arxiv.org/abs/2508.03955v1)
- [SpA2V: Harnessing Spatial Auditory Cues for Audio‚Äëdriven Spatially‚Äëaware Video Generation](http://arxiv.org/abs/2508.00782v1)
- [OmniAvatar: Efficient Audio‚ÄëDriven Avatar Video Generation with Adaptive Body Animation](http://arxiv.org/abs/2506.18866v1) [![GitHub Stars](https://img.shields.io/github/stars/Omni-Avatar/OmniAvatar?style=social)](https://github.com/Omni-Avatar/OmniAvatar)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://omni-avatar.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/OmniAvatar/OmniAvatar-14B)
- [InterActHuman: Multi‚ÄëConcept Human Animation with Layout‚ÄëAligned Audio Conditions](http://arxiv.org/abs/2506.09984v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://zhenzhiwang.github.io/interacthuman/)
- [AlignHuman: Improving Motion and Fidelity via Timestep‚ÄëSegment Preference Optimization for Audio‚ÄëDriven Human Animation](http://arxiv.org/abs/2506.11144v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://alignhuman.github.io/)
- [Audio‚ÄëSync Video Generation with Multi‚ÄëStream Temporal Control](http://arxiv.org/abs/2506.08003v1) [![GitHub Stars](https://img.shields.io/github/stars/baaivision/MTVCraft?style=social)](https://github.com/baaivision/MTVCraft)   [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/BAAI/MTVCraft)
- [LLIA¬†‚Äî¬†Enabling Low‚ÄëLatency Interactive Avatars: Real‚ÄëTime Audio‚ÄëDriven Portrait Video Generation with Diffusion Models](http://arxiv.org/abs/2506.05806v1) [![GitHub Stars](https://img.shields.io/github/stars/MeiGen-AI/llia?style=social)](https://github.com/MeiGen-AI/llia)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://meigen-ai.github.io/llia/)
- [TalkingMachines: Real‚ÄëTime Audio‚ÄëDriven FaceTime‚ÄëStyle Video via Autoregressive Diffusion Models](http://arxiv.org/abs/2506.03099v1) [![GitHub Stars](https://img.shields.io/github/stars/aaxwaz/TalkingMachines?style=social)](https://github.com/aaxwaz/TalkingMachines)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://aaxwaz.github.io/TalkingMachines/)


</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***FaceTalk:*** *Audio-Driven Motion Diffusion for Neural Parametric Head Models*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2312.08459)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://shivangi-aneja.github.io/projects/facetalk/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/shivangi-aneja/FaceTalk)


*   **[ECCV 2024]** ***UniTalker:*** *Scaling up Audio-Driven 3D Facial Animation Through A Unified Model*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2408.00762)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://x-niper.github.io/unitalker/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/X-niper/UniTalker)

*   **[ECCV 2024]** *Audio-Driven Talking Face Generation with Stabilized Synchronization Loss*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2307.09368)

*   **[NeurIPS 2024]** ***VASA-1:*** *Lifelike Audio-Driven Talking Faces Generated in Real Time*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/abs/2404.10667)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://www.microsoft.com/en-us/research/project/vasa-1/)




</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [AV‚ÄëLink: Temporally‚ÄëAligned Diffusion Features for Cross‚ÄëModal Audio‚ÄëVideo Generation](http://arxiv.org/abs/2412.15191v2) [![GitHub Stars](https://img.shields.io/github/stars/snap-research/AVLink?style=social)](https://github.com/snap-research/AVLink)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/AVLink/)
- [SAVGBench: Benchmarking Spatially Aligned Audio‚ÄëVideo Generation](http://arxiv.org/abs/2412.13462v1)
- [SINGER: Vivid Audio‚Äëdriven Singing Video Generation with Multi‚Äëscale Spectral Diffusion Model](http://arxiv.org/abs/2412.03430v1) [![GitHub Stars](https://img.shields.io/github/stars/yl4467/singer?style=social)](https://github.com/yl4467/singer)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://yl4467.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/yl2333/SINGER)
- [SyncFlow: Toward Temporally Aligned Joint Audio‚ÄëVideo Generation from Text](http://arxiv.org/abs/2412.15220v1)
- [FLOAT: Generative Motion Latent Flow Matching for Audio‚Äëdriven Talking Portrait](http://arxiv.org/abs/2412.01064v4) [![GitHub Stars](https://img.shields.io/github/stars/deepbrainai-research/float?style=social)](https://github.com/deepbrainai-research/float)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://deepbrainai-research.github.io/float/)
- [Stereo‚ÄëTalker: Audio‚Äëdriven 3D Human Synthesis with Prior‚ÄëGuided Mixture‚Äëof‚ÄëExperts](http://arxiv.org/abs/2410.23836v1)
- [A Simple but Strong Baseline for Sounding Video Generation: Effective Adaptation of Audio and Video Diffusion Models for Joint Generation](http://arxiv.org/abs/2409.17550v3)
- [DiffTED: One‚Äëshot Audio‚Äëdriven TED Talk Video Generation with Diffusion‚Äëbased Co‚Äëspeech Gestures](http://arxiv.org/abs/2409.07649v1) [![GitHub Stars](https://img.shields.io/github/stars/Ditzley/DiffTED?style=social)](https://github.com/Ditzley/DiffTED)

</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="human">üíÉ Human Image Animation</span>

<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


* **[AAAI 2026]** ***FantasyHSI:*** *Video-Generation-Centric 4D Human Synthesis In Any Scene through A Graph-based Multi-Agent Framework*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2509.01232.pdf) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fantasy-amap.github.io/fantasy-hsi/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Fantasy-AMAP/fantasy-hsi)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>


<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** *X-Dyna: Expressive Dynamic Human Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2501.10021)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/bytedance/X-Dyna)

*   **[CVPR 2025]** *StableAnimator: High-Quality Identity-Preserving Human Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2411.17697)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://francis-rings.github.io/StableAnimator/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Francis-Rings/StableAnimator)

*   **[CVPR 2025]** ***Disco4D:*** *Disentangled 4D Human Generation and Animation from a Single Image*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Disco4D_Disentangled_4D_Human_Generation_and_Animation_from_a_Single_CVPR_2025_paper.html)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://disco-4d.github.io/)

*   **[ICCV 2025]** ***DreamActor-M1:*** *Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/288)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://grisoon.github.io/DreamActor-M1/)

*   **[ICCV 2025]** ***Animate Anyone 2:*** *High-Fidelity Character Image Animation with Environment Affordance*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2502.06145)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://humanaigc.github.io/animate-anyone-2/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HumanAIGC/animate-anyone-2)

*   **[ICCV 2025]** *Multi-identity Human Image Animation with Structural Video Diffusion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/638)

*   **[ICCV 2025]** ***OmniHuman-1:*** *Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2201)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://omnihuman-lab.github.io/)

*   **[ICCV 2025]** ***AdaHuman:*** *Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2505.24877)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nvlabs.github.io/AdaHuman/)

*   **[ICCV 2025]** ***Ponimator:*** *Unfolding Interactive Pose for Versatile Human-human Interaction Animation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1453)


*   **[ICLR 2025]** ***Animate-X:*** *Universal Character Image Animation with Enhanced Motion Representation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2411.17697)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lucaria-academy.github.io/Animate-X/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/antgroup/animate-x)



</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](http://arxiv.org/abs/2507.15064v1) [![GitHub Stars](https://img.shields.io/github/stars/Francis-Rings/StableAnimator?style=social)](https://github.com/Francis-Rings/StableAnimator) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://francis-rings.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/FrancisRing/StableAnimator)
- [HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions](http://arxiv.org/abs/2505.22977v1) [![GitHub Stars](https://img.shields.io/github/stars/vivoCameraResearch/Hyper-Motion?style=social)](https://github.com/vivoCameraResearch/Hyper-Motion) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vivocameraresearch.github.io/hypermotion/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/shuolin/HyperMotion)
- [MTVCrafter: 4D Motion Tokenization for Open‚ÄëWorld Human Image Animation](http://arxiv.org/abs/2505.10238v4) [![GitHub Stars](https://img.shields.io/github/stars/DINGYANB/MTVCrafter?style=social)](https://github.com/DINGYANB/MTVCrafter) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/yanboding/MTVCrafter)
- [TT‚ÄëDF: A Large‚ÄëScale Diffusion‚ÄëBased Dataset and Benchmark for Human Body Forgery Detection](http://arxiv.org/abs/2505.08437v1) [![GitHub Stars](https://img.shields.io/github/stars/HashTAG00002/TT-DF?style=social)](https://github.com/HashTAG00002/TT-DF)
- [AnimateAnywhere: Rouse the Background in Human Image Animation](http://arxiv.org/abs/2504.19834v1) [![GitHub Stars](https://img.shields.io/github/stars/liuxiaoyu1104/AnimateAnywhere?style=social)](https://github.com/liuxiaoyu1104/AnimateAnywhere)
- [UniAnimate‚ÄëDiT: Human Image Animation with Large‚ÄëScale Video Diffusion Transformer](http://arxiv.org/abs/2504.11289v1) [![GitHub Stars](https://img.shields.io/github/stars/ali-vilab/UniAnimate-DiT?style=social)](https://github.com/ali-vilab/UniAnimate-DiT) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ZheWang123/UniAnimate-DiT)
- [Taming Consistency Distillation for Accelerated Human Image Animation](http://arxiv.org/abs/2504.11143v1)
- [Multi‚Äëidentity Human Image Animation with Structural Video Diffusion](http://arxiv.org/abs/2504.04126v1)
- [DreamActor‚ÄëM1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance](http://arxiv.org/abs/2504.01724v3) [![GitHub Stars](https://img.shields.io/github/stars/grisoon/DreamActor-M1?style=social)](https://github.com/grisoon/DreamActor-M1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://grisoon.github.io/DreamActor-M1/)
- [DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High‚ÄëQuality Human Image Animation](http://arxiv.org/abs/2503.21246v2) [![GitHub Stars](https://img.shields.io/github/stars/gulucaptain/DynamiCtrl?style=social)](https://github.com/gulucaptain/DynamiCtrl)
- [EvAnimate: Event‚Äëconditioned Image‚Äëto‚ÄëVideo Generation for Human Animation](http://arxiv.org/abs/2503.18552v2) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://potentialming.github.io/projects/EvAnimate)

</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>



*   **[CVPR 2024]** *MotionFollower: Editing Video Motion via Lightweight Score-Guided Diffusion*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://arxiv.org/pdf/2405.20325)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://francis-rings.github.io/MotionFollower/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Francis-Rings/MotionFollower)

*   **[CVPR 2024]** *MotionEditor: Editing Video Motion via Content-Aware Diffusion*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://openaccess.thecvf.com/content/CVPR2024/papers/Tu_MotionEditor_Editing_Video_Motion_via_Content-Aware_Diffusion_CVPR_2024_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://francis-rings.github.io/MotionEditor/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Francis-Rings/MotionEditor)


*   **[CVPR 2024]** ***MagicAnimate:*** *Temporally Consistent Human Image Animation using Diffusion Model*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1109/CVPR52733.2024.00147)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://showlab.github.io/magicanimate/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/magic-research/magic-animate)

*   **[ECCV 2024]** ***Champ:*** *Controllable and Consistent Human Image Animation with 3D Parametric Guidance*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://doi.org/10.1007/978-3-031-73001-6_9)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://fudan-generative-vision.github.io/champ/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/fudan-generative-vision/champ)

*   **[NeurIPS 2024]** ***HumanVid:*** *Demystifying Training Data for Camera-controllable Human Image Animation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/23f3a0f82d79d985b6076bc84d14f66b-Abstract-Datasets_and_Benchmarks_Track.html)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://humanvid.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zhenzhiwang/HumanVid)

*   **[NeurIPS 2024]** ***TPC:*** *Test-time Procrustes Calibration for Diffusion-based Human Image Animation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/d6fdc3002dda4c3cad3d595ac6fa5352-Abstract-Conference.html)

*   **[ICLR 2024]** *DisPose: Disentangling Pose Guidance for Controllable Human Image Animation*<br>
    [![ArXiv](https://img.shields.io/badge/arXiv-PDF-b31b1b?style=for-the-badge&logo=arxiv)](https://openreview.net/forum?id=AumOa10MKG)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://lihxxx.github.io/DisPose/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/lihxxx/DisPose)


</details>



<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [DreamDance: Animating Human Images by Enriching 3D Geometry Cues from 2D Poses](http://arxiv.org/abs/2412.00397v1) [![GitHub Stars](https://img.shields.io/github/stars/PKU-YuanGroup/DreamDance?style=social)](https://github.com/PKU-YuanGroup/DreamDance) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://pang-yatian.github.io/Dreamdance-webpage/)
- [High Quality Human Image Animation using Regional Supervision and Motion Blur Condition](http://arxiv.org/abs/2409.19580v1)
- [Dormant: Defending against Pose-driven Human Image Animation](http://arxiv.org/abs/2409.14424v2) [![GitHub Stars](https://img.shields.io/github/stars/Manu21JC/Dormant?style=social)](https://github.com/Manu21JC/Dormant)
- [TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models](http://arxiv.org/abs/2407.09012v1) [![GitHub Stars](https://img.shields.io/github/stars/eccv2024tcan/TCAN?style=social)](https://github.com/eccv2024tcan/TCAN) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://eccv2024tcan.github.io/)
- [UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation](http://arxiv.org/abs/2406.01188v1) [![GitHub Stars](https://img.shields.io/github/stars/ali-vilab/UniAnimate?style=social)](https://github.com/ali-vilab/UniAnimate) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://unianimate.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ZheWang123/UniAnimate-DiT)
- [VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation](http://arxiv.org/abs/2405.18156v1) [![GitHub Stars](https://img.shields.io/github/stars/Kelu007/VividPose?style=social)](https://github.com/Kelu007/VividPose) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kelu007.github.io/vivid-pose/)


</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)



### <span id="acceleration">‚ö° Fast Video Generation (Acceleration)</span>

<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[AAAI 2026]** ***Turbo-VAED:*** *Fast and Stable Transfer of Video-VAEs to Mobile Devices*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2508.09136.pdf) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hustvl/Turbo-VAED) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/hustvl/Turbo-VAED)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>


<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2025]** *Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://cvpr.thecvf.com/virtual/2025/poster/34471)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://liewfeng.github.io/TeaCache/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/ali‚Äëvilab/TeaCache)

*   **[CVPR 2025]** *CausVid: From Slow Bidirectional to Fast Autoregressive VDMs*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2025/papers/Yin_From_Slow_Bidirectional_to_Fast_Autoregressive_Video_Diffusion_Models_CVPR_2025_paper.pdf)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://github.com/tianweiy/CausVid)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/tianweiy/CausVid)

*   **[CVPR 2025]** ***BlockDance:*** *Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=yJAk0n0NyU)

*   **[ICCV 2025]** ***AdaCache:*** *Adaptive Caching for Faster Video Generation with Diffusion Transformers*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.02397)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://adacache-dit.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/AdaCache-DiT/AdaCache)

*   **[ICCV 2025]** ***TaylorSeer:*** *From Reusing to Forecasting: Accelerating Diffusion Models with TaylorSeers*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/448)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://taylorseer.github.io/TaylorSeer/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Shenyi-Z/TaylorSeer)

*   **[ICCV 2025]** *Accelerating Diffusion Transformer via Gradient-Optimized Cache*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/1026)

*   **[ICCV 2025]** ***V.I.P.:*** *Iterative Online Preference Distillation for Efficient Video Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2508.03254)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jiiiisoo.github.io/VIP.github.io/)

*   **[ICCV 2025]** ***DMDX:*** *Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://iccv.thecvf.com/virtual/2025/poster/2528)

*   **[ICCV 2025]** ***OmniCache:*** *A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for DiT*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2508.16212)

*   **[ICLR 2025]** ***FasterCache:*** *Training-Free Video Diffusion Model Acceleration with High Quality*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=yJAk0n0NyU)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)]([[Ê≠§Â§ÑÊõøÊç¢‰∏∫GitHubÈìæÊé•]](https://github.com/Vchitect/FasterCache))


* **[NeurIPS 2025]** ***MagCache:*** *Fast Video Generation with Magnitude-Aware Cache*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://neurips.cc/virtual/2025/poster/118625) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zehong-ma.github.io/MagCache/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Zehong-Ma/MagCache)

*   **[ICML 2025]** ***Sparse VideoGen:***  *Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=u8CA3qIS0V)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://svg-project.github.io/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/svg-project/Sparse-VideoGen)

*   **[ICML 2025]** *Fast Video Generation with Sliding Tile Attention*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=U74MOXPEJd)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://hao-ai-lab.github.io/blogs/sta/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/hao-ai-lab/FastVideo)

*   **[ICML 2025]** ***Ca2-VDM:*** *Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://icml.cc/virtual/2025/poster/45139)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/Dawn‚ÄëLX/CausalCache‚ÄëVDM)

*   **[ICML 2025]** ***AsymRnR:*** *Video Diffusion Transformers Acceleration with Asymmetric Reduction and Restoration*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://icml.cc/virtual/2025/poster/46432)





</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder](https://huggingface.co/papers/2509.25182) [![GitHub Stars](https://img.shields.io/github/stars/dc-ai-projects/DC-VideoGen?style=social)](https://github.com/dc-ai-projects/DC-VideoGen)
- [LongLive: Real-time Interactive Long Video Generation](https://huggingface.co/papers/2509.22622) [![GitHub Stars](https://img.shields.io/github/stars/NVlabs/LongLive?style=social)](https://github.com/NVlabs/LongLive) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://nvlabs.github.io/LongLive) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/Efficient-Large-Model/LongLive-1.3B)
- [Rolling Forcing: Autoregressive Long Video Diffusion in Real Time](https://huggingface.co/papers/2509.25161) [![GitHub Stars](https://img.shields.io/github/stars/TencentARC/RollingForcing?style=social)](https://github.com/TencentARC/RollingForcing) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kunhao-liu.github.io/Rolling_Forcing_Webpage/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/TencentARC/RollingForcing)
- [UltraGen: High-Resolution Video Generation with Hierarchical Attention](https://huggingface.co/papers/2510.18775) [![GitHub Stars](https://img.shields.io/github/stars/sjtuplayer/UltraGen?style=social)](https://github.com/sjtuplayer/UltraGen) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sjtuplayer.github.io/projects/UltraGen)
- [Stable Video Infinity: Infinite-Length Video Generation with Error Recycling](https://huggingface.co/papers/2510.09212) [![GitHub Stars](https://img.shields.io/github/stars/vita-epfl/Stable-Video-Infinity?style=social)](https://github.com/vita-epfl/Stable-Video-Infinity) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://stable-video-infinity.github.io/homepage/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/vita-video-gen/svi-model)
- [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://huggingface.co/papers/2511.01266) [![GitHub Stars](https://img.shields.io/github/stars/alex4727/MotionStream?style=social)](https://github.com/alex4727/MotionStream)
- [Less is Enough: Training‚ÄëFree Video Diffusion¬†Acceleration via Runtime‚ÄëAdaptive¬†Caching](http://arxiv.org/abs/2507.02860v1) [![GitHub Stars](https://img.shields.io/github/stars/H-EmbodVis/EasyCache?style=social)](https://github.com/H-EmbodVis/EasyCache)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://h-embodvis.github.io/EasyCache/)
- [Compact Attention: Exploiting¬†Structured Spatio‚ÄëTemporal¬†Sparsity for Fast Video¬†Generation](http://arxiv.org/abs/2508.12969v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://yo-ava.github.io/Compact-Attention.github.io/)
- [MagCache: Fast Video Generation with Magnitude‚ÄëAware Cache](http://arxiv.org/abs/2506.09045v1) [![GitHub Stars](https://img.shields.io/github/stars/Zehong-Ma/MagCache?style=social)](https://github.com/Zehong-Ma/MagCache)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://zehong-ma.github.io/MagCache/)
- [Seedance¬†1.0: Exploring the Boundaries of¬†Video¬†Generation¬†Models](http://arxiv.org/abs/2506.09113v2)
- [SuperGen: An Efficient Ultra‚Äëhigh‚Äëresolution Video Generation System with Sketching and¬†Tiling](http://arxiv.org/abs/2508.17756v1)
- [MixCache: Mixture‚Äëof‚ÄëCache for Video Diffusion Transformer¬†Acceleration](http://arxiv.org/abs/2508.12691v1)
- [SwiftVideo: A Unified Framework for Few‚ÄëStep Video Generation through Trajectory‚ÄëDistribution¬†Alignment](http://arxiv.org/abs/2508.06082v1)
- [Taming Diffusion¬†Transformer for¬†Real‚ÄëTime Mobile¬†Video Generation](http://arxiv.org/abs/2507.13343v1) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://snap-research.github.io/mobile_video_dit/)
- [SRDiffusion: Accelerate Video Diffusion¬†Inference via¬†Sketching‚ÄëRendering Cooperation](http://arxiv.org/abs/2505.19151v1) [![GitHub Stars](https://img.shields.io/github/stars/alibaba/SRDiffusion?style=social)](https://github.com/alibaba/SRDiffusion)
- [Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic‚ÄëAware¬†Permutation](http://arxiv.org/abs/2505.18875v1)
- [DVD‚ÄëQuant: Data‚Äëfree Video Diffusion¬†Transformers¬†Quantization](http://arxiv.org/abs/2505.18663v1) [![GitHub Stars](https://img.shields.io/github/stars/lhxcs/DVD-Quant?style=social)](https://github.com/lhxcs/DVD-Quant)
- [AccVideo: Accelerating Video Diffusion Model with Synthetic¬†Dataset](http://arxiv.org/abs/2503.19462v1) [![GitHub Stars](https://img.shields.io/github/stars/aejion/accvideo?style=social)](https://github.com/aejion/accvideo)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://aejion.github.io/accvideo/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/aejion/AccVideo)
- [Region¬†Masking to Accelerate Video Processing on Neuromorphic¬†Hardware](http://arxiv.org/abs/2503.16775v1)
- [DSV: Exploiting Dynamic Sparsity to Accelerate Large‚ÄëScale Video DiT¬†Training](http://arxiv.org/abs/2502.07590v3)



</details>

</details>

<details>
<summary><h4>‚ú® 2024</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

*   **[CVPR 2024]** ***Cache Me if You Can:*** *Accelerating Diffusion Models through Block Caching*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024/papers/Wimbauer_Cache_Me_if_You_Can_Accelerating_Diffusion_Models_through_Block_CVPR_2024_paper.pdf)

*   **[NeurIPS 2024]** *Motion Consistency Model: Accelerating Video Diffusion with Disentangled Motion-Appearance Distillation*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/c859b99b5d717c9035e79d43dfd69435-Abstract-Conference.html)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://yhzhai.github.io/mcm/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/yhZhai/mcm)

*   **[NeurIPS 2024]** *Training-Free Adaptive Diffusion with Bounded Difference Approximation Strategy*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://openreview.net/forum?id=cS63YtJ49A)
    [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://jiakangyuan.github.io/AdaptiveDiffusion/)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UniModal4Reasoning/AdaptiveDiffusion)

*   **[NeurIPS 2024]** *Fast and Memory-Efficient Video Diffusion Using Streamlined Inference*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2411.01171)
    [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/wuyushuwys/FMEDiffusion)

*   **[IJCAI 2024]** ***FasterVD:*** *On Acceleration of Video Diffusion Models*<br>
     [![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://www.ijcai.org/proceedings/2024/1044.pdf)

</details>





<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Accelerating Video Diffusion Models via Distribution Matching](http://arxiv.org/abs/2412.05899v1)
- [Adaptive Caching for Faster Video Generation with Diffusion Transformers](http://arxiv.org/abs/2411.02397v2) [![GitHub Stars](https://img.shields.io/github/stars/AdaCache-DiT/AdaCache?style=social)](https://github.com/AdaCache-DiT/AdaCache)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://adacache-dit.github.io)
- [OSV: One Step is Enough for High-Quality Image to Video Generation](http://arxiv.org/abs/2409.11367v2)
- [HAVANA: Hierarchical stochastic neighbor embedding for Accelerated Video ANnotAtions](http://arxiv.org/abs/2409.10641v1)
- [AnimateDiff-Lightning: Cross-Model Diffusion Distillation](http://arxiv.org/abs/2403.12706v1) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ByteDance/AnimateDiff-Lightning)


</details>

</details>

[<small>‚áß Back to ToC</small>](#contents)

### <span id="rl">üéØ Reinforcement Learning for Video Generation</span>

<details>
<summary><h4>‚ú® 2026</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>

* **[AAAI 2026]** *VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation*<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/abs/2412.21059) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zai-org/VisionReward) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/zai-org/VisionReward-Video)

</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>
</details>

</details>


<details>
<summary><h4>‚ú® 2025</h4></summary>

<details>
<summary><h4>‚úÖ Published Papers</h4></summary>


* **[ICCV 2025]** ***LongAnimation:*** Long Animation Generation with Dynamic Global‚ÄëLocal Memory<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2507.01945) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://cn-makers.github.io/long_animation_web/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CN-makers/LongAnimation) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/CNcreator0331/LongAnimation) 

* **[ICCV 2025]** ***TesserAct:*** Learning 4D Embodied World Models<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](http://arxiv.org/pdf/2504.20995) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://tesseractworld.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/UMass-Embodied-AGI/TesserAct) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging_Face-yellow?style=for-the-badge)](https://huggingface.co/anyeZHY/tesseract)

* **[ICLR 2025]** ***DartControl:*** A Diffusion‚ÄëBased Autoregressive Motion Model for Real‚ÄëTime Text‚ÄëDriven Motion Control<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2410.05260) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://zkf1997.github.io/DART/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/zkf1997/DART)

* **[ICLR 2025]** ***FLIP:*** Flow‚ÄëCentric Generative Planning as General‚ÄëPurpose Manipulation World Model<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.08261) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://nus-lins-lab.github.io/flipweb/) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/HeegerGao/FLIP)

* **[CVPR 2025]** ***VideoDPO:*** Omni‚ÄëPreference Alignment for Video Diffusion Generation<br>
[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=for-the-badge)](https://arxiv.org/pdf/2412.14167) [![Project Page](https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white)](https://videodpo.github.io) [![GitHub](https://img.shields.io/badge/GitHub-Repo-blue?style=for-the-badge&logo=github)](https://github.com/CIntellifusion/VideoDPO)




</details>

<details>
<summary><h4>üí° Pre-Print Papers</h4></summary>

- [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/pdf/2510.02283.pdf) [![GitHub Stars](https://img.shields.io/github/stars/justincui03/Self-Forcing-Plus-Plus?style=social)](https://github.com/justincui03/Self-Forcing-Plus-Plus) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://self-forcing-plus-plus.github.io/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/gdhe17/Self-Forcing)
- [LongCat-Video Technical Report](https://arxiv.org/pdf/2510.22200.pdf) [![GitHub Stars](https://img.shields.io/github/stars/meituan-longcat/LongCat-Video?style=social)](https://github.com/meituan-longcat/LongCat-Video) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://meituan-longcat.github.io/LongCat-Video/) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/meituan-longcat/LongCat-Video)
- [PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning](https://arxiv.org/pdf/2510.13809.pdf) [![GitHub Stars](https://img.shields.io/github/stars/KwaiVGI/PhysMaster?style=social)](https://github.com/KwaiVGI/PhysMaster) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://sihuiji.github.io/PhysMaster-Page/)
- [LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment](https://arxiv.org/pdf/2412.04814) [![GitHub Stars](https://img.shields.io/github/stars/CodeGoat24/LiFT?style=social)](https://github.com/CodeGoat24/LiFT)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://codegoat24.github.io/LiFT/)
- [VideoScore: Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation](https://arxiv.org/abs/2406.15252) [![GitHub Stars](https://img.shields.io/github/stars/TIGER-AI-Lab/VideoScore?style=social)](https://github.com/TIGER-AI-Lab/VideoScore/)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://tiger-ai-lab.github.io/VideoScore/)
- [Video Perception Models for 3D Scene Synthesis](http://arxiv.org/pdf/2506.20601) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vipscene.github.io)
- [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](http://arxiv.org/pdf/2506.18655) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://wwenxu.github.io/RDPO/)
- [VQ‚ÄëInsight: Teaching VLMs for AI‚ÄëGenerated Video Quality Understanding via Progressive Visual Reinforcement Learning](http://arxiv.org/pdf/2506.18564)
- [Toward Rich Video Human‚ÄëMotion2D Generation](http://arxiv.org/pdf/2506.14428) [![GitHub Stars](https://img.shields.io/github/stars/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation?style=social)](https://github.com/FooAuto/Toward-Rich-Video-Human-Motion2D-Generation)
- [AlignHuman: Improving Motion and Fidelity via Timestep‚ÄëSegment Preference Optimization for Audio‚ÄëDriven Human Animation](http://arxiv.org/pdf/2506.11144) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://alignhuman.github.io/)
- [Multimodal Large Language Models: A Survey](http://arxiv.org/pdf/2506.10016)
- [Seedance¬†1.0: Exploring the Boundaries of Video Generation Models](http://arxiv.org/pdf/2506.09113)
- [ContentV: Efficient Training of Video Generation Models with Limited Compute](http://arxiv.org/pdf/2506.05343) [![GitHub Stars](https://img.shields.io/github/stars/bytedance/ContentV?style=social)](https://github.com/bytedance/ContentV)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://contentv.github.io) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/ByteDance/ContentV-8B)
- [Photography Perspective Composition: Towards Aesthetic Perspective Recommendation](http://arxiv.org/pdf/2505.20655) [![GitHub Stars](https://img.shields.io/github/stars/vivoCameraResearch/p-p-c?style=social)](https://github.com/vivoCameraResearch/p-p-c)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vivocameraresearch.github.io/ppc)
- [Scaling Image and Video Generation via Test‚ÄëTime Evolutionary Search](http://arxiv.org/pdf/2505.17618) [![GitHub Stars](https://img.shields.io/github/stars/tinnerhrhe/EvoSearch-codes?style=social)](https://github.com/tinnerhrhe/EvoSearch-codes)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://tinnerhrhe.github.io/evosearch/)
- [InfLVG: Reinforce Inference‚ÄëTime Consistent Long Video Generation with GRPO](http://arxiv.org/pdf/2505.17574) [![GitHub Stars](https://img.shields.io/github/stars/maple-aigc/InfLVG?style=social)](https://github.com/maple-aigc/InfLVG)
- [AvatarShield: Visual Reinforcement Learning for Human‚ÄëCentric Video Forgery Detection](http://arxiv.org/pdf/2505.15173) [![GitHub Stars](https://img.shields.io/github/stars/zhipeixu/AvatarShield?style=social)](https://github.com/zhipeixu/AvatarShield)
- [RLVR‚ÄëWorld: Training World Models with Reinforcement Learning](http://arxiv.org/pdf/2505.13934) [![GitHub Stars](https://img.shields.io/github/stars/thuml/RLVR-World?style=social)](https://github.com/thuml/RLVR-World)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://thuml.github.io/RLVR-World) [![Huggingface Face](https://img.shields.io/badge/Hugging-Face-orange?logo=website)](https://huggingface.co/collections/thuml/rlvr-world-682f331c75a904b8febc366a)
- [Diffusion‚ÄëNPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models](http://arxiv.org/pdf/2505.11245) [![GitHub Stars](https://img.shields.io/github/stars/G-U-N/Diffusion-NPO?style=social)](https://github.com/G-U-N/Diffusion-NPO)
- [DanceGRPO: Unleashing GRPO on Visual Generation](http://arxiv.org/pdf/2505.07818) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://dancegrpo.github.io/)
- [VideoHallu: Evaluating and Mitigating Multi‚Äëmodal Hallucinations on Synthetic Video Understanding](http://arxiv.org/pdf/2505.01481)
- [Reasoning Physical Video Generation with Diffusion Timestep Tokens via Reinforcement Learning](http://arxiv.org/pdf/2504.15932)
- [SkyReels‚ÄëV2: Infinite‚Äëlength Film Generative Model](http://arxiv.org/pdf/2504.13074) [![GitHub Stars](https://img.shields.io/github/stars/SkyworkAI/SkyReels-V2?style=social)](https://github.com/SkyworkAI/SkyReels-V2)
- [FingER: Content Aware Fine‚Äëgrained Evaluation with Reasoning for AI‚ÄëGenerated Videos](http://arxiv.org/pdf/2504.10358)
- [Aligning Anime Video Generation with Human Feedback](http://arxiv.org/pdf/2504.10044)
- [Discriminator‚ÄëFree Direct Preference Optimization for Video Diffusion](http://arxiv.org/pdf/2504.08542)
- [Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments](http://arxiv.org/pdf/2504.02918) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://physics-from-video.github.io/morpheus-bench/)
- [OmniCam: Unified Multimodal Video Generation via Camera Control](http://arxiv.org/pdf/2504.02312)
- [VPO: Aligning Text‚Äëto‚ÄëVideo Generation Models with Prompt Optimization](http://arxiv.org/pdf/2503.20491) [![GitHub Stars](https://img.shields.io/github/stars/thu-coai/VPO?style=social)](https://github.com/thu-coai/VPO)
- [Zero‚ÄëShot Human‚ÄëObject Interaction Synthesis with Multimodal Priors](http://arxiv.org/pdf/2503.20118) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://thorin666.github.io/projects/ZeroHOI)
- [Judge Anything: MLLM as a Judge Across Any¬†Modality](http://arxiv.org/pdf/2503.17489) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://urrealhero.github.io/judgeanythingweb/)
- [MagicID: Hybrid Preference Optimization for ID‚ÄëConsistent and Dynamic‚ÄëPreserved Video Customization](http://arxiv.org/pdf/2503.12689) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://echopluto.github.io/MagicID-project/)
- [Unified Reward Model for Multimodal Understanding and Generation](http://arxiv.org/pdf/2503.05236) [![GitHub Stars](https://img.shields.io/github/stars/codegoat24/UnifiedReward?style=social)](https://github.com/codegoat24/UnifiedReward)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://codegoat24.github.io/UnifiedReward/)
- [Pre‚ÄëTrained Video Generative Models as World Simulators](https://arxiv.org/pdf/2502.07825)
- [Harness Local Rewards for Global Benefits: Effective Text‚Äëto‚ÄëVideo Generation Alignment with Patch‚Äëlevel Reward Models](https://arxiv.org/pdf/2502.06812)
- [IPO: Iterative Preference Optimization for Text‚Äëto‚ÄëVideo Generation](https://arxiv.org/pdf/2502.02088)
- [MJ‚ÄëVIDEO: Fine‚ÄëGrained Benchmarking and Rewarding Video Preferences in Video Generation](https://arxiv.org/pdf/2502.01719) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://aiming-lab.github.io/MJ-VIDEO.github.io/)
- [HuViDPO:¬†Enhancing Video Generation through Direct Preference Optimization for Human‚ÄëCentric Alignment](https://arxiv.org/pdf/2502.01690)
- [Zeroth‚Äëorder Informed Fine‚ÄëTuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer](https://arxiv.org/pdf/2502.00639) [![GitHub Stars](https://img.shields.io/github/stars/RTkenny/RLR-Opimtizer?style=social)](https://github.com/RTkenny/RLR-Opimtizer)
- [Improving Video Generation with Human Feedback](https://arxiv.org/pdf/2501.13918) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://gongyeliu.github.io/videoalign)
- [VisionReward: Fine‚ÄëGrained Multi‚ÄëDimensional Human Preference Learning for Image and Video Generation](https://arxiv.org/pdf/2412.21059) [![GitHub Stars](https://img.shields.io/github/stars/THUDM/VisionReward?style=social)](https://github.com/THUDM/VisionReward)
- [OnlineVPO: Align Video Diffusion Model with Online Video‚ÄëCentric Preference Optimization](https://arxiv.org/pdf/2412.15159) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://onlinevpo.github.io/)
- [The Matrix: Infinite‚ÄëHorizon World Generation with Real‚ÄëTime Moving Control](https://arxiv.org/pdf/2412.03568) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://thematrix1999.github.io/)
- [Improving Dynamic Object Interactions in Text‚Äëto‚ÄëVideo Generation with AI Feedback](https://arxiv.org/pdf/2412.02617)
- [Free$^2$Guide: Gradient‚ÄëFree Path Integral Control for Enhancing Text‚Äëto‚ÄëVideo Generation with Large Vision‚ÄëLanguage Models](https://arxiv.org/pdf/2411.17041) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://kjm981995.github.io/free2guide/)
- [A Reinforcement Learning‚ÄëBased Automatic Video Editing Method Using Pre‚Äëtrained Vision‚ÄëLanguage Model](https://arxiv.org/pdf/2411.04942)
- [Video to Video Generative Adversarial Network for Few‚Äëshot Learning Based on Policy Gradient](https://arxiv.org/pdf/2410.20657)
- [WorldSimBench: Towards Video Generation Models as World Simulators](https://arxiv.org/pdf/2410.18072) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://iranqin.github.io/WorldSimBench.github.io)
- [Animating the Past: Reconstruct Trilobite via Video Generation](https://arxiv.org/pdf/2410.14715)
- [VideoAgent: Self‚ÄëImproving Video Generation](https://arxiv.org/pdf/2410.10076) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://video-as-agent.github.io)
- [E‚ÄëMotion: Future Motion Simulation via Event Sequence Diffusion](https://arxiv.org/pdf/2410.08649) [![GitHub Stars](https://img.shields.io/github/stars/p4r4mount/E-Motion?style=social)](https://github.com/p4r4mount/E-Motion)
- [SePPO: Semi‚ÄëPolicy Preference Optimization for Diffusion Alignment](https://arxiv.org/pdf/2410.05255) [![GitHub Stars](https://img.shields.io/github/stars/DwanZhang-AI/SePPO?style=social)](https://github.com/DwanZhang-AI/SePPO)
- [Video Diffusion Alignment via Reward Gradients](https://arxiv.org/pdf/2407.08737) [![GitHub Stars](https://img.shields.io/github/stars/mihirp1998/VADER?style=social)](https://github.com/mihirp1998/VADER)   [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://vader-vid.github.io/)
- [InstructVideo: Instructing Video Diffusion Models with Human Feedback](https://arxiv.org/pdf/2312.12490) [![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=website)](https://instructvideo.github.io/)
- [AdaDiff: Adaptive Step Selection for Fast Diffusion Models](https://arxiv.org/pdf/2311.14768) [![GitHub Stars](https://img.shields.io/github/stars/Tangshengku/AdaDiff?style=social)](https://github.com/Tangshengku/AdaDiff)



</details>

</details>


[<small>‚áß Back to ToC</small>](#contents)


---

## <span id="datasets">üóÇÔ∏è Datasets</span>
| Dataset Name | Year | Modalities | Task | Paper | Link |
| :--- | :--- | :--- | :--- | :---: | :---: |
| **UCF101** | 2012 | Video | Unconditional Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/1212.0402.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://www.crcv.ucf.edu/data/UCF101.php) |
| **TaiChi-HD** | 2019 | Video | Unconditional Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://papers.nips.cc/paper_files/paper/2019/file/31c0b36aef265d9221af80872ceb62f9-Paper.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/AliaksandrSiarohin/first-order-model) |
| **SkyTimeLapse** | 2020 | Video | Unconditional Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500290.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zhangzjn/DTVNet) |
| **WebVid-10M** | 2021 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2104.00650.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://maxbain.com/webvid-dataset/) |
| **HD-VG-130M** | 2023 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2305.10874.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/daooshee/HD-VG-130M) |
| **FETV** | 2023 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://papers.nips.cc/paper_files/paper/2023/file/c481049f7410f38e788f67c171c64ad5-Paper-Datasets_and_Benchmarks.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/llyx97/FETV) |
| **InternVid** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2307.06942) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid) |
| **VidProM** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2403.06098.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/WangWenhao0716/VidProM) |
| **Panda-70M** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2402.19479) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://tsaishienchen.github.io/panda-70m/) |
| **SafeSora** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.14477v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/PKU-Alignment/safe-sora) |
| **ChronoMagic-Pro**| 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.18522v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/PKU-YuanGroup/ChronoMagic-Bench) |
| **T2V-CompBench** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2407.14505v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/KaiyueSun98/T2V-CompBench) |
| **VidGen-1M** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2408.02629v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/SAIS-FUXI/VidGen) |
| **PhyGenBench** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2410.05363v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/OpenGVLab/PhyGenBench) |
| **DH-FaceVid-1K** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2410.07151v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/DH-FaceVid-1K/DH-FaceVid-1K) |
| **StoryEval** | 2024 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2412.16211v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/ypwang61/StoryEval) |
| **HOIGen-1M** | 2025 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2503.23715) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://huggingface.co/datasets/HOIGen/HOIGen-1M) |
| **OpenVid-1M** | 2025 | Text, Video | Text-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://openreview.net/forum?id=j7kdXSrISM) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://nju-pcalab.github.io/projects/openvid/) |
| **HumanVid** | 2024 | Image, Video | Image-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://papers.nips.cc/paper_files/paper/2024/hash/23f3a0f82d79d985b6076bc84d14f66b-Abstract-Datasets_and_Benchmarks_Track.html) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zhenzhiwang/HumanVid) |
| **TIP-I2V** | 2024 | Text, Image, Video | Image-to-Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.04709v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/WangWenhao0716/TIP-I2V) |
| **TC-Bench** | 2024 | Text, Image, Video | Text-to-Video, Image-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2406.08656v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/weixi-feng/tc-bench) |
| **AnimeShooter** | 2025 | Text, Image, Video | Text-to-Video, Image-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2506.03126v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://qiulu66.github.io/animeshooter/) |
| **VE-Bench** | 2024 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2408.11481v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/littlespray/VE-Bench) |
| **DAVIS-Edit** | 2024 | Text, Video, Image | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.11045v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/AlonzoLeeeooo/StableV2V) |
| **DAVIS** | 2017 | Video, Image | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/1704.00675.pdf) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://davischallenge.org/) |
| **VIVID-10M** | 2024 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2411.15260v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://huggingface.co/datasets/KwaiVGI/VIVID-10M) |
| **Se√±orita-2M** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2502.06734v3) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/zibojia/SENORITA) |
| **FiVE-Bench** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2503.13684v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/MinghanLi/FiVE-Bench) |
| **InsViE-1M** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2503.20287v2) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/langmanbusi/InsViE) |
| **VEU-Bench** | 2025 | Text, Video | Video Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2504.17828) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/VEU-Benchmark/VEU-Benchmark) |
| **OpenS2V-5M** | 2025 | Text, Video, Audio | Text-to-Video, Image-to-Video, Subject-to-Video | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2505.20292) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://pku-yuangroup.github.io/OpenS2V-Nexus/) |
| **SpeakerVid-5M** | 2025 | Text, Video, Audio | Audio-Driven Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2507.09862) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://dorniwang.github.io/SpeakerVid-5M/) |
| **AIGVQA‚ÄëDB** | 2025 | Text, Video, Ratings | Text‚Äëto‚ÄëVideo Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/abs/2411.17221) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/wangjiarui153/AIGV-Assessor) |
| **EvalCrafter** | 2024 | Text, Video | Text‚Äëto‚ÄëVideo Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2310.11440) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://evalcrafter.github.io/) |
| **EditBoard** | 2025 | Video, Instruction, Edited Video | Video‚Äëto‚ÄëVideo Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i15.33754) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/Samchen2003/EditBoard) |
| **VE‚ÄëBench DB** | 2025 | Video, Text, Edited Video | Video‚Äëto‚ÄëVideo Editing | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://doi.org/10.1609/aaai.v39i7.32763) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://github.com/littlespray/VE-Bench) |
| **SAVGBench** | 2024 | Video, Audio, Spatial-Temporal Event | Audio‚ÄëDriven Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/abs/2412.13462v1) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](‚Äî) |
| **Morpheus** | 2025 | Video | Reinforcement Learning for Video Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](http://arxiv.org/pdf/2504.02918) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://physics-from-video.github.io/morpheus-bench/) |
| **MJ‚ÄëVideo** | 2025 | Video, Text, Rating | Text‚Äëto‚ÄëVideo Generation | [![Paper](https://img.shields.io/badge/Paper-Link-red?style=for-the-badge)](https://arxiv.org/pdf/2502.01719) | [![Website](https://img.shields.io/badge/Website-Link-orange?style=for-the-badge)](https://aiming-lab.github.io/MJ-VIDEO.github.io/) |





[<small>‚áß Back to ToC</small>](#contents)

---

## <span id="about-us">üéì About Us</span>

QuenithAI is a professional organization composed of top researchers, dedicated to providing high-quality 1-on-1 research mentoring for university students worldwide. Our mission is to help students bridge the gap from theoretical knowledge to cutting-edge research and publish their work in top-tier conferences and journals.

Maintaining this `Awesome Video Generation` list requires significant effort, just as completing a high-quality paper requires focused dedication and expert guidance. If you're looking for one-on-one support from top scholars on your own research project, to quickly identify innovative ideas and make publications, we invite you to contact us ASAP.

‚û°Ô∏è **Contact us via [WeChat](assets/wechat.jpg) or [E-mail](mailto:your.email@example.com) to start your research journey.**

---

„ÄåÂ∫îËææÂ≠¶ÊúØ„Äç(QuenithAI) ÊòØ‰∏ÄÂÆ∂Áî±È°∂Â∞ñÁ†îÁ©∂ËÄÖÁªÑÊàêÔºåËá¥Âäõ‰∫é‰∏∫ÂÖ®ÁêÉÈ´òÊ†°Â≠¶ÁîüÊèê‰æõÈ´òË¥®Èáè1V1ÁßëÁ†îËæÖÂØºÁöÑ‰∏ì‰∏öÊú∫ÊûÑ„ÄÇÊàë‰ª¨ÁöÑ‰ΩøÂëΩÊòØÂ∏ÆÂä©Â≠¶ÁîüÂüπÂÖªÂá∫Ëâ≤ÂçìË∂äÁöÑÁßëÁ†îÊäÄËÉΩÔºåÂú®È°∂Á∫ß‰ºöËÆÆÂíåÊúüÂàä‰∏äÂèëË°®Ëá™Â∑±ÁöÑÊàêÊûú„ÄÇ

Áª¥Êä§‰∏Ä‰∏™GitHubË∞ÉÁ†î‰ªìÂ∫ìÈúÄË¶ÅÂ∑®Â§ßÁöÑÁ≤æÂäõÔºåÊ≠£Â¶ÇÂÆåÊàê‰∏ÄÁØáÈ´òË¥®ÈáèÁöÑËÆ∫Êñá‰∏ÄÊ†∑ÔºåÁ¶ª‰∏çÂºÄ‰∏ìÊ≥®ÁöÑÊäïÂÖ•Âíå‰∏ì‰∏öÁöÑÊåáÂØº„ÄÇÂ¶ÇÊûúÊÇ®Â∏åÊúõÂú®Ëá™Â∑±ÁöÑÁ†îÁ©∂È°πÁõÆ‰∏≠ÔºåËé∑ÂæóÊù•Ëá™È°∂Â∞ñÂ≠¶ËÄÖÁöÑ‰∏ÄÂØπ‰∏ÄÊîØÊåÅÔºåÊàë‰ª¨ËØöÈÇÄÊÇ®‰∏éÊàë‰ª¨ÂèñÂæóËÅîÁ≥ª„ÄÇ

‚û°Ô∏è **Ê¨¢ËøéÈÄöËøá [ÂæÆ‰ø°](assets/wechat.jpg) Êàñ [ÈÇÆ‰ª∂](mailto:your.email@example.com) ËÅîÁ≥ªÊàë‰ª¨ÔºåÂºÄÂêØÊÇ®ÁöÑÁßëÁ†î‰πãÊóÖ„ÄÇ**


[<small>‚áß Back to ToC</small>](#contents)

---



## <span id="contributing">ü§ù Contributing</span>

Contributions are welcome! Please see our [**Contribution Guidelines**](CONTRIBUTING.md) for details on how to add new papers, correct information, or improve the repository.

---

## <span id="community">üí¨ Join the Community</span>

Join our community to stay up-to-date with the latest advancements, share your work, and collaborate with other researchers and developers in the field of video generation.

<div align="center">
  <a href="YOUR_OFFICIAL_WEBSITE_URL">
    <img src="assets/group.png" alt="QuenithAI Logo">
  </a>
</div>